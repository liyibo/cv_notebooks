{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图像到图像的转换是一类视觉问题，其目标是学习训练数据中对齐的图像对之间输入到输出的映射。但是，在很多任务中并不存在配对的训练数据。我们提出一种在没有配对训练数据情况下，将输入数据从原域 X 映射到目标域 Y 的方法。我们的目标是学习映射 G：X→Y，使得 G(X) 中的图像分布与使用对抗性损失的分布 Y 不可区分。因为这个映射是高度欠约束的，所以我们将它与一个逆映射 F：Y→X 耦合起来，引入一个循环一致性损失来强制 $ F(G(x)) \\approx X $（反之亦然）。这种方法适用于风格转换，目标形变，季节转移，照片增强等）等任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 摘要  \n",
    "\n",
    "1 该文章提出了一种可以不用匹配成对的数据进行图像到图像进行转换的方法，缺乏对偶实例的监督情况下，在集合层面上进行监督学习  \n",
    "2 “循环一致”的性质，就是说，如果把一个从英语到法语的句子翻译过来，然后从法语翻译成英语，应该回到原来的句子。在数学上，如果有一个翻译器 G：X→Y 和另一个翻译器 F：Y→X，那么 G 和 F 应该是彼此逆的，并且这两个映射应该是双射。通过同时训练映射 G 和 F 来应用这个结构假设，并增加一个鼓励 $ F(G(x)) \\approx X $ 和 $ G(F(x)) \\approx Y $ 的循环一致性损失。将这种损失与 X 域和 Y 域的对抗性损失相结合，就可以完全实现对不成对的图像转换的目标。  \n",
    "3 generator 用来将输入的两幅图像转换成对方的风格，discriminator 用来检查生成图像的质量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 论文阅读  \n",
    "\n",
    "### Introduction  \n",
    "\n",
    "1873年春天，克劳德·莫奈在阿根廷附近塞纳河岸边放置画架时，看到了什么(图1，左上角)？如果莫奈在一个凉爽的夏天夜晚在卡西斯的小港上发生了什么事(图1左下角)呢？可以推断这两组图像之间的风格差异，从而想象一下，如果我们将它从一套“翻译”成另一套图像，可能会是什么样子。  \n",
    "\n",
    "在本文中，我们提出了一种方法，可以学习做同样的事情：捕捉一个图像集合的特殊特征，并找出如何将这些特征转换为其他图像集合，所有的操作都在没有任何配对训练的样本中进行的。  \n",
    "\n",
    "这个问题可以更广泛地描述为图像到图像的转换，将图像从给定场景的一个表示 x 转换到另一个 y，例如灰度转换为彩色，图像转换为语义标签，边缘图转换成照片。在计算机视觉方面的多年研究已经在监督学习中产生了强大的翻译系统，其中示例图像对{x，y}是存在的（图 2 左）。然而，获得配对训练 数据可能是困难的和昂贵的。 例如，对于诸如语义分割的任务，仅存在几个数据集，并且相对较小。由于期望的输出是非常复杂的，通常需要艺术创作，所以获得用于诸如艺术风格的图形任务的输入 - 输出对可能更加困难。对于许多任务，如目标变形（斑马，图 1 中上），期望的输出甚至没有明确定义。  \n",
    "\n",
    "因此，我们寻求一种算法，可以学习在没有配对输入输出示例的域之间进行转换（图 2，右）。我们假设这些领域之间存在着某种潜在的关系 - 例如，它们是同一个基础场景的两种不同的效果图 - 并试图学习这种关系。虽然我们缺乏对偶实例的监督，但是我们可以在集合层面上利用监督：在域 X 中给予一组图像，在域 Y 给予不同的集合。我们可以训练一个映射 G：X→Y，这样输出 $\\overline{y} = G(x)$，x∈X，并且判别网络对$\\overline{y}$ 与 y 是不可分的。从理论上讲，这个目标可以导致 $\\overline{y}$ 的输出分布与经验分布 $p_{data}(y)$ 相匹配（一般来说，这需要G是随机的）。因此最佳的G 将域 X 翻译成分布与 Y 相同的域 $\\overline{Y}$. 然而，这样的翻译并不能保证单个输入 x 和输出 y 是以有意义的方式配对的 - 有无穷多的映射G可以生成与 y 的相同分布。而且，在实践中，我们发现很难孤立地优化对抗样本：标准程序通常会导致模式崩溃问题，即所有输入图像都映射到相同的输出图像，并且优化未能取得进展。  \n",
    "\n",
    "这些问题要求我们增加更多的结构。因此，我们利用翻译应该是“循环一致”的性质，就是说，如果我们把一个从英语到法语的句子翻译过来，然后从法语翻译成英语，我们应该回到原来的句子。 在数学上，如果我们有一个翻译器 G：X→Y 和另一个翻译器 F：Y→X，那么 G 和 F 应该是彼此逆的，并且这两个映射应该是双射。通过同时训练映射 G 和 F 来应用这个结构假设，并增加一个鼓励 $ F(G(x)) \\approx x $ 和$ G(F(y)) \\approx y $ 的循环一致性损失。 将这种损失与 X 域和 Y 域的对抗性损失相结合，就可以完全实现我们对不成对的图像翻译的目标。  \n",
    "\n",
    "我们将我们的方法应用于广泛的应用，包括风格转换，物体变形，季节转换和照片增强。我们还将之前的方法与依靠手工定义的样式和内容因子或共享嵌入函数进行比较，并显示我们的方法优于这些基线。  \n",
    "\n",
    "### Related work  \n",
    "\n",
    "生成对抗网络（GANs）在图像生成，图像编辑和表示学习中取得了令人印象深刻的结果。最近的方法对条件图像生成应用和预测以及视频和 3D 数据等其他领域采用相同的思路。GANs 成功的关键在于对抗性损失的观念，这种损失迫使生成的图像原则上与真实图像无法区分。这对于图像生成任务特别有用，因为这正是计算机图形学大部分优化的目标。我们采取对抗性损失来学习映射，使得翻译后的图像不能与目标域中的图像区分开来。  \n",
    "\n",
    "图像到图像转换的思想至少可以追溯到赫茨曼（Hertzmann）等人的图像类比（Image Analogies），他们在一个输入输出训练图像对上采用非参数化的纹理模型。我们的方法建立在 Isola 等人的“pix2pix”框架上。它使用条件生成对抗网络学习输入映射到输出图像。类似的想法已经应用于各种任务，如从草图生成照片或从属性和语义布局。 然而，不像这些先前的作品，我们学习了没有配对训练样本的映射。    \n",
    "\n",
    "与上述方法不同，我们的公式并不依赖于输入和输出之间的任何特定的，预定义的相似性函数，也不假定输入和输出必须位于相同的低维嵌入空间中。这使得我们的方法成为许多视觉和图形任务的通用解决方案。  \n",
    "\n",
    "循环一致性使用传递性作为调整结构化数据的方法的想法有着悠久的历史。在视觉追踪中，强制执行简单的前向一致性一直是几十年来的标准技巧。在语言领域，通过“回译和重新翻译”来验证和改进翻译是人类翻译家（包括马克·吐温）以及机器所使用的技术。最近，运动，三维形状匹配，密集语义对齐和深度估计的结构中使用了高阶循环一致性。其中，周等人和戈达尔等人与我们的工作最相似，因为他们使用循环一致性损失作为使用传递性来监督 CNN 训练的一种方式。在这项工作中，我们引入了类似的损失，推动 G 和 F 相互一致。  \n",
    "\n",
    "神经风格转移是执行图像转换的另一种方式，它通过将一幅图像的内容与另一幅图像（通常是一幅绘画）的风格相结合来合成新颖的图像。另一方面，我们的主要焦点是通过试图捕获更高级别的外观结构之间的对应关系来学习两个图像集合之间的映射，而不是两个特定图像之间的映射。因此，我们的方法可以应用于其他在单样本转变上表现不好的任务，如绘画→照片，对象变形等。  \n",
    "\n",
    "### Formulation  \n",
    "\n",
    "我们的目标是学习给定训练样本的两个域 X 和 Y 之间的映射函数，其中 ${x_i}_{i=1}^N$,$x_i \\in X$ 和 ${y_j}_{j=1}^M$ ，$y_j \\in Y$。我们将数据分布表示为 $x 〜 p_{data}(x)$ 和 $y 〜 p_{data}(y)$。如图 3(a) 所示，我们的模型包括两个映射 G：X→Y和 F：Y→X。此外，我们引入两个对抗鉴别器 $D_X$ 和 $D_Y$，其中 $D_X$ 旨在区分图像 {x} 和转换的图像 $F(y)$; 同样，$D_Y$ 旨在区分 {y} 和 $G(x)$。 我们的目标包含两种类型的术语：对抗性损失，用于匹配生成图像的分布与目标域中的数据分布; 和周期一致性损失，以防止学习到的映射 G 和 F 相互矛盾。  \n",
    "\n",
    "#### 对抗性损失  \n",
    "\n",
    "我们将对抗损失应用于两个映射函数。对于映射函数 G：X→Y 及其鉴别器 $D_Y$，我们将目标表示为：\n",
    "\n",
    "$L_{GAN}(G,D_Y,X,Y) = E_{y〜p_{data(y)}}[logD_Y(y)] + E_{x〜p_{data(x)}}[log(1 - D_Y(G(x)))]$\n",
    "\n",
    "其中 G 试图产生看起来类似于来自域 Y 的图像的图像 G(x)，而 $D_Y$ 旨在区分经转换的样本 G(x) 和实际样本 y。 G 的目标是尽量减少这个损失，而对抗网络 D 试图最大化这个损失，即 ${min}_G{max}_{D_y} L_{GAN}(G,D_Y,X,Y)$。我们为映射函数 F：Y→X 及其鉴别器 $D_X$ 也引入类似的对抗损失：即 ${min}_F{max}_{D_x} L_{GAN}(F,D_X,Y,X)$。  \n",
    "\n",
    "#### 周期一致性损失  \n",
    "\n",
    "对抗性训练理论上可以学习映射 G 和 F，分别产生同样分布的目标域 Y 和 X。然而，在容量足够大的情况下，网络可以将同一组输入图像映射到目标域中的任意图像，其中任何映射都可以学习到与目标分布匹配的输出分布。因此，对抗的损失本身并不能保证所学的函数可以将单个输入 $x_i$ 映射到期望的输出 $y_i$。为了进一步减少可能的映射函数的空间，我们认为学习的映射函数应该是循环一致的：如图 3(b)所示，对于来自域 X 的每个图像 x，图像转换循环应该能够使 x 返回到原始图像，即 x→G（x）→F（G（x））≈ x。我们称之为正向循环一致性。类似地，如图 3(c) 所示，对于来自域 Y 的每个图像 y，G 和 F 也应当满足反向循环一致性：y→F（y）→G（F（y））≈ y。我们可以使用循环一致性损失来激励这种行为：  \n",
    "\n",
    "$L_{cyc}(G,F) = E_{y〜p_{data(y)}}[||G(F(y)) - y||_1] + E_{x〜p_{data(x)}}[||G(F(G(x))) - x||_1]$\n",
    "\n",
    "在初步实验中，我们也尝试用 $F(G(x))$ 和 x 之间以及 $G(F(y))$ 和 y 之间的对抗损失替换为 L1 损失，但没有观察到改善的性能。  \n",
    "\n",
    "由循环一致性损失引起的行为可以在 图 4 中观察到：重建图像 $F(G(x))$ 最终与输入图像 x 紧密匹配。  \n",
    "\n",
    "#### 完整的目标  \n",
    "\n",
    "我们的完整目标是：  \n",
    "\n",
    "$L(G,F,D_X,D_Y) = L_{GAN(G,D_Y,X,Y)} + L_{GAN(F,D_X,Y,X)} + \\lambda L_{cyc}(G,F)$  \n",
    "\n",
    "注意，我们的模型可以看作是训练两个“自动编码器”：学习一个自动编码器 F◦G：X→X 与另一个 G◦F：Y→Y 联合。 然而，这些自动编码器每个都有特殊的内部结构：它们通过中间表示将图像映射到自身，这是将图像翻译成另一个域的过程。这样的设置也可以被看作是“对抗自动编码器”的一个特例，它使用对抗性损失来训练自动编码器的瓶颈层以匹配任意目标分布。在我们的例子中，X→X 自动编码器的目标分配是域 Y 的目标分配。  \n",
    "\n",
    "### Implementation  \n",
    "\n",
    "#### Network Architecture  \n",
    "\n",
    "我们调整了Johnson 等人的生成网络结构。该网络结构在神经风格转移和超分辨率上的表现令人印象深刻。网络包含两个 stride-2 的卷积，多个残差块，以及两个分步的卷积。对于128×128图像，我们使用6个块，对于256×256和更高分辨率的训练图像，使用9个块。类似 Johnson 等人。我们使用实例标准化。对于判别器网络，我们使用 70×70 PatchGANs，其目的是分类 70×70 重叠的图像块是真实的还是假的。这种 Patch 级鉴别器结构比全图像鉴别器具有更少的参数，并且可以以完全卷积的方式应用于任意大小的图像。  \n",
    "\n",
    "#### Training details  \n",
    "\n",
    "我们运用近期的两项技术来稳定我们的模型训练过程。首先，对于 LGAN（方程1），我们用最小二乘损失代替负对数似然目标。这种损失在训练期间更加稳定，并产生更高质量的结果。  \n",
    "\n",
    "其次，为了减少模型振荡，我们遵循 Shrivastava 等人的策略，使用生成图像的历史来更新鉴别器，而不是由生成网络生成的最新图像来更新。我们保留一个图像缓冲区，存储 50 个以前生成的图像。  \n",
    "\n",
    "对于所有的实验，我们在等式 3 中设定$\\lambda = 10$。我们使用批处理大小为 1 的Adam求解器。所有网络都是从零开始训练的，学习率为$0:0.0002$。 在前 100 个 epochs，我们保持相同的学习速度，并在接下来的 100 个 epochs 内将速率线性衰减到零。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 数据准备  \n",
    "\n",
    "1 输入数据是成对出现的，将待转换的两幅图同时送入GAN中  \n",
    "2 dataA和dataB是两组需要进行图像转换的数据，不用一一对应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(dataA)\n",
    "np.random.shuffle(dataB)\n",
    "batch_files = list(zip(dataA[idx * self.batch_size:(idx + 1) * self.batch_size],\n",
    "                   dataB[idx * self.batch_size:(idx + 1) * self.batch_size]))\n",
    "batch_images = [load_train_data(batch_file, args.load_size, args.fine_size) for batch_file in batch_files]\n",
    "batch_images = np.array(batch_images).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 损失函数g_loss  \n",
    "\n",
    "1 real_A 和 real_B 是构造出来送入训练的图像对  \n",
    "2 fake_B 和 fake_A 是分别根据real_A 和 real_B 由generator生成的图像  \n",
    "3 fake_A_ 和 fake_B_是分别根据fake_B 和 fake_A 由generator生成的图像  \n",
    "4 DB_fake 和 DA_fake 是分别将fake_B 和 fake_A送入discriminator输出的结果  \n",
    "5 criterionGAN(self.DA_fake, tf.ones_like(self.DA_fake)) 和 criterionGAN(self.DB_fake, tf.ones_like(self.DB_fake)) 是欺骗discriminator的对抗性损失函数，让discriminator将生成的图像分类的真实的图像  \n",
    "6 abs_criterion(self.real_A, self.fake_A_) 和 abs_criterion(self.real_B, self.fake_B_) 用来优化generator，使生成的图像接近真实的图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.real_data = tf.placeholder(tf.float32,\n",
    "                                [None, self.image_size, self.image_size,\n",
    "                                 self.input_c_dim + self.output_c_dim],\n",
    "                                name='real_A_and_B_images')\n",
    "\n",
    "self.real_A = self.real_data[:, :, :, :self.input_c_dim]\n",
    "self.real_B = self.real_data[:, :, :, self.input_c_dim:self.input_c_dim + self.output_c_dim]\n",
    "\n",
    "self.fake_B = self.generator(self.real_A, self.options, False, name=\"generatorA2B\")\n",
    "self.fake_A_ = self.generator(self.fake_B, self.options, False, name=\"generatorB2A\")\n",
    "self.fake_A = self.generator(self.real_B, self.options, True, name=\"generatorB2A\")\n",
    "self.fake_B_ = self.generator(self.fake_A, self.options, True, name=\"generatorA2B\")\n",
    "\n",
    "self.DB_fake = self.discriminator(self.fake_B, self.options, reuse=False, name=\"discriminatorB\")\n",
    "self.DA_fake = self.discriminator(self.fake_A, self.options, reuse=False, name=\"discriminatorA\")\n",
    "\n",
    "self.g_loss = self.criterionGAN(self.DA_fake, tf.ones_like(self.DA_fake)) \\\n",
    "    + self.criterionGAN(self.DB_fake, tf.ones_like(self.DB_fake)) \\\n",
    "    + self.L1_lambda * abs_criterion(self.real_A, self.fake_A_) \\\n",
    "    + self.L1_lambda * abs_criterion(self.real_B, self.fake_B_)\n",
    "\n",
    "\n",
    "criterionGAN = mae_criterion\n",
    "def abs_criterion(in_, target):\n",
    "    return tf.reduce_mean(tf.abs(in_ - target))\n",
    "\n",
    "def mae_criterion(in_, target):\n",
    "    return tf.reduce_mean((in_-target)**2)       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 损失函数d_loss  \n",
    "\n",
    "1 fake_A_sample和fake_B_sample 分别是由real_B 和 real_A 生成的假的图像  \n",
    "2 d_loss 是优化discriminator的损失函数，又来将real_B 和 real_A 与 fake_A_sample和fake_B_sample 区分出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.fake_A_sample = tf.placeholder(tf.float32,\n",
    "                                    [None, self.image_size, self.image_size,\n",
    "                                     self.input_c_dim], name='fake_A_sample')\n",
    "self.fake_B_sample = tf.placeholder(tf.float32,\n",
    "                                    [None, self.image_size, self.image_size,\n",
    "                                     self.output_c_dim], name='fake_B_sample')\n",
    "\n",
    "self.DB_real = self.discriminator(self.real_B, self.options, reuse=True, name=\"discriminatorB\")\n",
    "self.DA_real = self.discriminator(self.real_A, self.options, reuse=True, name=\"discriminatorA\")\n",
    "self.DB_fake_sample = self.discriminator(self.fake_B_sample, self.options, reuse=True, name=\"discriminatorB\")\n",
    "self.DA_fake_sample = self.discriminator(self.fake_A_sample, self.options, reuse=True, name=\"discriminatorA\")\n",
    "\n",
    "self.db_loss_real = self.criterionGAN(self.DB_real, tf.ones_like(self.DB_real))\n",
    "self.db_loss_fake = self.criterionGAN(self.DB_fake_sample, tf.zeros_like(self.DB_fake_sample))\n",
    "self.db_loss = (self.db_loss_real + self.db_loss_fake) / 2\n",
    "self.da_loss_real = self.criterionGAN(self.DA_real, tf.ones_like(self.DA_real))\n",
    "self.da_loss_fake = self.criterionGAN(self.DA_fake_sample, tf.zeros_like(self.DA_fake_sample))\n",
    "self.da_loss = (self.da_loss_real + self.da_loss_fake) / 2\n",
    "self.d_loss = self.da_loss + self.db_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 训练网络  \n",
    "\n",
    "1 d_optim 和 g_optim 分别用来优化discriminator和generator  \n",
    "2 现将输入图片送到generator，生成假的fake_A 和 fake_B，然后送入discriminator 进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.d_optim = tf.train.AdamOptimizer(self.lr, beta1=args.beta1) \\\n",
    "    .minimize(self.d_loss, var_list=self.d_vars)\n",
    "self.g_optim = tf.train.AdamOptimizer(self.lr, beta1=args.beta1) \\\n",
    "    .minimize(self.g_loss, var_list=self.g_vars)\n",
    "\n",
    "# Update G network and record fake outputs\n",
    "fake_A, fake_B, _, summary_str = self.sess.run(\n",
    "    [self.fake_A, self.fake_B, self.g_optim, self.g_sum],\n",
    "    feed_dict={self.real_data: batch_images, self.lr: lr})\n",
    "\n",
    "# Update D network\n",
    "_, summary_str = self.sess.run(\n",
    "    [self.d_optim, self.d_sum],\n",
    "    feed_dict={self.real_data: batch_images,\n",
    "               self.fake_A_sample: fake_A,\n",
    "               self.fake_B_sample: fake_B,\n",
    "               self.lr: lr})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
