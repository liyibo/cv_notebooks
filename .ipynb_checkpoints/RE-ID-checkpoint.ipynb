{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person Re-identification: Past, Present and Future  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "由于 re-id 的应用性和研究意义，目前已经越来越重要。它旨在发现其他相机中感兴趣的人。早期，主要是有关 hand-crafted 算法与小规模的evaluation 的文章。近年来出现了大数据集和利用大数据的深度学习算法。考虑到不同的任务，我们将大多数当前的 re-ID 方法分为两类，image-based和video-based。在这两项任务中，都将 hand-crafted 和深度学习进行评估。此外，描述并讨论了两个与真实世界的应用非常接近的新的 re-id 识别任务，end-to-end re-ID 和 fast re-ID in very large galleries。本文的主要工作如下：1）介绍了人物识别的历史及其与图像分类和实例检索的关系； 2）调查了基于图像和视频的 re-ID 中手工系统和大规模数据方法的广泛选择； 3）描述了大数据集中端到端的 re-id 和快速检索的未来方向； 4）最后简要介绍一些重要而尚未开发的问题。  \n",
    "\n",
    "## 1 INTRODUCTION  \n",
    "\n",
    "。。。  \n",
    "\n",
    "在现代计算机视觉中，人物重识别的任务与旧时代有着相似的见解。在视频监控中，当呈现感兴趣的人（查询）时，人物重识别告诉该人是否在另一个地方（时间）被另一个摄像机观察到。这项任务的出现可以归因于：1）对公共安全需求的不断增加；2）在主题公园，大学校园和街道上广泛存在的大型摄像机网络。这两个原因使得仅依靠人类暴力劳动准确而有效地发现一个感兴趣的人或跨摄像机跟踪一个人是非常耗费时间和精力的。  \n",
    "\n",
    "技术上讲，视频监控中的人物重识别系统可分为三个模块，即人物检测，人物跟踪和人物检索。通常认为，前两个模块是独立的计算机视觉任务，因此大多数 re-ID 作品集中在最后一个模块，即人物检索。本文中，如果没有指定，人物重识别是指人物检索模块。从计算机视觉的角度来看，身份识别系统中最具挑战性的问题是如何在较大的外观变化下正确匹配同一人的两幅图像，如照明，姿态和视点等，这些都具有重要的科学价值。鉴于其研究和应用的重要性， re-ID 的社区正在快速增长，顶级期刊刊登的文章也越来越多(Fig. 1)。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/1/1.jpg?raw=true)\n",
    "\n",
    "### 1.1 Organization of This Survey  \n",
    "\n",
    "目前已经有不少 re-ID 的综述文章。本文中，我们主要讨论 re-ID 的视觉部分，这也是 re-ID 的焦点，并且向读者介绍了之前文章中摄像机标定和视图拓扑方法。与以前的 survey 的另一个不同之处在于，我们关注的是目前可用或可能在将来可见的不同 re-ID 子任务，而不是非常详细的技术或体系结构。我们特别强调深度学习方法，端到端的身份认证和超大规模身份认证，这些都是目前流行的话题并反映未来趋势。本 survey 首先在 1.2 节中引入了re-ID 的简要历史，在 1.3 节介绍了与分类和检索的关系，然后，我们分别在第 2 节和第 3 节中描述基于图像和基于视频的人物重识别的先前文献，这两类都有 handcrafted 和 deeply-learned 系统两种方法。在第 4 节中，由于检测，跟踪和再识别之间的关系还没有得到广泛的研究，我们将讨论以前的几个作品并指出未来的研究重点。在第 5 部分中，将引入采用最先进的检索模型的大规模再认证，这也是一个重要的未来方向。第 6 部分将总结其他一些未解决的问题，第 7 部分是结论。  \n",
    "\n",
    "### 1.2 A Brief History of Person Re-ID  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/1/2.jpg?raw=true)\n",
    "\n",
    "re-ID 研究始于多摄像头跟踪。自那时以来，一些重要的重识别方向已经开发出来。本文中，我们简要介绍一下 re-ID 历史中的一些里程碑（图2）。  \n",
    "\n",
    "**多摄像头跟踪** 在早期，作为一个没有正式提出的术语，person re-ID 与多摄像机跟踪紧密地结合在一起，其中表征模型与不相交摄像机之间的几何校准相结合。1997年，Huang 和 Russell 提出了一种贝叶斯公式，用于估计在其他相机视图中观察到证据的情况下，预测一台相机中物体的表征。表征模型包括多个时空特征，如颜色，车辆长度，高度和宽度，速度和观察时间。  \n",
    "\n",
    "**Multi-camera tracking with explicit “re-identification”** 据我们所知，多像机跟踪中第一次提出“person re-ID”这一术语的工作于2005 年由阿姆斯特丹大学的 Wojciech Zajdel 所发表。在他们的 ICRA'05 论文标题为“Keeping track of humans: Have I seen this person before?”，Zajedel 等人旨在“重新确定一个人离开视野后再重新进入”。在他们的方法中，假定每个人都有一个独特的潜在标签，并且定义一个动态贝叶斯网络来对标签和特征（颜色和空间 - 时间线索）之间的概率关系进行编码。进入人员的 ID 由通过近似贝叶斯推断算法计算的后验标签分布确定。  \n",
    "\n",
    "**The independence of re-ID (image-based)** 一年后的 2006 年，Gheissari 等人仅使用了用于前景检测的 spatial-temporal 分割算法之后的人的视觉线索。基于颜色和显著边缘直方图的视觉匹配由 articulated pedestrian 模型或 Hessian-Affine 兴趣点算子执行，在三个视野有重叠的摄像机拍摄的 44 个人的数据集上进行实验。请注意，尽管 Gheissari 等人使用视频帧设计时空分割方法，特征设计和匹配过程都不使用视频信息，因此我们将它归类为基于图像的 re-ID，这项工作标志着人类再识别与多摄像头跟踪的分离，以及其作为独立计算机视觉任务的开始。  \n",
    "\n",
    "**Video-based re-ID** 最初打算用于视频跟踪，但大多数 re-ID 的作品都专注于图像匹配。在2010年，有两篇作品提出用于多重 re-ID，其中帧是随机选择的。色彩是两个作品中常见的特征，Farenzena 等人另外使用分割模型来检测前景。对于距离测量，都是计算两个图像中边界框之间的最小距离，Bazzani 等人进一步将 Bhattacharyya 距离用于颜色和一般缩影特征。结果表明，每人使用多个帧有效地提高了单帧版本的结果，并且随着所选帧数量的增加，re-ID 的准确性将会饱和。  \n",
    "\n",
    "**Deep learning for re-ID** 2014年，深度学习在图像分类中的成功扩展到了 re-ID，当时 Yi 等人和 Li 等都采用 siamese 网络来确定一对输入图像是否属于同一个 ID。选择 siamese 模型的原因可能是每个身份训练样本的数量是有限的（通常是两个）。除了参数设置的一些变化之外，主要区别在于Yi 在网络中增加了额外的成本函数，而 Li 使用更精细的主体分割。两人实验数据也不一样，因此这两种方法不能直接进行比较。虽然其在小数据集上的表现尚不稳定，但深度学习方法自此成为 re-ID 的流行选项。  \n",
    "\n",
    "**End-to-end image-based re-ID** 虽然大多数作品在实验中使用由固定检测器生成的边界框或人工标点的边界框，但仍有必要研究行人检测器对re-ID 精度的影响。2014年，Xu 等人通过结合检测和 re-ID 分数来解决这个问题，结果表明，在 CAMPUS 数据集中，联合考虑检测和重识别置信度会导致比单独使用它们更高的人物检索准确性。  \n",
    "\n",
    "### 1.3 Relationship with Classification and Retrieval  \n",
    "\n",
    "根据训练和测试类别之间的关系，person re-ID 位于图像分类和实例检索之间（表1）。对于图像分类，每个类都有可用的训练图像，并且测试图像也是是定义好的这些类。对图像检索，通常不存在训练数据，因为提前不知道要查询的内容，数据集也可能包含各种各样的目标。所以训练类别是行不通的，测试数据也是没有见过的。  \n",
    "\n",
    "person re-ID 图像分类相似，有训练数据是存在的，其中图像包含不同的身份。person re-ID 也与实例检索类似，因为测试身份是不可见的：除了训练和测试图像都是行人之外，它们与训练身份没有重叠。  \n",
    "\n",
    "因此，person re-ID  可以利用分类和检索两个方面的成果。一方面，使用训练数据，可以在人的空间中学习识别度量距离或特征嵌入。另一方面，当涉及到检索时，高效的索引结构和散列技术可能有益于大型图库中的 re-ID。在这项调查中，有效学习和有效的检索方法都将被作为未来重要的方向被引入。  \n",
    "\n",
    "## 2 IMAGE-BASED PERSON RE-ID  \n",
    "\n",
    "自 Gheissari 等人在2006年开展工作以来，person re-ID 主要是使用单幅图像进行探索。让我们考虑一个封闭世界的玩具模型，其中 G 是由 N 个图像组成的图库（数据库），表示为 $\\{g_i\\}_{i=1}^N$，它们属于 N 个不同的身份。给定一个测试（查询）图像 q，其身份由下式确定：  \n",
    "\n",
    "$i^* = arg {max}_{i \\in 1,2,...,N}sim(q,g_i)$\n",
    "\n",
    "其中 $i^*$ 是测试数据 q 的 id，sim 是某种相似度函数。  \n",
    "\n",
    "### 2.1 Hand-crafted Systems  \n",
    "\n",
    "从公式 1 中可以看出，简单的 re-ID 系统需要两个组件，即图像描述和距离度量。  \n",
    "\n",
    "#### 2.1.1 Pedestrian Description  \n",
    "\n",
    "在行人描述中，最常用的特征是颜色，而纹理特征不怎么用。有的工作中，行人前景从背景中分割出来，并且为每个身体部分计算一个对称轴。基于身体结构，计算加权颜色直方图（WH），最大稳定颜色区域（MSCR）和 recurrent high-structured patches（RHSP）。WH 为靠近对称轴的像素分配较大的权重，并为每个部分形成颜色直方图。MSCR 可检测稳定的颜色区域并提取颜色，面积和质心等特征。相反，RHSP 是一种捕获经常性纹理补丁的纹理特征。 Gheissari 等人提出了一种空-时分割方法来检测稳定的前景区域。对于局部区域，计算 HS 直方图和边缘直方图。后者编码主要局部边界取向和边缘两侧的 RGB 比率。Gray 和 Tao 在亮度通道上使用 8 个颜色通道（RGB，HS 和 YCbCr）和 21 个纹理 filter，行人被划分为水平条纹。后来的许多作品采用与上述相同的特征。同样，Mignon 等人从RGB，YUV 和 HSV 通道以及水平条纹中的 LBP 纹理直方图建立特征向量。  \n",
    "\n",
    "与上述早期作品相比，近年来手工制作的特征几乎保持不变。在 Zhao 等人的一系列作品中，从每个密集采样的 10×10 小块中提取 32 维 LAB 颜色直方图和 128 维 SIFT 描述符。采用邻接约束搜索来查找图库图像中具有相似纬度的水平条纹中查询补丁的最佳匹配。 Das 等人将 HSV 直方图应用于提出的轮廓的头部，躯干和腿部。Li 等人也从补丁中提取局部颜色描述符，但是使用分层高斯化来聚合它们以捕获空间信息。Pedagadi 等人在使用 PCA 降维之前，从 HSV 和 YUV 空间提取颜色直方图和矩。Liu 等人为每个局部斑块提取 HSV 直方图，梯度直方图和 LBP 直方图。为了提高 RGB 值对光度方差的鲁棒性，Yang 等人为 global 行人颜色描述引入基于颜色描述符的 salient 颜色名称（SCNCD）。还分析了背景和不同色彩空间的影响。Liao 等人提出local maximal occurrence（LOMO）描述符，其中包括颜色和 SILTP 直方图。在同一水平条纹中的 box 经历最大汇集并且在对数转换之前构建三级金字塔模型。Zheng 等人提出为每个本地补丁提取 11 维颜色名称描述符[45]，并通过 Bag-of-Words（BoW）模型将它们聚合成一个全局向量。  \n",
    "\n",
    "除了直接使用低级别的颜色和纹理特征之外，另一个不错的选择是基于属性的特征，可以将其视为中级表示。相信与低级描述符相比，属性对于图像翻译更加稳健。低级颜色和纹理特征用于训练属性分类器。属性加权后，生成的矢量集成在 SDALF 框架中，与其他视觉特征融合。Liu 等人使用带注释的属性来改进潜在的 Dirichlet 分配（LDA）模型，以滤除噪声 LDA 主题。Liu 等人提出以无监督的方式发现一些具有共同属性的行人原型，并根据原型自适应地确定不同查询人的特征权重。最近的一些作品借用外部数据进行属性学习。Su 等人将同一个人但是不同摄像机的二元语义属性嵌入连续的低秩属性空间，使得属性向量对于匹配具有更大的区分性。 Shi 等人建议从现有的时尚摄影数据集中学习多种属性，包括颜色，纹理和类别标签。这些属性直接传送到监控视频下的 re-ID，并取得竞争结果。最近，Li 等人收集了一个具有丰富注释的行人属性大型数据集，以促进基于属性的 re-ID 方法。  \n",
    "\n",
    "#### 2.1.2 Distance Metric Learning  \n",
    "\n",
    "在手工制作的 re-ID 系统中，一个好的距离度量对于它的成功至关重要，因为高维视觉特征通常不会捕获样本差异下的不变因子。这些度量学习方法被归类为监督学习与无监督学习，全局学习与局部学习等。在个人身份识别中，大多数作品属于监督式全局距离度量学习的范畴。  \n",
    "\n",
    "全局度量学习的总体思路是保持同一类的所有向量更接近，同时将不同类的向量进一步分开。最常用的公式是基于 Mahalanobis 距离函数的类，它使用线性缩放和特征空间的旋转来扩展欧几里德距离。两个向量 xi 和 xj 之间的平方距离可以写为，  \n",
    "\n",
    "$d(x_i,x_j) = (x_i-x_j)^TM(x_i-x_j)$\n",
    "\n",
    "其中 M 是一个正半定矩阵，等式 2 可以被表述为由 Xing 等人提出的凸规划问题。  \n",
    "\n",
    "在个人身份识别中，目前最流行的度量学习方法，KISSME 就是基于上述方程式的。在这个方法中，关于一个对 (i,j) 是否相似的决定被表述为似然比。 采用成对差分$(x_{i,j} = x_i - x_j)$，并假定差分空间是零均值的高斯分布。  \n",
    "\n",
    "基于公式2，已经引入了许多其他度量学习方法。在早期，一些经典的度量学习方法针对最近邻分类。Weinberger 等人提出了为目标邻居（匹配对）设置边界并惩罚那些侵入边界（冒名顶替者）的 large margin 最近邻学习（LMNN）方法。这种方法属于有监督的局部距离度量学习。为了避免 LMNN中 遇到过拟合问题，Davis 等人提出信息论度量学习（ITML）作为满足给定的相似性约束和确保学习的度量接近初始距离函数之间的折衷。  \n",
    "\n",
    "近年来，Hirzer 等人提出了放宽积极约束，它为矩阵 M 提供了充分的近似，并且计算成本低得多。除了Mahalanobis距离之外，Chen 等人还增加了一个双线性相似性，这样可以模拟 cross-patch 的相似性。在中，全局距离度量与局部自适应阈值规则相结合，该规则还包含(xi,xj)的正交信息。Liao 等人建议坚持积极的半定义约束，并建议对正面和负面样本进行不同的加权。Yang 等人考虑图像对之间的差异和共同性，并且显示不同对的协方差矩阵可以从相似对的协方差矩阵推断出来，这使得学习过程可以扩展到大型数据集。  \n",
    "\n",
    "除了学习距离度量标准之外，一些作品专注于学习可区分性子空间。Liao 等人提出了将投影 w 学习到一个低维子空间，用与线性判别分析（LDA）类似的方法解决了交叉视图数据。  \n",
    "\n",
    "$J(w) = \\frac {w^TS_bw}{w^TS_w^W}$\n",
    "\n",
    "其中 Sb 和 Sw 分别是类间和类内散度矩阵。然后，使用 KISSME 在得到的子空间中学习距离函数。为了学习 w，Zhang 等人进一步采用 null Foley-Sammon 变换来学习一个满足 0 类内散度和正类间散度的判别性零空间。为了降低尺寸，Pedagadi 等人顺序结合无监督 PCA（主成分分析）和监督局部Fisher 判别分析，保留局部结构。   \n",
    "\n",
    "除了使用 Mahalanobis 距离（方程2）的方法外，还有一些使用其他学习工具，例如支持向量机（SVM）或 boosting。Prosser 等人建议学习一组弱RankSVMs，随后组装成一个更强大的 ranker。此外，structural SVM 也可以被用来在决策级别组合不同颜色描述符。Zhang 等人为每个训练身份学习一个特定的 SVM，并将每个测试图像映射到从其视觉特征推断出的权重向量。Gray 和 Tao 提出使用 AdaBoost 算法来选择和组合许多不同类型的简单特征到单个相似函数中。  \n",
    "\n",
    "## 2.2 Deeply-learned Systems  \n",
    "\n",
    "自 Krizhevsky 等人以来，基于 CNN 的深度学习模型一直很流行。一般来说，社区中通常使用两种 CNN 模型。第一种是用于图像分类和对象检测的分类模型。第二种是使用图像对或 triplets 作为输入的 siamese 模型。深度学习 re-ID 的主要瓶颈是缺乏训练数据。大多数重识别数据集仅为每个身份提供两个图像，例如 VIPeR，因此目前大多数基于 CNN 的重识别方法都集中在 siamese 模型上。在[15]中，输入图像被划分为三个重叠的水平部分，并且这些部分经过两个卷积层加上一个全连接层，将它们融合并输出该图像的向量，使用余弦距离计算两个输出向量的相似度。Li 等人不同之处在于添加了一个补丁匹配层，它将不同水平条纹中的两幅图像的卷积响应相乘，类似于领域的 ACS。后来，Ahmed 等人通过计算交叉输入邻域差异特征来改进 siamese 模型，该特征将来自一个输入图像的特征与另一个图像的邻近位置中的特征进行比较。Wu 等人使用较小尺寸的卷积滤波器加深网络，称为“PersonNet”。Varior 等人将长期短期记忆（LSTM）模块纳入连体网络。 LSTM 依次处理图像部分，以便记忆空间连接以增强深度特征的判别能力。Varior 等人建议在每个卷积层之后插入一个门控函数，以在将一对测试图像馈送到网络中时捕获有效的细微图案。该方法在几个基准上达到了最新的精度，但其缺点也是显而易见的。查询必须在发送到网络之前与每个图库图像配对 - 这是大型数据集中时间效率低下的过程。Liu 等人建议将一个基于 soft attention 的模型集成在一个 siamese 网络中，以自适应地关注输入图像对的重要局部部分；然而，虽然这些作品使用图像对，这种方法也受到计算效率低下的限制。Cheng 等人设计一个三重损失函数，将三个图像作为输入。在第一卷积层之后，针对每个图像划分四个重叠的主体部分并且与 FC 层中的全局部分融合。Su 等人提出了一个三阶段学习过程，其中包括使用独立数据集的属性预测和在具有 ID 标签的数据集上训练的属性三元组损失。  \n",
    "\n",
    "siamese 模型的缺点是它没有充分利用 re-ID 标注。事实上，siamese 模型只需要考虑成对（或三重）标签。告诉图像对是否相似（属于相同的身份），这在 re-ID 中是较弱的标签。另一个潜在的有效策略是使用分类/识别模式，该模式充分利用了再识别标签。在[76]中，来自多个数据集的训练身份联合形成了训练集，并且在分类网络中采用了 softmax 损失。在较大的数据集上，如 PRW 和 MARS，分类模型在没有仔细训练样本选择的情况下可以实现良好的性能。然而，每个 ID 需要有更多的训练数据才能使模型收敛。为了进行比较，本文给出了两种模型的基线结果。在表 2 中，我们在 Market-1501 数据集上实施了识别和验证模型。所有的网络都使用默认的参数设置，并通过 ImageNet 预训练的模型进行微调。图像在被送入网络之前被调整为 224×224。初始学习率设置为 0.001，并在每个时期后减少 0.1 倍。训练在 36 次迭代之后完成。我们可以清楚地看到，识别模型优于验证模型，并且与最近的结果相比，residual-50 模型在 Market-1501 上获得了最先进的再识别精度。  \n",
    "\n",
    "上述作品以端到端的方式学习深度特征，还有一些以低级特征为输入的选择。在[79]中，包括 SIFT 和颜色直方图在内的低级描述符被汇总到每个图像的单个 Fisher Vector 中。混合网络在输入 Fisher 矢量上构建全连接层，并执行线性判别分析（LDA）作为目标函数，以产生具有低级类内差异和高级类间差异的嵌入。 \n",
    "\n",
    "### 2.3 Datasets and Evaluation  \n",
    "\n",
    "#### 2.3.1 Datasets  \n",
    "\n",
    "许多基于图像的重标识的数据集已经发布，一些常用的数据集汇总在表 3 中，最经常使用的是 VIPeR。它包含 632 个身份，每个身份有两个图像。y. 10\n",
    "random train/test splits 用于测试模型是否稳定，并且每个 split 在训练和测试集中都有 316 个不同的身份。这些数据集反映了各种情况，例如，GRID 数据集在一个地铁站收集，iLIDS 在机场到达大厅收集，CUHK01，CUHK02，CUHK03 和 Market-1501 在大学校园里被收集。近年来，可以在几个不同的方面看到收集数据方面的进步。  \n",
    "\n",
    "首先，数据集规模在不断增加。其中很多数据集的规模相对较小，尤其是那些早期的数据集，但最近的数据集（如 CUHK03 和 Market-1501）较大。两者都有超过 1000 个 ID 和超过 10000 个边界框，并且这两个数据集都为训练深度学习模型提供了大量的数据。这就是说，我们必须承认目前的数据量还远远不能令人满意，还非常需要更大的数据集。  \n",
    "\n",
    "其次，边界框倾向于由行人检测器（如 DPM 和 ACF ）生成，而不是手绘。在实际应用中，使用人工绘制数据集边界框是不可行的，因此必须使用检测器。 这可能会导致边界框偏离理想的边界框。在[16]中表明，由于检测器误差（如未对齐），使用检测到的边界框通常会导致与手绘边界框相比的精确度更低。 在[44]中，库中包含了大量错误的检测结果，这在使用检测器时是不可避免的。[44]中的实验表明，随着更多的误检被添加到数据集中，re-ID 的精确度就会不断的下降。因此，有必要研究具有实际缺陷的数据集。  \n",
    "\n",
    "第三，在收集期间使用更多的相机。例如，Market-1501 中的每个身份可以由多达 6 台摄像机捕获。这种设计需要具有良好泛化能力的度量学习方法，而不是在特定相机对之间仔细调整。事实上，一个具有 n 个摄像机网络中，摄像机对的数量是 $C_n^2$，因此从每个摄像机收集标注数据并且训练 $C_n^2$ 个距离度量是不可接受的。    \n",
    "\n",
    "#### 2.3.2 Evaluation Metrics  \n",
    "\n",
    "在评估重识别算法时，通常使用累积匹配特征（cmc）曲线。CMC 表示查询 id 出现在不同候选列表中的概率。不管数据集中匹配到了几个，CMC 只计算第一个匹配到的结果。所以基本上，只有当每个查询真实结果存在时 CMC 才是准确的评估方法。在实践中，当人们更关心查询结果出现在匹配到的结果最前面时，这种测量是可以接受的。  \n",
    "\n",
    "然而，对于研究的完整性，当数据集中存在多个真实结果时，Zheng 等人建议使用均值平均精度（mAP）进行评估。动机是一个完美的身份识别系统应该能够将所有真实的匹配返回给用户。这种情况可能是两个系统同样有能力发现第一个基本事实，但具有不同的检索召回能力。在这种情况下，CMC 没有足够的区分能力，但是可以使用。因此，mAP 与 CMC 一起用于 Market-1501 数据集。  \n",
    "\n",
    "#### 2.3.3 Re-ID Accuracy Over the Years  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/1/6.jpg?raw=true)\n",
    "\n",
    "在本节中，我们在图 3 中总结了多年来在几个有代表性数据集上重识别的准确率。所呈现的数据集是 VIPeR，CUHK01，iLIDS，PRID 450S，CUHK03 和 Market-1501。我们将目前的方法大致分为两类，即手工制作特征和深度学习。对于每个数据集，显示了在相应年份报告中最高的重识别准确率的代表性方法，从这些结果可以得出三个主要结论。  \n",
    "\n",
    "首先，多年来从六个数据集中可以观察到性能改善的明显趋势。我们观察到 VIPeR，CUHK01，i-LIDS，PRID 450S，CUHK03 和 Market-1501的表现分别增加了 +51.9％，+56.7％，+35.0％，+42.6％，+57.2％ 和 +31.62％。例如，在研究最多的数据集 VIPeR 中，从 2008 年到 2016 年，rank-1 精度从2008 年的 12.0％ 增加到 2015 年的 63.9％，提高了+51.9％。对于 Market-1501 数据集，自 2015 年发布以来，最先进的结果已从 44.42％增加到76.04％，提高了 31.62％。  \n",
    "\n",
    "其次，除了 VIPeR 之外，深度学习方法在其余 5 个数据集上产生了新的 state of the art 的结果。在这 5 个数据集（CUHK01，i-LIDS，PRID 450S，CUHK03 和 Market-1501）上，深度学习的性能优于手工制作的系统。在 CUHK03 和 Market-1501 这两个迄今为止最大的数据集中，我们观察到深度学习的压倒性优势。由于 VIPeR 相对较小，深度学习的优势无法全面体现；相反，手工制定的度量学习在这种情况下可能更有优势。考虑到图像分类和目标检测的情况，很有可能深度学习的系统将在未来几年继续支配再认证任务。  \n",
    "\n",
    "第三，我们推测还有很大的进一步改进空间，尤其是当大型数据集将要发布时。例如，在 Market-1501 数据集中，尽管没有使用多个查询，最佳的 rank-1 准确率为 65.88％，但是 mAP 非常低（39.55％）。这表明尽管在 6 台摄像机中找到第一个真正的匹配（rank-1 准确率）相对容易，但找到较难的样本并提高 MAP 并不是简单的事情。另一方面，虽然我们似乎能够在这些数据集上达到 60％ 到 70％ 的 rank-1 准确率，但我们必须记住，这些数据集在实际应用中只占很小的一部分。事实上，除了[44]之外，还有报道指出，在增加 10倍的数据集上，即使对于表现最佳的方法，rank-1 准确率降低 10 倍。因此，考虑到低 mAP（re-ID recall）和当前数据集的小规模，我们非常乐观的认为基于图像的重识别还有很大的进步空间。  \n",
    "\n",
    "## 3 VIDEO-BASED PERSON RE-ID  \n",
    "\n",
    "论文中，人物重识别大多是单张图片（单张照片）。近年来，由于增加了数据丰富性，导致更多的研究可能性，基于视频的 re-ID 已经变得流行。它与基于图像的重识别共享一个类似的的公式。基于视频的 re-ID 用两组边界框 $\\{q_i\\}_{i=1}^{n_q}$ 和 $\\{g_i\\}_{i=1}^{n_g}$ 替换图像 q 和 g，其中 nq 和 ng 分别是每个视频序列内的边界框的数量。与边界框特征一样重要的是，基于视频的方法更注重多镜头匹配方案和时间信息的整合。  \n",
    "\n",
    "### 3.1 Hand-crafted Systems  \n",
    "\n",
    "2010年的前两个 re-ID 试验都是手工制作的系统。他们基本上使用基于颜色的描述符，并可选地使用前景分割来检测行人。他们使用类似的基于图像的重新识别方法的图像特征，两者主要的区别是匹配函数。如 1.2 节所述，两种方法通常计算两组边界框特征之间的最小欧式距离作为集合的相似度。实质上，这种方法应该被分类为“multi-shot”的人物识别，其中两组帧之间的相似性起着关键作用。这种多重匹配策略被后来的作品采用。在[86]中，基于一组协方差特征，multiple shots 用于训练一个描述性增强模型。在[99]中，SURF 局部特征用于检测和描述短视频序列中的兴趣点，短视频序列依次在 KD 树中索引以加速匹配。在[11]中，生成时空图来标识前景分割的空 - 时稳定区域。随后使用聚类方法计算本地描述以改进匹配性能。Cong 等人采用视频序列中的流形几何结构来构建具有基于颜色的特征的更紧凑的空间描述符。Karaman 等人建议使用条件随机场（CRF）将空间和时间域中的约束合并。在[102]中，使用颜色和选定的人脸图像来建立一个模型，该模型捕捉外观特征以及其随时间变化的模型。Karanam 使用一个人的 multi-shots，并建议将探查特征作为同一人的线性组合。一个身份的多个照片也可以用来增强身体部位对准。在[85]中，为了寻找精确的部分到部分的对应关系，Cheng 等人提出了一种迭代算法，其中由于部分检测器的改进，每次迭代后图形结构的拟合变得更加精确。在[104]中，估计了行人姿势，并且具有相同姿势的框架与更高的置信度相匹配。  \n",
    "\n",
    "上述方法通常基于多个镜头构建外观模型，并且最近的趋势是将时间线索并入模型中。Wang 等人建议使用空 - 间描述符来重识别行人。其功能包括 HOG3D  和 gait energy image（GEI）。通过设计流量能量分布图（FEP），可以检测行走周期，以便使用本地最小/最大值周围的帧来提取运动特征。最后，通过区分视频排名模型选择可靠的时空特征并进行匹配。Liu 等人建议将视频序列解构成表示与某些动作基元相对应的身体动作的一系列单元，从该基元提取Fisher 矢量用于人的最终表示。Gao 等人利用行人的周期性特征，并将行走周期分成若干段。在[110]中，基于密集计算的多方向梯度提出了一种新的时空描述符，并丢弃了在短时间内发生的噪声运动。  \n",
    "\n",
    "在匹配视频时，距离度量学习也很重要。 在[111]中，提出了一组验证方法，其中采用 transfer ranking 来判断查询是匹配到的是相同身份的图像之一。 在[89]中，所提出的局部匹配模型的多重延伸使得最佳匹配对的距离最小化并且减少了交叉视图变换的数量。Zhu 等人同时提出学习视频内和视频间距离度量，以使视频表示更加紧凑并区分不同身份的视频。  \n",
    "\n",
    "### 3.2 Deeply-learned Systems  \n",
    "\n",
    "在基于视频的 re-ID 中，数据量通常比基于图像的数据集大，因为每个 tracklet 包含多个帧（表4）。  \n",
    "\n",
    "基于视频和基于图像的 re-ID 之间的基本区别在于，对于每个匹配单元（视频序列）而言，多个图像应该采用多个匹配策略或单个匹配策略。较老的作品一般使用多重匹配策略，这会导致较高的计算成本，并且可能在大型数据集上存在问题。另一方面，pooling-based 方法将帧级特征聚合成具有更好可扩展性的全局向量。结果，当前基于视频的重识别方法通常涉及合并步骤。这个步骤可以是 max/average pooling，或通过全连接的层学习。在 Zheng 等人的系统中，时间信息没有被明确地捕获；相反，一个 id 的 frames 作为训练数据，通过 softmax 训练分类模型。frames 特征通过 max pooling ，从而在三个数据集上产生有竞争力的准确率。这些方法被证明是有效的，但还有很大的改进空间。关于这一点，重认证的社区可以借鉴 action/event 识别。例如，Xu 等人建议将 CaffeNet 的第 5 卷积层中的列特征聚合成 Fisher 矢量或 VLAD，直接进行 CNN 特征转移。Fernando 等人提出了一个 learning-to-rank 的模型来捕捉视频中帧特征随着时间的推移如何演变，这产生了视频范围内时间动态的视频描述符。Wang 等人将多级编码层嵌入到 CNN 模型中并产生不同序列长度的视频描述符。  \n",
    "\n",
    "另一个好的做法是在最终表示中注入时间信息。在手工制造系统中，Wang 等人和刘等人在 iLIDS-VID 和 PRID-2011 数据集上使用纯空时特征并报告了有竞争性的准确率。然而，在[21]中，表明时空特征在 MARS 数据集上不具有足够的辨别力，因为许多行人在相同的摄像机下共享类似的运动，并且同一人的运动特征在不同相机下可能不同。在[21]中提出的观点是，外观特征在大规模视频再认证系统中至关重要。在文献[114]中，通过 CNN 模型从连续的视频帧中提取特征，然后传到 recurrent final layer，以便允许时间步之间的信息流。然后使用最大或平均池合并特征，以生成视频的外观特征。所有这些结构都被整合到一个 siamese 网络中。 [120]中使用了类似的体系结构，他们的区别是 two-fold ，首先，在[120]中使用了特定类型的 RNN，门控重发单元（GRU）。其次，[114]采用识别损失，这有利于损失收敛和性能改进。虽然这两个作品采用 siamese 网络进行损失计算，但 Yan et al 和郑等人使用将每个输入视频分类为各自身份的识别模型。在[115]中，手工制作的低级功能（如颜色和LBP）被送入多个 LSTM，LSTM 输出连接到 softmax 层。在动作识别中，Wu 等人提出从视频中提取外观和时空特征，并建立一个混合网络来融合这两种特征。在本文中，我们注意到外观和时空模型的区别组合可能是未来视频 re-ID 研究的有效解决方案。  \n",
    "\n",
    "### 3.3 Datasets and Evaluation  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/1/7.jpg?raw=true)\n",
    "\n",
    "存在多个 Re-ID 数据集，例如ETH，3DPES，PRID-2011，iLIDS-VID 和 MARS。表 4 总结了这些数据集的一些统计数据。ETH 数据集使用一台移动摄像机。它包含三个序列，并提供每个序列的多个图像。这个数据集相对容易，multi-shot 场景的 re-ID 准确率接近100％。3DPeS 数据集由 8 台非重叠户外相机收集。虽然视频已发布，但此数据集通常用于 single-shot re-ID。PRID-2011 和 iLIDSVID 的相似之处在于两个数据集都由 2 台摄像机捕获，每个身份有 2 个视频序列。iLIDSVID 在室内场景下拍摄了 300 个身份。 PRID-2011 每个相机在户外拍摄了 385 和 749 个身份，并且在这个数据集中有 200 个身份在两个相机中被观察到。在测试期间，根据[105]的提案，PRID-2011 使用了 178 个身份。一般认为，由于遮挡非常严重，iLIDS-VID 比 PRID-2011 更具挑战性。最近发布的 MARS 数据集，是一个大型视频 reID 数据集，包含超过 20000 个视频序列中的 1261 个身份，使用 DPM 检测器和 GMMCP 跟踪器生成。由于最近才发布，我们没有提供关于 MARS 数据集结果的广泛总结。图 4 给出了对三个有代表性的视频（多镜头）re-ID 数据集（即 ETHZ，iLIDS-VID，PRID-2011）的最新结果的评估。得出两个主要结论：  \n",
    "\n",
    "首先，ETHZ 数据集已达到其饱和性能。2015 年，Lisanti 等人和马丁内尔等人报告 rank-1 accuracy 接近 100%。在[124]中，每个序列使用 5 个图像，ETHZ 序列 1, 2 和 3 的 rank-1 accuracy 分别为 99.8％，99.7％ 和 99.9％。每个序列 10 帧的结果更高，达到 100％。主要原因是 ETHZ 数据集具有相对较少的身份，并且由于仅使用一个移动照相机而导致图像方差较低。这可能是第一个重识别数据集，几乎可以实现其最初目标。  \n",
    "\n",
    "其次，有效的视频再识别仍在 iLIDS-VID 和 PRID-2011 数据集上进行研究。自推出以来，我们观察到他们 rank-1 准确性（包括 ETHZ 数据集）的明显改善。对于 iLIDS-VID，Wang 等人的 rank-1 准确率为 23.3％。在 PRID-2011 上，Wang等人的 rank-1 准确率为 19.0％，两年后，Zheng等人使用在 MARS 数据集上进行微调的 CNN  max pooling，将这一分数提高 58.3％。  \n",
    "\n",
    "第三，深度学习方法在基于视频的身份识别中产生绝对优异的准确率。在 iLIDS-VID 和 PRID-2011 数据集上，表现最好的方法都是基于卷积神经网络，并可选择的插入循环神经网络。与基于图像的 reID 相比，训练数据的数量在视频重识别中明显更大。MARS 提供了超过 500k 的训练数据，而 MarketS-1501 数据集中的 13k 则是 MARS 的延伸。利用这些训练数据，不仅对基于视频的重识别，而且对基于图像的数据集进行识别网络的训练也是可行的。我们还注意到，虽然 MARS 数据集的 rank-1 准确率达到了 68.3％，但其 mAP 仍然较低（49.3％），并且在评估每个摄像机对的性能时，性能进一步降低。因此，我们认为视频再识别技术的研究仍有很大的改进空间。  \n",
    "\n",
    "## 4 FUTURE: DETECTION, TRACKING AND PERSON RE-ID  \n",
    "\n",
    "### 4.1 Previous Works  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/1/9.jpg?raw=true)\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/1/10.jpg?raw=true)\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/1/11.jpg?raw=true)\n",
    "\n",
    "尽管人的身份识别来源于多摄像头跟踪，但现在作为一个独立的研究主题进行研究。在这次调查中，我们认为重识别是未来的一个重要方向，将行人检测和追踪作为一种情景加入，而是扮演更加独立的角色。具体来说，我们考虑一个端到端的重识别系统，它将原始视频作为输入，并集成行人检测和跟踪以及重识别。  \n",
    "\n",
    "直到最近，大多数 reID 作品都基于两个假设：第一，给出了行人边界框的图库；其次，边界框是手绘的，具有完美的检测质量。但是，在实践中，这两个假设并不成立。一方面，图库的大小随检测器阈值而变化。较低的阈值会产生更多的边界框（较大的图库，较高的召回率和较低的精度），反之亦然。当检测召回/精度因阈值不同而发生变化时，重识别精度不会保持稳定。另一方面，当使用行人检测器时，边界框通常会存在检测错误，例如未对准，漏检和错误警报。此外，当使用行人追踪器时，追踪错误可能导致轨迹内的异常帧，即具有不同身份的背景或行人。因此，行人检测和跟踪的质量可能会直接影响重识别的准确性，这在再识别社区中很少讨论。下面我们将回顾一下这方面的几部作品。  \n",
    "\n",
    "在最初尝试解决第二个问题时，引入了几个数据集，即 CUHK03，Market-1501 和 MARS。这些数据集不会假定完美的检测/跟踪输出，并且更接近实际应用。例如，Li 等人表明，在 CUHK03 上，使用检测到的边界框的重识别精度低于用手绘边界框获得的重识别精度。后来的作品也报道了这一结果。这些发现与实际应用密切相关。在 MARS 上，跟踪错误（图 8）以及检测错误被展示出来，但跟踪错误将如何影响重识别的准确度仍然未知。  \n",
    "\n",
    "尽管数据集通过引入检测/跟踪错误而取得了进展，但它们并未明确评估检测/跟踪如何影响重识别，这中评估可以为最终如何在大量现有作品中选择检测器/跟踪器提供重要见解。据我们所知，Xu 等人提出了关于端到端人员重识别的第一项工作。他们使用术语“commonness”来描述图像边界框如何类似于行人，而术语“uniqueness”表示图库边界框和查询之间的相似性。通用性和唯一性由它们的产品以指数函数进行融合。此方法通过消除虚假背景检测的影响起作用。尽管 Xu 等人考虑检测对 reID 的影响，其局限性在于缺乏全面的基准测试和对图库动态问题的考虑。  \n",
    "\n",
    "2016年，Xiao 等人和 Zheng 等人同时引入基于大规模数据集的端到端身份认证系统。这两个作品采用原始视频帧和查询边界框作为输入（图 5）。需要先对原始帧执行行人检测，然后生成的边界框形成重识别库，然后，利用经典的 reID。这个过程被称为“人员搜索”，不再局限于重识别（图5（b））：它同时关注检测模块（图5（a））。这条管道的一个非常重要的方面是，如果有相同的重识别特征，更好的行人检测器往往会产生更高的重识别精度。另一个有趣的话题是行人检测是否有助于 person re-ID。在[18]，[77]中，检测可信度被整合到最终的重识别分数中。在[128]中，行人检测和 re-ID 在 CNN 模型中被联合考虑，类似于 Faster R-CNN，而在[77]中，ID-discriminative embedding（IDE）根据 R-CNN 模型预先训练的 CNN 模型进行行人检测。这些方法提供了关于弱标记检测数据如何帮助提高重识别准确性的初步见解。  \n",
    "\n",
    "尽管如此，在所谓的“端到端”系统中，没有提及行人跟踪，也不知道任何现有的作品/数据集解决跟踪对重识别的影响。该工作将其视为将检测，跟踪和检索集成到一个框架中的“终极”目标，并评估每个模块对整体 reID 性能的影响。 因此，本调查要求提供大规模数据集，以提供边界框注释以用于三项任务。  \n",
    "\n",
    "### 4.2 Future Issues  \n",
    "\n",
    "#### 4.2.1 System Performance Evaluation  \n",
    "\n",
    "适当的评估方法是一个关键而又很棘手的话题。一般来说，没有单一的“正确”协议，特别是对于探索不足的端到端 reID 任务。基于所使用的特定检测器/跟踪器及其参数，端到端的重识别系统与动态图库中大多数当前的重识别研究不同。此外，如何评估个人身份识别方案中的检测/跟踪性能，目前还很不明确。因此，这项调查提出了两个方面的系统评估问题。  \n",
    "\n",
    "首先，使用有效的评估指标对 reID 中的行人检测和跟踪是至关重要的。评估协议应该能够以现实和公正的方式对探测器/跟踪器的性能进行量化和评级，并提供重识别准确性的信息。例如，行人检测主要采用 log-average miss rate (MR)，这是在 precision range of FPPI (false positives per image)进行平均得到的。有些还使用 PASCAL VOC 中的 average precision (AP)。Dollar 等人认为，在某些任务（如汽车应用）中，使用针对 FPPI 的 miss rate 优于 precision recall curves，因为可接受的 FPPI 可能存在上限。与行人检测的汽车应用相反，人员重识别旨在找到一个不一定关心误报率的人。所以基本上我们可以使用 miss rate and average precision 来评估用于人员重识别中的行人检测。  \n",
    "\n",
    "AP/MR 计算中的一个重要参数是 intersect over union（IoU）分数的相交。如果其与真值边界框的 IoU 得分大于阈值，则检测到的边界框被认为是正确的。通常，阈值设置为 0.5，然而 Zhang 等人在各种 IoU 分数下研究“完美单帧检测器”和自动检测器之间的区别。KITTI 基准要求汽车检测的 IoU 为0.7，而行人为 0.5。对于人员重识别，这个问题是可以接受的。一些关于它的线索依然存在，如果我们更接近[77]中的结论，我们应该意识到使用较大的IoU 得分（例如0.7）与较低的 IoU 相比是更好的评估标准。图 6 显示了 PRW 数据集上的检测准确性（AP）和重识别准确性(rank-1 or mAP)之间的关系。在 IoU = 0.7 下的两个任务之间明显存在线性关系，而在 IoU = 0.5 下存在分散的情节。因此检测器和识别器之间的相关性与更大的 IoU 更加一致。尽管如此，它还远远不能令人满意。  \n",
    "\n",
    "考虑到边界框定位质量对于重识别精度非常重要，在评估检测器质量时要研究 IoU 阈值并查看它是否符合重识别的准确性是一个好主意。我们的直觉是，较大的 IoU 标准得到更好的定位结果，但是必须有一定的限制，因为当 IoU 变大时探测器性能的差异趋于减小。探索[138]中提出的从 0.5 到 1 的IoU平均召回率（AR）的用法并且将 AR 用于不同数量的检测器阈值。这样的评估指标考虑了召回和定位，并且我们推测它可能在 reID 中特别具有信息性，其中行人检测召回和边界框质量是至关重要的  \n",
    "\n",
    "虽然至少有一些线索可以指导行人检测的评估，但如何评估个人身份识别下的跟踪功能在很大程度上是未知的。在多目标跟踪（MOT）基准中，使用了多种评估指标，包括多目标跟踪精度（MOTP），主要是跟踪目标（轨道被追踪覆盖的地面真实人数的百分比）结果为至少 80％），false positive 总数（FP），ID switches 总数（IDS），轨迹碎片总数（Frag），每秒处理的帧数（Hz ）等等。有些指标可能是有限的指示能力，如处理速度，因为跟踪是一个离线步骤。对于 reID，我们预计跟踪精度是至关重要的，因为在 tracklets 中存在影响池化效果的异常图像是不理想的。我们还推测，80％ 可能不是用于评估重识别中 MT 的最佳阈值。正如[105]所建议的那样，在步行周期内提取特征是一种很好的做法，因此产生长的跟踪序列可能不会给 reID 带来太多的改进。未来，一旦用来评估跟踪和重识别的数据集发布，一个紧迫的问题就是设计适当的度量来评估不同的跟踪器。  \n",
    "\n",
    "评估程序中的第二个问题涉及整个系统的重识别准确性。与图库固定的传统 reID 相反，在端到端重识别系统中，图库随检测/跟踪阈值而变化。更严格的阈值表示更高的检测/跟踪信心，因此图库较小，背景检测较少，反之亦然。此外，图库的大小对 reID 的准确性有直接影响。让我们以一个极端案例为例。当检测/跟踪阈值非常严格时，图库可能非常小，甚至真实数据也被排除在外。在另一个极端情况下，当检测/跟踪阈值被设置为一个非常宽松的值时，图库将会非常大，并且包含多个背景检测，这可能对 re-ID 产生负面影响，如[44]所示。因此，可以预见的是，过于严格或过于宽松的阈值导致劣质图库，并且 reID 的评估反应了 reID 准确率与图库大小之间的动态变化。在[77]中，郑等人针对每个图像的不同检测次数绘制 rank-1 准确度和 mAP，观察到曲线先上升到峰值然后下降。在 PRW 数据集中，峰值位于每个图像 4-5 个检测点处，其可以用作每个图像的平均行人数量的估计。在[128]中，采用了类似的协议，即将 rank-1 匹配率相对于检测召回作图，并在召回= 70％ 时达到其最大值。当召回进一步增加时，错误检测的普遍性将损害重识别的准确性。还可以探索其他一些想法，例如，根据 FPPI 绘制 reID 的准确性。请记住，图库的大小取决于检测器的阈值，未来可以设计其他新的评估指标，这些指标是信息性和无偏见的。  \n",
    "\n",
    "我们还指出了端到端 reID 系统中的另一种评估协议。在实践中，当呈现查询边界框/视频序列时，尽管可以在特定帧中定位身份并且通过行人检测/跟踪来告知其坐标，但是系统仅知道哪个帧身份重新出现在查询人的具体位置，然后可以通过高效率的人工找到。实质上，确定被查询人出现的确切帧比“检测/跟踪+重新识别”任务相对更容易，因为检测/跟踪错误可能不会产生大的影响。在这种情况下，reID 的准确度应该高于标准的 reID 任务。而且，mean\n",
    "average precision 可以用于检索的视频帧。由于这项任务不需要非常精确地定位人员，因此我们可以使用宽松的检测器/提议或跟踪器来提高帧级调用。可以学习探测器/proposals 来定位一个宽松的 IoU 限制的行人区域，并更加重视匹配，即从较大的边界框/空间-时间管中找到特定的人。  \n",
    "\n",
    "#### 4.2.2 The Influence of Detector/Tracker on Re-ID  \n",
    "\n",
    "人员身份识别来自行人追踪，如果他们被确定为具有相同身份，则来自多个摄像头的踪迹相关联。这一系列研究将重识别作为跟踪系统的一部分，并且不评估定位/跟踪精度对重识别准确性的影响。但是，即使 reID 独立以来，大多数研究都是在手绘图像包围盒上进行的，这是一种难以满足实际情况的理想情况。因此，在端到端的身份识别系统中，重要的是要了解检测/跟踪对重识别的影响，并提出检测/跟踪/数据可以帮助重识别的方法。 \n",
    "\n",
    "首先，行人/跟踪误差的确会影响重识别的准确性，但内在机制和可行解决方案仍然是有待解决的。错误检测（图7）可能会导致行人失准，尺度变化，部分失踪，最重要的是误报和错误检测，这会影响到身份识别性能并带来新的挑战。  \n",
    "\n",
    "一些 reID 作品会将检测/追踪错误考虑在内。Zheng 等人建议融合 local-local 和 global-local 匹配来解决严重遮挡或部分缺失的 reID 问题。Xu 等人通过将 GMM 编码的描述符与先前的分布进行匹配来计算“共同性”分数。该分数可用于消除不包含人体或不能提供良好的人体定位的误报。以类似的方式，Zheng 等人提出将检测器置信度（在平方根之后）整合到 reID 相似性分数中，据此重新对边界框进行排序。这些作品在发生错误后解决检测错误。尽管如此，在早期阶段可能会避免检测/跟踪错误。例如，在 Xiao 等人设计的网络中，在 fast R-CNN 子模块中添加了定位损失。它规定了对于有效的身份识别系统至关重要的定位质量。  \n",
    "\n",
    "未来的调查需要揭示人员重识别对检测/跟踪质量的依赖性。由于开发没有错误的检测器/跟踪器的想法太过于理想化，我们主张研究如何将检测置信度集成到 re-ID 分数中，即如何通过有效识别异常值来纠正错误，以及如何训练上下文不依赖于检测到的边界框的模型。例如，使用聚类算法来滤除 tracklet 中的不一致帧可以有效净化跟踪序列。在另一个例子中，检测到的边界框可以被放大以包括可能丢失的身体部位并学习区分性特征。  \n",
    "\n",
    "其次，我们应该意识到，如果设计得当，检测和跟踪可能有助于重识别。在[77]中，在 R-CNN 模型上进行微调的 IDE 网络被证明比直接在 ImageNet 预训练模型上进行精细调整的 IDE 网络更有效。这说明了在行人检测中使用过量标记数据的重要性，即具有 ID 注释和误报检测的行人。在[128]中，端到端网络集成了背景检测的损失，这被假定为提高了学习嵌入的判别能力。将检测分数整合到 re-ID 相似性中也可以被视为检测有助于重识别的替代方案。  \n",
    "\n",
    "看起来行人检测/跟踪可能有助于重识别或者相反，但是如果我们考虑通用图像分类和细粒度分类的类比，我们可能会想到一些线索。例如，在细粒度数据集上微调 ImageNet 预先训练的 CNN 模型是更快收敛和更高细粒度识别准确率的有效方法。通过向 RCNN 部分反向传播 reID 损失来联合训练行人检测和重识别模型也是一个好主意。能够区分不同的身份可能有利于从背景中区分行人的任务，后者也可能对前者有所帮助。  \n",
    "\n",
    "可以探索的想法之一是使用无监督的跟踪数据。在视频中，追踪行人并不是一件困难的事情，尽管跟踪错误是不可避免的。面部识别，颜色和非背景信息是改善跟踪效果的有用工具，如哈利波特的掠夺者地图。在跟踪序列中，人的外观可能在一定程度上经历差异，但可以预期，大部分边界框是同一个人。在这种情况下，每个轨迹代表一个人，其中包含一些嘈杂但粗略可用的训练样本。因此，我们可以利用 racking results 来训练行人验证/识别模型，从而减轻对大规模监督数据的依赖。作为另一个有前景的想法，值得尝试使用自动编码器或生成对抗网络（GAN）。使用这样的无监督网络直接学习人员描述符以帮助解决人身份识别中的数据问题也是有趣的。  \n",
    "\n",
    "## 5 FUTURE: PERSON RE-ID IN VERY LARGE GALLERIES  \n",
    "\n",
    "近年来，身份认证社区的数据量显著增加，例如，VIPeR 和 iLIDS 中的数百个图库图像与 PRW 和 LSPS 中的图像数量相比显著增加导致了深度学习方法的优势。然而，显然目前的数据集还远未达到实际的规模。假设在具有 100 个摄像机的区域级监视网络中，如果每秒使用一个视频帧用于行人检测，并且每帧产生平均 10 个边界框，则运行该系统12小时将产生 $43.2 × 10^6$ 个边界框。但据我们所知，以前没有任何作品报道过在这样一个大型图库里 reID 的表现。似乎文献中使用的最大的图库是 500k，证据表明，与具有 19k 图库的 Market-1501 相比，MAP 下降了 7％ 以上。此外，在[44]中，近似最近邻搜索被用于快速检索，但代价是精确度受损。  \n",
    "\n",
    "从研究和应用的角度来看，非常大的图库中的人物再认证应该是未来的关键方向。应该尝试改善准确率和效率问题。  \n",
    "\n",
    "一方面，健壮和大规模的描述符和距离度量的学习更重要。这与目前的研究一致。在大规模图像识别之后，人员重识别将进行大规模评估。尽管目前的方法在非常有限的时间窗口内解决了一对或多对摄像机之间的重识别问题，但长期以来摄像机网络中的鲁棒性尚未得到充分考虑。在[36]，[146]中，摄像机网络中的 reID 一致性使用成对的匹配准确率进行优化，但测试数据集（WARD 和 RAiD）只有 3 和 4 个摄像机并少于 100 个身份。在具有 n 个相机的网络中，相机对的数量是 $O(n^2)$。考虑到较长的记录时间和缺乏注释数据，通常不能以成对方式训练距离度量或 CNN 描述符。因此，培养适应各种照明条件和摄像机位置的 global re-ID 模型是重中之重。为实现这一目标，一种选择是设计无监督描述符，其目的是寻找视觉上相似的人并将视觉上不相似的人视为错误匹配，但是无监督的方法可能倾向于照明变化。  \n",
    "\n",
    "另一方面，效率是这样一个大规模环境中的另一个重要问题。尽管在小型数据集中几乎可以忽略计算时间，但在我们的实验中，在使用 3.1GHz Intel Xeon E5-2687w v3（10个内核），64GB 内存的服务器上使用 MATLAB 2014 时，计算需要 8.50s 100-dim 浮动矢量与数量为 1000 万的 100-dim 矢量之间的距离。如果我们使用从 CaffeNet 和 C++ 编程中提取的 4,096 维浮点向量，所用的时间会显着增加到 60.7s，包括 33.2s 的距离计算和 26.8s 的数据从磁盘加载。显然，根据特征尺寸和图库大小，查询时间显着增加，这对于实际使用而言是不希望的。据我们所知，以前的 person re-ID 很少关注效率问题，因此缺乏有效的解决方案，但幸运的是，我们可以求助于图像检索社区寻求答案，本调查提供了两个可能的方向。    \n",
    "\n",
    "**Inverted index-based** 倒排索引是基于 Bag-of-Words（BoW）检索方法的数据结构。基于局部描述符的量化结果，倒排索引具有 k 个条目，其中 k 表示码本大小。索引结构因此具有 k 个条目，并且每个都附加到倒排列表，其中本地描述符被索引。基线倒排索引的结构如图 9 所示。将索引描述符的图像 ID 和词频（TF）存储在一系列作品中，可以存储许多其他元数据，例如二进制签名，特征坐标等。对于实例检索中倒排索引的基本知识和最新进展，我们引用读者最近的一项调查[19]。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/1/12.jpg?raw=true)\n",
    "\n",
    "在 person re-ID 中，局部描述符的使用非常流行。颜色和纹理特征通常从局部补丁中提取。虽然以前的一些作品使用了复杂的匹配算法，但最好使用大图库下的倒排索引来加速匹配过程。通常需要码本来将局部描述符量化为视觉单词，并且由于局部描述符是高维的，因此需要较大的码本来减少量化误差。 在这些情况下，倒排索引即可使用，这在很大程度上节省了存储器成本，并且如果适当使用，可以具有与无量化自适应情况相同的准确度。  \n",
    "\n",
    "**Hashing-based** Hashing 一直是一个广泛研究的近似最近邻搜索解决方案，其目的在于减少当图库较大或距离计算成本较高时查找确切最近邻的成本。在里程碑式的工作频谱 hash 之后，学习 hash 在社区中很受欢迎。它指的是学习 hash 函数，y = h（x），将向量 x 映射到紧致 y，并且旨在保持搜索过程的效率的同时在秩列表中找到真正的最近邻居。一些经典的学习哈希方法包括 product 量化（PQ），迭代量化（ITQ）等。这两种方法在训练中都是高效的并且具有合理的检索精度。它们不需要标记数据，因此当大量训练数据可能不可用时适用于重识别任务。  \n",
    "\n",
    "Supervised hashing 的另一个应用是图像检索，这是本节的重点。哈希函数通过深度学习网络端到端地学习，该网络输出给定输入图像的二进制向量。为了充分利用通用实例检索数据集中缺乏的训练数据，这一行着重于几个图像分类数据集，如 CIFAR-10 和 NUS-WIDE。在 person re-ID，应用场景与图像检索的 deep hashing 很好地吻合。在大型图库中，非常需要高效但准确的哈希方法，这是一种在重识别中较少探索的方向。如表 1 所示，训练类别在 reID 数据集中是存在的，而测试过程是一项标准的检索任务，所以当前受监督的哈希方法很容易被用于重识别。我们发现的唯一相关工作是[158]，该算法在一个 triplet-loss CNN 网络中学习哈希函数，并通过正则化来强化邻接一致性。该方法在 CUHK03 数据集上进行了测试，该数据集在每个测试分组中包含100 个身份，因此在这个意义上，超大图库的性能评估仍然不足。因此，本次调查需要非常大的重识别数据集，以评估重识别方法和可扩展算法的可扩展性，尤其是那些使用哈希编码的算法，以进一步将此任务推向实际应用。  \n",
    "\n",
    "## 6 OTHER IMPORTANT YET UNDER-DEVELOPED OPEN ISSUES  \n",
    "\n",
    "### 6.1 Battle Against Data Volumn  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/1/13.jpg?raw=true)\n",
    "\n",
    "标注大规模数据集一直是视觉社区的一个焦点。这个问题在个人身份识别中更具挑战性，因为除了绘制行人的边界框之外，还必须为他分配一个 ID。ID 分配不是微不足道的，因为在行人首次出现后，行人可能会重新进入视野（FOV）或长时间进入另一个观察摄像机。这使得协作标注变得困难，因为两个协作者在标注 ID 上进行通信的成本很高。这些困难部分解释了为什么当前数据集中 为什么每个 ID 的图片数量都非常有限。过去两年已经见证了一些大规模数据集的发布，例如 Market-1501，PRW，LSPS 和 MARS，但从实际应用的角度来看，它们还远远不够满意。在这项调查中，我们相信两种替代策略可以帮助绕过数据问题。  \n",
    "\n",
    "首先，如何使用来自追踪和检测数据集的注释尚处于探索阶段。与 reID 相比，跟踪和检测标注在人员重新进入 FOV 时不需要进行 ID 分配：大部分工作都花费在边框绘制上。在[77]中，显示在 R-CNN 阶段添加更多的行人和背景训练数据有益于后面 IDE 描述符的训练。在[50]，[75]中，来自独立数据集的属性注释被用来表示 reID 图像。由于这些属性可以通过工作人员之间的协作进行注释，并具有很好的泛化能力，因此它们也是重识别数据不足的替代方案。因此，缺乏训练数据时，外部资源对训练身份识别系统很有价值。  \n",
    "\n",
    "除了第 4.2.2 节提到的预训练/无监督策略之外，一种新颖的解决方案是从未标记的数据中检索出可能被视为度量学习/ CNN 训练中的“true positives”的 hard negatives。这种策略已经在对象分类中进行了评估，其中一小部分标签在训练之前受到干扰。它可以有效地扩大训练集，同时降低模型过拟合的风险。我们的初步实验表明，这个方向在基线方面有了不错的改进。  \n",
    "\n",
    "第二种策略是迁移学习，将经过训练的模型从源域转移到目标域。以前，监督学习需要大量标记数据，这限制了重识别系统扩展到其他摄像机。在文献[160]中，提出了一种无监督的主题模型，用于发现用于 re-ID 匹配的图像块。在文献[161]中，提出了一种弱监督方法，该方法需要来自其他重识别数据集的全注释以及目标场景中捕获的少量样本。在[162]，[163]中，提出了无监督转移学习，其中目标数据集未标记。 Ma 等人采用跨域排序 SVM，而 Peng 等人将转移问题表述为字典学习任务，该任务学习共享不变潜变量并且偏向于目标数据集。这些方法表明，从源头学习 reID 模型是可行的，并且从无监督数据中挖掘出区分线索是有益的。将 CNN 模型转换为其他 re-ID 数据集可能会更加困难，因为深层模型可以很好地适应来源。Xiao 等人收集多个源再识别数据集并联合训练目标数据集的识别模型。根据我们的经验，使用现成的度量学习方法也可以在一定程度上纠正转移效应，但无监督转移学习对于深度学习模型仍然是一个公开问题。  \n",
    "\n",
    "### 6.2 Re-ranking Re-ID Results  \n",
    "\n",
    "重识别过程（图5（b））可以看作是一个检索任务，其中重新排序是提高检索准确性的重要步骤。它指的是重新排序可以发现重新排序知识的初始排序结果。关于搜索重排方法的详细调查，我们将读者引荐给[164]。  \n",
    "\n",
    "此主题存在一些以前的作品。重新排序可以在循环中完成，也可以完全自动执行。当涉及在线人员标签时，Liu 等人提出允许用户从初始等级列表中提供简单的否定的以及可选地提供的一些硬性否定的后排序优化（POP）方法。稀疏的人体反馈可以实现查询人的即时自动区分特征选择。在一个改进中，Wang 等人设计人体验证增量学习（HVIL）模型，该模型不需要任何预先标记的训练数据，并从人类反馈累积学习以提供实例模型更新。许多渐进式学习的 HVIL 模型被合并为一个单一的整体模型，以便在人的反馈不再可用时使用。 Martinel 等人在相似的性质中，建议为查询找到最相关的图库图像，将它们发送给人类标签，最后使用标签更新重新识别模型。自动重排方法也在几篇作品中进行了研究。Zheng 等人提出了一种查询自适应融合方法来组合多个 re-ID 系统的排序结果。具体而言，使用初始分数曲线的形状，并且认为该曲线对于良好特征呈现“L”形状。在[95]中，基于 CMC 曲线的直接优化来组合各种度量。 Garc'ıa 等人分析等级列表中的无监督鉴别上下文信息。这进一步与脱机中学到的重新排序度量结合在一起。 Leng 等人使用互惠邻近邻居的思想来改进基于离线步骤构建图像关系的初始秩列表。  \n",
    "\n",
    "重新排序仍然是个人重识别的开放方向，而它在实例检索中已经被广泛研究。应用场景可以描述如下。当搜索感兴趣的人时，由于图像变化频繁，很可能在某些相机下的图像很难找到。但是我们也许能够找到一些与 hard positives 更类似的结果。所以以这种方式，一旦简单的结果被返回，就可以找到hard positives。在实例检索中重新排序的方法可以很容易地通过人新识别。由于训练数据可用于 reID（表1），因此可以根据训练分布设计重排序方法。例如，当进行 k-NN 重排时，可根据得分从训练集中确定返回结果的有效性。由于重新识别的重点是行人，重排名方法可以专门设计。例如，在获得初始等级列表之后，可以选择排名靠前的图像的子集，并且可以检索包含它们的视频帧。随后，我们可以通过昂贵的滑动窗口方法找到最佳定位而不会产生太多的计算负担，从而避免检测器未对准的影响。  \n",
    "\n",
    "### 6.3 Open-World Person Re-ID  \n",
    "\n",
    "大多数现有的重识别作品可被视为识别任务。查询身份被假定存在于图库中，并且任务旨在确定查询的 ID。相比之下，开放世界的再认证系统研究个人验证问题。也就是说，基于识别任务，公开世界问题给方程增加了另一个条件。  \n",
    "\n",
    "$sim(q,g_{i^*}) > h$\n",
    "\n",
    "其中 h 是我们可以断言查询 q 属于身份 $i^*$ 的阈值;否则，q 被确定为不包含在图库中的离群值身份，尽管 $i^*$ 是识别过程中第一个排名的身份。  \n",
    "\n",
    "在文献中，开放世界的个人身份识别仍处于早期阶段，并提出了几项工作来帮助确定这一任务。Zheng 等人设计一个系统，由几个已知身份的观察列表和包括目标和非目标个体在内的多个探测器组成。他们的工作旨在实现 high true target recognition（TTR）和 low false target recognition（FTR）率，其计算被验证为查询总数的目标身份的查询数量的比率。Liao 等人将开放世界的再识别分成两个子任务，即检测和识别；前者决定图库中是否存在 probe 身份，后者是否向接受的 probe 分配了 ID。 因此提出了两种不同的评估指标，the detection and identification rate（DIR）和 the false accept rate（FAR），基于此可以绘制 ROC 曲线。  \n",
    "\n",
    "如[172]，[173]所示，开放世界的再识别仍然是一个具有挑战性的任务，如 low false accept rate下的低识别率问题。挑战主要在于两个方面，即检测和识别，这两个方面都局限于令人满意的匹配精度 - 这是标准重识别任务的研究重点。如[173]中所示，100% FAR 对应于标准关闭式 reID，其准确率受当前技术水平的限制；由于真实匹配的召回率低，因此较低的 FAR 伴随着较低的再识别准确性。因此，从技术角度来看，关键目标是提高匹配精度，在此基础上可以针对新颖性检测（验证）方法设计概率模型。而且，当关注重新识别的准确性时，开放世界的 reID 也应该考虑图库的动态。在一个具有不断传入的边界框的动态系统中，如果确定不属于任何现有的图库标识，则将新的标识添加到“观察列表”，反之亦然。动态注册新的身份可以自动建立数据库，并通过预先组织的图库方便重新识别过程。  \n",
    "\n",
    "## 7 CONCLUDING REMARKS  \n",
    "\n",
    "在本文中，对人员重识别进行了调查。首先介绍了人员身份识别的简要历史，介绍了其在图像分类和实例检索中的异同。然后，回顾现有的基于图像和基于视频的方法，这些方法分为手工制作和深度学习系统。定位在图像分类和实例检索之间，人员重识别距离成为一个准确和高效的应用程序还有很长的路要走。因此，从以前的调查中，本文更加强调了未完善但对未来重要的可能性上，例如集成了行人检测和跟踪的端到端身份识别系统，以及在非常大的图库中的个人身份识别，我们认为这是实际系统的必要步骤。我们还强调了一些可能引起社区进一步关注的重要公开问题。它们包括解决数据量问题，reID 重新排序方法和开放 reID 系统。总而言之，鉴别特征学习，检测器/跟踪优化和高效数据结构的整合将导致成功的人员重新识别系统。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepReID: Deep Filter Pairing Neural Network for Person Re-Identification  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "人员重新识别是为了匹配不相交摄像机视图中行人检测器检测到的行人图像。挑战以照明视角，姿势，视点，模糊效果，图像分辨率，相机设置，遮挡和背景混乱等各种复杂变化的形式呈现。另外，由行人检测器引起的不对准将影响大多数现有的人员重新识别方法，其使用手动裁剪的行人图像并假设完美的检测。  \n",
    "\n",
    "在本文中，我们提出了一种新的滤波器配对神经网络（FPNN）来联合处理不对齐，光度和几何变换，遮挡和背景杂波。所有关键部件都经过共同优化，以最大限度地提高每个部件与他人合作时的强度。与使用手工特征的现有作品相比，我们的方法自动从数据中学习最适合重新识别任务的特征。学习的滤波器对编码光度变换。它的深层架构可以模拟复杂的光度和几何变换。我们构建了 13164 幅 1360 个行人的最大基准数据集。与现有仅提供手动裁剪的行人图像数据集不同，我们的数据集提供了自动检测的边界框，用于接近实际应用的评估。我们的神经网络显着优于此数据集上的最新方法。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/2/1.jpg?raw=true)\n",
    "\n",
    "人员重新识别的目的是使在非重叠照相机视图中观察到的行人的视觉特征相匹配。它在视频监控中有着重要的应用，如跨摄像机跟踪，多摄像机事件检测和行人检索。这个问题非常具有挑战性，因为由于照明，姿势，视点，图像分辨率，摄像机的光度设置和混乱的背景的大的变化，在不同摄像机视图中难以匹配拍摄行人的视觉特征。图 1 显示了一些示例。  \n",
    "\n",
    "图 2 显示了人员重新识别系统的典型 pipline。实际上，它应该从自动行人检测开始，这是从长时间录制的视频中提取行人的重要步骤。给定行人检测边界框，现有作品中都使用手动设计的特征用于表征所有的图像区域，虽然它们可能不适合做人员重新识别的任务。由于照明条件和照相机设置的改变，同一个人的图像区域经历光度变换，它们的几何变换是由错位和视点和姿势的变化造成的。这种变换可以通过学习映射函数或相似性度量来标准化，它也被认为对于遮挡和背景混乱是强健的。所有现有的工程都可以分别或顺序优化管道中的每个模块。如果前面的步骤中丢失了有用的信息，则后面无法恢复。在训练过程中建立这些组件之间的自动交互对整个系统的性能至关重要。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/2/2.jpg?raw=true)\n",
    "\n",
    "本文的贡献有三个。首先，我们提出了一种用于人员重新识别的滤波器配对神经网络（FPNN）。与现有作品相比，这种深度学习方法有几个重要的优点和新颖之处。（1）共同处理统一深度神经网络下的不对齐，光度和几何变换，遮挡和背景杂波。在训练期间，图 2 中的所有关键组件都进行了联合优化。每个组件在与他人合作时最大限度地发挥其优势。（2）除了使用手工制作的特征之外，它还可以自动学习数据重新识别任务的最佳特征，以及光度和几何变换的学习。两个配对滤镜应用于不同的相机视图以进行特征提取。（3）尽管现有作品假设交叉变换是 unimodal 的，但深层结构及其最大分组层可以模拟复杂变换的混合。  \n",
    "\n",
    "其次，我们用精心设计的训练策略训练提出的神经网络，包括损失，数据增强，数据平衡和 bootstrapping。这些策略解决了该任务中的误检，过拟合以及正面和负面训练样本极度不平衡的问题。  \n",
    "\n",
    "再次，我们重新审视人员再识别问题，建立一个引入自动行人检测之后可以进行评估的大规模数据集。所有现有的数据集都很小，这使得他们很难训练深度神经网络。我们的数据集有 1360 个行人的 13164 张图像；请参阅表 1 中的比较。现有数据集仅提供手动裁剪的行人图像，并假设评估协议中有完美的检测。如图 1 所示，实际中的自动检测会引起较大的错位，并可能严重影响现有方法的性能。我们的数据集提供了手动裁剪图像与使用最先进的检测器自动检测的边界框进行全面评估。  \n",
    "\n",
    "## 2. Related Work  \n",
    "\n",
    "许多研究旨在改善图 2 中管道的各个组成部分。现有人员重新识别系统中使用的视觉特征是手动设计的。全局特征用颜色和纹理的分布来描述视觉词的直方图。它们对于错位，姿态变化以及观点的变化有一些不变性。但是，由于失去了空间信息，他们的判别力很低。为了增加区分能力，已经使用了patch-based 局部特征。在计算两幅图像之间的相似度时，比较两个相应 patch 的视觉特征。解决错位问题时，挑战是在两个摄像机视图中匹配 patch。手工制作的特征很难实现辨别力和稳健性之间的平衡。最佳特征设计取决于摄像机视图中的光度和几何变换。例如，如果照度变化较大，则色彩空间应该以较粗糙的比例量化。如果特征设计与图 2 中的其他组件无关，则很难实现此类优化。虽然可以在后面的步骤中选择和加权特征，但如果特征池没有进行最佳设计，则性能会下降。正确的方法是与其他组件一起自动从数据中学习特征。如果没有深度学习，这很难实现。  \n",
    "\n",
    "人们可以假设光度或几何变换模型，并从训练样本中学习模型参数。例如，Prosser 等人假定光度变换是双向累积亮度传递函数，它将在一个摄像机视图中观察到的颜色映射到另一个。Porikli 通过相关矩阵分析了解了摄像机视图之间的颜色失真函数。他们假设变换是 unimodal 的。在我们提出的滤波器配对神经网络中，光度变换是通过滤波器对和 maxout grouping layer 来学习的。另一方面，几何变换是通过 patch matching 层，卷积最大化层和全连接层来学习的。提出的神经网络可以模拟复杂变换。  \n",
    "\n",
    "学习适当的距离/相似性度量可以进一步抑制交叉相机变换，遮挡和背景杂波的影响。Gray 等人和 Prosser 等分别使用 boosting 和 RankSVM 来选择特征并计算图像之间的距离。也有许多度量学习算法用于人员重新识别。图 2 中的所有组件都在现有作品中单独或顺序优化。  \n",
    "\n",
    "深度学习在解决许多计算机视觉问题方面取得了巨大的成功，包括手写数字识别，物体识别，物体检测，图像分类，场景理解和人脸识别。虽然一些深度学习作品分享共同优化视觉系统组件的精神，但他们的问题，挑战，模型和培训策略与我们完全不同。他们没有设计特殊的图层来明确处理多视图的光度和几何变换，patch matching 和背景混乱的错误检测。据我们所知，这篇论文是第一个使用深度学习进行人物识别的作品。  \n",
    "\n",
    "## 3. Model  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/2/3.jpg?raw=true)\n",
    "\n",
    "所提出的 FPNN 的架构如图 3 所示。它由六层组成，以处理未对准，多视图光度和几何变换，遮挡和背景混乱以及人员重新识别。下面介绍每层的设计。  \n",
    "\n",
    "### 3.1. Feature extraction  \n",
    "\n",
    "第一层是一个卷积和最大池化层。需要在不同摄像机视图中观察到两个行人图像 I 和 J 作为输入。他们有三个颜色通道（RGB 或 LAB），尺寸为Him×Wim。光度变换用卷积层建模，输出是由滤波器对提取的局部特征。通过对整个图像进行卷积滤波，将所有 local patches 的响应提取为局部特征。 应用于不同摄像机视图的滤镜（Wk，Vk）是成对的。如果使用 K1 个滤波器对并且每个滤波器的尺寸为 m1×m1×3，则每个图像的输出映射具有 K1 个通道并且大小为 H0×W0×K1，其中 $H_0 = H_{im}-m_1+1$ 和 $W_0 = W_{im}-m_1+1$，我们定义滤波函数 $f,g: R^{H_{im} \\times W_{im} \\times 3} \\rightarrow R^{H_{0} \\times W_{0} \\times K_1} $。 \n",
    "\n",
    "$f_{i,j}^k = \\sigma((W_k * I)_{i,j} + b_k^I)$  \n",
    "$g_{i,j}^k = \\sigma((V_k * J)_{i,j} + b_k^I)$\n",
    "\n",
    "卷积运算用 \\* 表示。使用非线性激活函数 $\\sigma(\\cdot)$ 来重新缩放线性输出并选择为 $\\sigma(x) = max(x,0)$。滤波后，每个补丁由一个 K1 通道特征向量表示。激活函数标准化和平衡不同的特征通道。自动从数据中学习滤波器对的参数 $\\{(W_k,V_k,b_k^I,b_k^J)\\}$。两个配对的滤波器代表了对人重新识别最具有区别性的相同特征。它们适用于不同的相机视图，它们的差异反映了光度变换。卷积层之后是 max-pooling，这使得这些特征对局部错位具有鲁棒性。每个特征映射被分割成 H1×W1 个子区域，每个子区域的最大响应作为输出。Max-pooling 层的输出是 H1×W1×K1 特征映射。  \n",
    "\n",
    "### 3.2. Patch matching  \n",
    "\n",
    "第二个 patch matching layer 用于匹配跨视图的 local patches 的滤波器响应。考虑到几何约束，行人图像被划分为 M 条水平条纹（图 3 中的高度因子），并且每条具有 W1 个贴片。图像 patches 仅在同一条纹内匹配。由于存在代表不同特征的 K1 滤波器对，所以 patch matching 层的输出为 $K_1MW_1 \\times W_1$ 的位移矩阵。patch matching 层的输出是  \n",
    "\n",
    "$S_{(i,j)(i',j')}^k = f_{ij}^kg_{i'j'}^k$\n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/2/4.jpg?raw=true)\n",
    "\n",
    "这些位移矩阵编码不同特征下 patch 匹配的空间模式。如图 4 所示。如果矩阵元素 $S_{(i,j)(i',j')}^k$ 具有较大值，则表明patches $(i,j)$ 和 $(i',j')$ 都具有对由滤波器对 $(W_k,V_k)$ 编码的特定特征的高响应。  \n",
    "\n",
    "### 3.3. Modeling mixture of photometric transforms  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/2/5.jpg?raw=true)\n",
    "\n",
    "由于各种视图内和视图间的变化，一个视觉特征（例如红色衣服）可能经历多次光度变换。为了提高 patch matching 的鲁棒性，增加了一个 maxout-grouping 层。K1 个特征通道的 patch 位移矩阵被分成 T 个组。在每个组中，只有最大激活被传递到下一层。这样，每个特征都由多个冗余通道表示。它允许建模混合的光度变换情况。在训练过程中，使用反向传播算法，只有具有最大响应的滤波器对才能接收梯度并进行更新。它驱动同一组中的过滤器对来竞争梯度。最终，只有一个过滤器对训练样本有很大的响应。因此，图像 patch 与学习的 filter 对具有稀疏的响应。众所周知，稀疏性有消除噪声和冗余的特性。maxout grouping 层的输出是 TMW1×W1 位移矩阵。如图 5 所示  \n",
    "\n",
    "### 3.4. Modeling part displacement  \n",
    "\n",
    "身体部位可以被视为相邻的 patches。在 patches 位移矩阵的顶部添加另一个 convolution and max-pooling layer，以更大尺度地获得身体部位的位移矩阵。它将 MTW1×W1 patches 位移矩阵作为输入，并将其作为具有 T 通道的 MW1×W1 图像。与第一个卷积层类似，对所有 M 个图像都应用 $K_2m_2×m_2×T$ 滤波器，该层的输出为 $MW_2×W_2×K_2$ 的特征图。学习的 filters 捕获部分位移的局部模式。  \n",
    "\n",
    "### 3.5. Modeling pose and viewpoint transforms  \n",
    "\n",
    "行人具有各种姿势和视点变换。这种全局几何变换可以看作局部位移的不同组合，它们的分布是多模式的。例如，两个转换可以在上半身共享相同的位移，但腿的位移不同。卷积和 maxpooling 层中的隐藏节点的每个输出可以被视为用特定的视觉特征检测到的可能的部分位移。所有这些隐藏节点构成下一个全连接层的输入向量。在下一层中，每个隐藏节点是所有可能的部分位移的组合，并且表示全局几何变换。N 个隐藏节点能够模拟全局的几何变换。  \n",
    "\n",
    "### 3.6. Identity Recognition  \n",
    "\n",
    "最后一个 softmax 图层使用 softmax 函数来测量两个输入图像是否属于同一个人，或者没有给出在前一图层中检测到的全局几何变换。它的输出是一个定义的二元变量 y。  \n",
    "\n",
    "如果两个行人图像 $(I_n,J_n)$ 匹配，令 y = 1，否则 y = 0。x 是来自前一层的输入, a0，a1，b0 和 b1 是要学习的组合权重和偏差项。给定 H 训练样本对的类别标签，负对数似然值用作训练 cost。\n",
    "\n",
    "$cost = - \\sum_n^H y_n \\log (p(y=1|\\Phi,(I_n,J_n))) + (1-y_n) \\log (1 - p(y=1|\\Phi,(I_n,J_n)))$\n",
    "\n",
    "它对错误分类的样品施加很大的惩罚。例如，如果 $y_n = 0$ 并且 $p (y=1|\\Phi,(I_n,J_n)) = 1$，$(1-y_n) \\log (1 - p(y=1|\\Phi,(I_n,J_n))) \\rightarrow - \\infinit$。 Φ表示要学习的整个神经网络的参数集合。  \n",
    "\n",
    "## 4. Training Strategies  \n",
    "\n",
    "我们的训练算法采用小批量随机梯度下降。训练数据划分为小批量。训练误差是在 soft-max 层中的每个小批量计算得到的，并且反推到较低层。此外，还提出了几种精心设计的训练策略。  \n",
    "\n",
    "### 4.1. Dropout  \n",
    "\n",
    "由于大量的交叉视图变化，错位，姿态变化和遮挡，个人重识别可能导致同一人（但在不同视图中）的某些 patches 不匹配。为了使训练好的 FPNN 可以容忍 patch 的错误检测，采用了 dropout 策略。对于每个训练样本作为每次训练迭代的输入，第一卷积层的一些输出被随机选择并设置为零，然后对经过 dropout 之后的特征图计算反向传播梯度，从而使模型更加鲁棒。  \n",
    "\n",
    "### 4.2. Data Augmentation  \n",
    "\n",
    "在训练集中，匹配的样本对（正样本）比不匹配的对（负样本）少几个数量级 如果他们直接用于训练，网络倾向于预测所有输入不匹配。我们通对每个行人图像进行简单的平移变换来增加数据。对于尺寸为 Him×Wim 的原始行人图像，在原始图像中心周围随机采样 5 幅相同大小的图像，匹配的样本对被放大25倍。  \n",
    "\n",
    "### 4.3. Data balancing  \n",
    "\n",
    "每个小批量保留所有正面训练样本，并在训练过程的最初阶段随机选择相同数量的负面训练样本。该网络在初始训练后达到了合理的良好配置。随着训练过程的进行，我们逐渐增加每个小批量中的负样品数量，使其达到 5：1 的比例。  \n",
    "\n",
    "### 4.4. Bootstrapping  \n",
    "\n",
    "在网络稳定后，我们继续选择 difficult negative samples，他们有很大概率被预测为同一个人，并将它们与所有正样本进行组合以迭代地进一步训练网络。由于存在大量的负面训练样本，在每个 epoch 后重新预测当前网络的所有负样本是非常耗时的。我们只重新预测在上一个 epoch 选择的困难样本。 由于这些样本已用于更新网络，因此预计其更新后的预测比其他样本的变化更大。  \n",
    "\n",
    "在每个 epoch k 之后，每个负样本 x 被赋予一个分数 sk。选择具有最小 sk 的样本重新训练网络。一开始，  \n",
    "\n",
    "$s_0 = 1 - p(x is a matched pair | \\Phi_0 )$\n",
    "\n",
    "其中 $\\Phi_0$ 是网络的配置。如果在前一个 epoch k 中选择 x 作为训练的困难样本，则其分数更新为  \n",
    "\n",
    "$s_k = \\frac{1 − p(x is a matched pair|\\Phi_k) + s_{k-1}}{2}$\n",
    "\n",
    "其中 $\\Phi_k$ 是在 epoch k 之后训练的网络的配置；$s_k = \\lambda s_{k-1}$。递减参数 $\\lambda$ 被设置为0.99。这增加了这些负样品很长时间不被选择的机会。  \n",
    "\n",
    "## 5. Dataset  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/2/6.jpg?raw=true)\n",
    "\n",
    "所有现有的数据集都太小而无法训练深度神经网络。我们构建了一个更大的数据集，其中包括 1360个行人的 13164 个图像。它被命名为 CUHK03，因为我们在以前的作品中已经发布了两个 re-id 数据集（CUHK01 和CUHK02）。表 1 中列出了这些数据集规模的比较。整个数据集由六台监视摄像机拍摄。每个身份由两个不相交的相机视图观察，并且在每个视图中平均具有 4.8 个图像。图 1（a）显示了一些示例。除了规模外，它还有以下特点  \n",
    "\n",
    "（1）除了手动裁剪的行人图像之外，我们还提供用最先进的行人检测器检测的样本。这是一个更现实的设置，并带来现有数据集中罕见的新问题。从图1（a）我们可以看出，这个数据集中失配，遮挡和身体部位缺失是相当普遍的。不准确的检测也使得几何变换变得复杂。我们进一步提供原始图像帧，研究人员可以在这个数据集上尝试自己的检测器。  \n",
    "\n",
    "（2）一些现有的数据集假设一对摄像机视图，并且它们的交叉视图变换相对简单。在我们的数据集中，从多对相机视图中收集的样本都是混合的，它们形成复杂的 crossview 变换。此外，我们的摄像头监控行人向不同方向行走的开放区域，从而导致即使在同一对摄像头之间进行多次视图变换。  \n",
    "\n",
    "（3）图像是从几个月内录制的一系列视频中获取的。即使在单个相机视图内，照明变化也是由天气，太阳方向和阴影分布引起的。我们的相机具有不同的设置，这也导致了光度变换。  \n",
    "\n",
    "## 6. Experimental Results  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/2/7.jpg?raw=true)\n",
    "\n",
    "大多数评估是在新数据集上进行的，因为现有数据集太小而无法训练深度模型，在CUHK01 也进行了评估。我们的数据集被分为训练集（1160人），验证集（100人）和测试集（100人）。每个人每个视图大约有 4.8 张照片，这意味着在数据增加之前，将有近 26000 个正样本训练对。mini-batch 包含 512 个图像对。因此，需要大约 300 个 mini-batch 才能过一遍训练集。验证集用于设计网络体系结构（参数如表 2 所示）。实验进行 20 次随机分割，所有累积匹配特征（CMC）曲线均为单次结果。  \n",
    "\n",
    "每个图像都使用直方图均衡进行预处理并转换为LAB色彩空间。它被标准化为（64×32×3）的大小，并且与该位置中的所有像素的平均值相减。我们的算法是用 GTX670 GPU 实现的。训练过程大约需要五个小时才能完成。  \n",
    "\n",
    "我们比较了三种重识别方法（KISSME，eSDC 和 SDALF），四种最先进的度量学习方法（信息理论度量学习（ITML）），Logistic 距离度量学习（LDM），最大边缘最近邻（LMNN）和 Metric Learning to Rank（RANK）），并直接使用欧几里得距离来比较特征。LMNN 和 ITML 是广泛使用的度量学习算法，并已被用于人员重识别。RANK 针对排序问题进行了优化，而人员重识别是排序问题。LDM 专门用于面部和人员识别问题。当使用度量学习方法和欧几里得距离，采用稠密颜色直方图和均匀采样补丁的密集 SIFT 的手工特征。通过广泛的实验评估，已经表明，这些局部特征比其他大多数特征对人员重识别更有效，并且公开可用。  \n",
    "\n",
    "### 6.1. Experiments on our new dataset  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/2/8.jpg?raw=true)\n",
    "\n",
    "在我们的 CUHK03 数据集上，我们使用手动标记的行人边界框和自动检测的边界框进行比较。图6（a）绘制使用手动标记边界框的 CMC 曲线。我们的 FPNN 优于所有方法。与最佳表现方法相比，Rank-1 识别率的相对改善程度为 46％。  \n",
    "\n",
    "图6（b）显示了使用自动检测的边界框的结果，这会导致未对齐。其他方法的性能显着下降。例如，表现最佳的 KISSME 的 Rank-1 识别率下降了 2.47％，而 FPNN 仅下降了 0.76％。它表明 FPNN 对未对准更加鲁棒。  \n",
    "\n",
    "为了比较不同学习方法的学习能力和泛化能力，我们做了另一项实验，在训练集中添加了 107 个行人的 933 个图像，同时保持测试集不变。因此，训练集有 1267人。这些额外的 933 个图像是从与测试集中不同的四个摄像机视图中捕获的。添加与测试集数据的光度和几何变换都不匹配的训练样本会使学习变得更加困难。图 6（c）显示了不同方法的 Rank-1 识别率的变化。据观察，大多数方法的性能下降，因为它们的有限学习能力不能有效地处理更复杂的训练集以及训练和测试集之间的不匹配。相反，由于 FPNN 的学习能力很大，并且额外的训练样本改善了可以由不同摄像机设置共享的学习低级特征，因此FPNN 的性能得到了提高。  \n",
    "\n",
    "### 6.2. Evaluation of training strategies  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/2/9.jpg?raw=true)\n",
    "\n",
    "图7（a）和（b）中的实验显示了我们的 dropout and bootstrapping 训练策略的有效性。图7（a）显示验证集合上不同数量的 mini-batches 之后的 Rank-1 识别率，dropout rates 从 0％ 到 20％ 不等。没有 dropout rates，识别率会随着更多的 mini-batches 而降低。这表明过拟合发生。dropout rates 为 5％ 时，识别率很高，并收敛于验证集。dropout 使得训练好的 FPNN 可以容忍错误检测 patches ，并具有良好的泛化能力。如果 dropout rates 很高（例如20％2），即使泛化能力很好，它也不能达到很好的识别率，因为没有足够的特征传递到下一层。  \n",
    "\n",
    "图7（b）显示了有和没有 bootstrapping 策略的 FPNN 的 CMC 曲线。Bootstrapping 有效地将 Rank-1 识别率从 15.66％ 提高到 19.89％。然而，Rank-20 的差异较小。这可能是由于 Rank-20 后错过的样本特别困难，而 FPNN 已经放弃了适应这些极端情况以保持稳健。  \n",
    "\n",
    "### 6.3. Experiments on the CUHK01 dataset  \n",
    "\n",
    "我们在发布的 CUHK01 数据集上进一步评估 FPNN。在这个数据集中，有 971 个人，每个人在任一相机视图中只有两个图像。再次选择 100 人进行测试，其余 871 人进行训练和验证。这个数据集对我们的方法是具有挑战性的，因为少量的样本不能很好地训练深度模型。其上只有 3000 对正训练样本（与我们新数据集中的 26000 个样本相比）。尽管如此，我们的 FPNN 比较了大多数方法，Rank-1 比只略低于 KISSME，但是它的 Rank-n（n> 10）率与 KISSME 相当。  \n",
    "\n",
    "## 7. Conclusion  \n",
    "\n",
    "在本文中，我们提出了一种新的用于员重识别的滤波器配对神经网络。该方法在统一的深层架构下联合优化特征学习，光度变换，几何变换，未对齐，遮挡和分类。它学习滤波器对来编码光度变换。它的学习能力可以模拟复杂的光度和几何变换的混合。采用一些有效的训练训策略对网络进行良好的训练。它在大型基准数据集上的表现优于最先进的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-Person: Learning Discriminative Deep Features for Person Re-Identification  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "最近，许多人重识别方法（ReID）依靠 part-based 特征表示来学习具有区分性的行人描述符。然而，这些 part 之间的空间上下文关系被忽略了。本文中，我们建议应用长短时记忆网络（LSTM）以端对端的方式对行人进行建模，从头部到脚部将其视为一系列身体部位。整合背景信息增强了局部表达的区分能力。我们还利用局部和全局特征之间的互补信息。此外，我们将识别任务和排序任务都集成在一个网络中，其中同时学习判别嵌入和相似性测量。这导致了一种名为 DeepPerson 的新型三分支框架，该框架学习人员重识别的高度区分性特征。实验结果表明，DeepPerson 在三个具有挑战性的数据集上（包括Market-1501，CUHK03 和 DukeMTMC-reID）大幅超越了最先进的方法。具体而言，结合重新排序的方法，我们在单一查询设置下实现 Market-1501 上90.84% 的 mAP。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "人员重识别（Re-ID）是指跨多个非重叠摄像机匹配特定人员的任务。由于其各种监视应用，它在计算机视觉领域一直受到越来越多的关注。尽管经过数十年对人身份识别任务的研究，但由于照明，姿势，背景混乱，遮挡和视觉外观模糊等方面的巨大差异，它仍然非常具有挑战性。  \n",
    "\n",
    "大多数早期的人物 Re-ID 作品着重于区分手工特征表示或用于相似性测量的鲁棒距离度量。受益于深度学习和增加大规模数据集的发展，最近的 person Re-ID 方法将特征提取和距离度量结合到端到端深度卷积神经网络（CNN）中。尽管如此，最新的基于 CNN 的方法试图设计更好的特征表示或开发更强大的特征学习，但很少将这两个方面结合在一起。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/3/1.jpg?raw=true)\n",
    "\n",
    "基于 CNN 的方法专注于更好的特征表示可以大致分为三类：1）全局全身表示，这在很多方法中都被采用；2）局部身体部分表示，它在很多作品中都有变体的使用。在许多作品中使用了直接分割成预定义的刚体部分。实际上，如图 1 所示，由于姿态变化，不完美的行人检测器和遮挡，这种简单的分区不能正确对齐两个行人图像，导致 part-based 特征不够鲁棒。最近的一些作品试图用一些复杂的方法来开发更好的身体分割或使用额外的姿势标注。所有这些方法都会忽略身体各部分之间的上下文信息。在[33]中，作者建议首先将原始人物图像转换为连续的 LOMO 和颜色名称特征，然后依靠递归神经网络（RNN）对空间背景进行建模。然而，序列特征提取和空间上下文建模的分离阻碍了端到端的训练和优化，导致性能下降；3）全局和局部表示的组合，它将全局特征和部分特征连接为最终特征表示。与单个分支模型相比，这种组合的特征表示通常需要更多的计算和测试阶段额外的空间，因为与单个分支模型相比，额外的分支会在实践中产生较慢的运行时间。  \n",
    "\n",
    "专用于强健特征学习的方法通常将个人 Re-ID 问题视为分类任务或排序任务。由于最近增加了大规模的 Re-ID 数据集，在许多作品中，人员 Re-ID 被认为是一个多类人员识别任务。所获得的由倒数第二个全连接层给出的 ID-discriminative Embedding (IDE) 已经显示出人员 Re-ID 的巨大潜力。然而，识别任务的训练目标与测试程序并不完全一致。这种学习的 IDE 对于训练集上的识别模型是最佳的，但是在测试阶段描述看不见的人的图像可能不是最佳的。此外，识别任务在图像检索的测试阶段并没有明确的学习到一个相似性度量函数。另一方面，排序任务旨在使正对之间的距离比负对之间的距离更接近给定的阈值。因此，需要明确学习一个度量函数。然而，Re-ID 标注的所有身份信息未被充分利用。最近，一些作品利用 triplet loss 在分类和排序任务中来学习更多的人的可区分性特征。  \n",
    "\n",
    "在本文中，我们建议将行人模拟为从头到脚的一系列身体部位，并且将所有部位特征与空间上下文信息一起学习，而不是对每个单独部位使用独立分支。为此，我们以端到端的方式应用长短时记忆网络（LSTM），通过事先了解身体结构来增强局部特征的判别能力，并减轻由分离的特征提取器引起的未对准。我们在建议的 Deep-Person 模型中也采用 global fullbody 表示。我们将全局和 part-based 特征提供给两个独立的网络分支以进行识别任务。与全局和局部表示的经典组合不同，它将全局特征和 part-based 特征的特征连接起来作为 Re-ID 的最终特征表示，我们进一步添加一个使用 triplet loss 的排序任务分支来明确学习相似性度量。更具体地说，我们使用骨干特征 $f_b$ 的 global average pooled $f_m$（如图 2 所示）作为相似性估计，也被认为是最终的行人描述符。这种三分支 Deep-Person 模型为 person Re-ID 学习高判别性特征。  \n",
    "\n",
    "本文的主要贡献有三个方面：1）我们建议将行人视为从头到脚的一系列身体部位，并以端对端的方式应用 LSTM 以提高局部特征的判别能力；2）我们开发了一种新颖的三分支框架，它利用两种互补优势：局部身体部分和全局全身特征表示，以及识别任务和排序任务，从而实现更好的特征学习。提议的 Deep-Person 对于人员 Re-ID 产生高度区别性的特征； 3）在测试阶段，提议的 Deep-Person 只执行骨干网络的正向传递，然后是 global average pooling。 因此，与单分支模型相比，我们的模型在测试过程中不需要额外的运行时间和空间，而在三个流行的 Re-ID 数据集上仍然优于最先进的方法。  \n",
    "\n",
    "## 2. Related Work  \n",
    "\n",
    "我们关注两种与人类 Re-ID 紧密相关的深度学习方法：依赖于 part-based 的表示方法和专注于 multi-loss 学习的方法。有关 person Re-ID 方法的完整回顾，感兴趣的读者可参考[44]。  \n",
    "\n",
    "**Part-based person Re-ID approaches** 有一些方法使用 part-based 的表示来学习人员重识别的区分性特征。按照 part 分割的策略，part-based 可以粗略地分为两类：1）刚体部分分割法，在许多方法中被广泛采用，作者使用预定义的 rigid grids 作为局部区域。每个部分都被送入一个单独的分支。然后将所有单独的局部特征连接在一起，作为最终的 part-based 的表示；2）灵活的身体局部分区，这是定位身体局部区域更合理的方法。例如，Yao 等人使用无监督方法生成一组 part boxes，然后使用 RoI pooling 来生成 part 特征。具有新颖空间约束的空间变换网络（STN）被用于定位可变性人体区域。有了额外的姿势标注，Zhao 等人利用学习的身体关节获得子区域边界框。Su 等人通过将 pose based parts 归一化为固定的大小和方向来进一步扩展，并引入姿态变换网络（PTN）来消除姿态变化。  \n",
    "\n",
    "虽然刚体部分易于实现，但由于遮挡和不准确的行人检测，因此刚体部分的 misalignment（见图1）对于区分性特征学习不具有优势。基于灵活的身体部位的表示在一定程度上通过定位适当的部分来缓解错位问题。然而，这样的方法通常具有更复杂的过程或需要额外的先验知识（例如，人体姿势）。此外，现有基于刚性或灵活性身体部位的方法都忽略了身体部位之间的关系。而行人总是可以从头到脚分解成一系列身体部位。基于行人的这一简单但重要的属性，我们建议以序列方式一起学习所有部分，而不是通过独立部分特征提取器来丢弃空间上下文。为此，我们自然将 LSTM 应用于序列级建模。LSTM 也用于模拟空间上下文信息。然而，作者首先将输入图像划分为刚性部分，并为每个单独部分提取手工特征，然后使用 LSTM 在单独的步骤中对空间关系进行建模。所提出的 Deep-Person 对深度特征提取和序列建模以端到端的方式联合到一起，从而为 person Re-ID 带来更多的区分性特征。  \n",
    "\n",
    "**Approaches based on joint multi-loss learning** 最近，Zheng 等人提出 person Re-ID 位于实例检索任务和图像分类任务之间。对于第一点，Zheng Re-ID 被视为排序任务，其中排序损失被用于特征学习。在[8]中，一个新术语被添加到原始 triplet loss 中，以进一步拉近同一个人的距离。Hermans 等人在 batch 中引入使用 hard mining 的方式引入了标准 triplet loss 的变体。从分类任务的角度来看，通常用 Softmax loss 来解决 Re-ID 问题。有两种方法可以将人员重识别作为分类任务。第一个被称为验证网络，它将一对图像作为输入，并通过二元分类网络确定它们是否属于相同的身份。第二个被称为识别网络，即多类别识别网络，其中每个个体被认为是一个独立的类别。  \n",
    "\n",
    "分类任务和排序任务是相辅相成的。有些方法同时利用这两种损失函数优化网络。例如，在[5,34]中，triplet loss 和 verification loss 一起训练。 识别损失和验证损失同时在[11,24]中优化。提出的 Deep-Person 同样也使用了triplet loss 和 verification loss。与[22]不同的是，用于 part based\n",
    "表征的验证损失也用来学习全局特征，进一步提高了学习特征对于 person Re-ID 的识别能力。  \n",
    "\n",
    "## 3. Architecture of Deep-Person  \n",
    "\n",
    "### 3.1. Overview of Deep-Person  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/3/2.jpg?raw=true)\n",
    "\n",
    "所提出的 Deep-Person 模型着重于特征表示和特征学习。它基于以下两种互补设计：1）全局特征表达和局部特征表达；2）基于 Softmax 的识别分支和具有 triplet loss. 的排序分支。  \n",
    "\n",
    "人类再识别方面的最新进展依赖于深度学习，以便从检测到的行人中学习区分性特征。正如[17]中指出的那样，学习到的全身特征表示更多地关注全局信息，例如姿态和形状。而在某些情况下，只有特定的身体部位，如头部，上身或下身才能区分人的身份。从这个意义上讲，检测到的行人的 part-based 特征表征与全局表示是互补的。我们建议使用基于 LSTM 的 RNN 来自然地模拟行人体各部分之间的空间依赖性，这可以看作是从头到脚的一系列身体部位。 希望将互补的全局表示和基于 LSTM 的局部表示相结合，可以增强学习特征对人员重识别的判别能力。  \n",
    "\n",
    "最近，大多数 person Re-ID 深度模型的一个共同组成部分是基于 Softmax 的识别分支，该分支根据学习的深度特征区分不同的 ID。然而，识别分支的训练目标与 person Re-ID 的目标并不完全一致，该目标旨在将每个检索图像与一个图库图像进行配对。这是因为识别分支没有明确地学习人员 Re-ID 所需的相似性测量。最近，如[22]所提倡的，具有 triplet loss 的距离排序分支有助于学习一对图像之间的相似性度量。从这个意义上说，识别分支和基于triplet loss 的距离排序分支构成了另一个互补性。  \n",
    "\n",
    "我们提出的 Deep-Person 模型利用了上述两种互补。整体架构如图 2 所示。它由两个主要部分组成：（1）用于学习大小为 H×W×C 的共享低级特征 $f_b$ 的主干网；（2）多分支网络通过三个互补分支学习高度区分性的行人特征： partbased 识别分支 $B_p^I$，global-based 识别分支 $B_g^I$ 以及使用 triplet loss 的距离排序分支 $B_t^R$。采用联合学习策略同时优化每个分支特征表示并发现相关补充信息。  \n",
    "\n",
    "### 3.2. Part-Sequence Learning Using LSTM  \n",
    "\n",
    "在许多方法中已经表明，基于部分的表示对于人员 Re-ID 是有用的。大多数 part based 的方法粗略地将提取的行人分解为预定义的刚体部分（参见图1），其分别大致对应于头部，肩部，上身，上腿和下腿。然后将每个分段部分馈送到单个分支中以学习相应的局部特征。这在某些情况下可能会产生有趣的结果。然而，每个部分的个体过程忽略了不同部分之间的空间依赖关系，这种关系对于学习 person Re-ID 的区分特征很有用。此外，由于姿态变化，不准确的行人检测和/或遮挡，这样的分区会导致同一个人在不同图像中的潜在的 misalignment。图 1 给出了一些例子。图1（a）中左侧图像的上半部分与右侧图像的头部部分对齐。  \n",
    "\n",
    "我们注意到图像中的行人可以从头到脚分解成一系列身体部位。尽管每个部分并不总是位于不同图像中的相同位置，但由于身体结构的先验知识，所有的行人部分都可以按照顺序进行建模。行人的顺序表现自然会激励我们诉诸于带有 LSTM 单元的 RNN。事实上，带有 LSTM 单元的 RNN 已被广泛应用于许多基于序列的问题中，如图像标题，机器翻译，语音识别等。LSTM 在图像分类和对象检测中也表现出很高的潜力，其中 LSTM 建模空间依赖性并捕获更丰富的上下文信息。受益于内部门控机制，LSTM 可以控制从当前状态到下一个状态的信息流。因此，LSTM 单元能够传播某些相关的上下文并过滤出一些不相关的部分。基于上述见解，我们建议采用 LSTM 来模拟 person ReID 的身体部位序列。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/3/3.jpg?raw=true)\n",
    "\n",
    "更具体地说，为了获得空间上下文，我们直接从共享的低级特征 $f_b$ 中提取特征向量序列，而没有任何明确的分割。如图 2 所示，$f_b$ 的每一行经历average pooling，生成长度为 C 的特征向量。如图 3 所示，每个特征向量都描述了原始图像中的一个矩形区域。然后在特征序列上建立双层双向 LSTM。 由于 LSTM 的上下文建模，每个长度为 U 的结果特征向量可以更好地描述其相关部分。最后，表征底层局部区域的所有结果特征向量被连接在一起，作为最终的 part-based 人员表示。这种 part-based 的特征是通过带有 $N_c$ 个输出神经元的 Softmax 层学习的，其中 $N_c$ 表示行人身份的数量。  \n",
    "\n",
    "### 3.3. Global Representation Learning  \n",
    "\n",
    "Part-based 特征更注重行人可区分性的细节。仅使用 part-based 表示来区分具有非常相似的视觉细节的两个不同身份（例如，穿着相同的衣服）是困难的。在这种情况下，需要姿态和/或形状信息来区分它们。事实上，全局特征是对 part-based 局部特征的补充，并且包含更多的高级语义，比如姿态和形状。与 person Re-ID 的许多深度神经网络类似，我们直接通过在共享低级特征 $f_b$ 之后插入 global average pooling 和全连接层来提取全局表示。 然后附加一个带 $N_c$ 个神经元的 Softmax 层，用于全局特征学习。  \n",
    "\n",
    "### 3.4. Deep Metric Learning with Triplet Loss  \n",
    "\n",
    "在第二部分描述的 part-based 和全局表征学习没有明确学习测试期间人员 Re-ID 所需的相似性度量。我们建议在 Deep-Person 模型中加入一个负责距离排序的第三个分支。为此，我们将另一个独立的 global average pooling 应用于共享的低级特征 $f_b$，这导致度量空间中的特征 $f_m$ 用于相似性估计。该特征也被用作 person Re-ID 的最终描述符。  \n",
    "\n",
    "更具体地说，我们采用[14]中提出的改进的 triplet loss 来进行深度学习。其主要思想在于通过对 P 类（个人身份）进行随机抽样来形成称为 PK 抽样的 batch，然后随机抽样每个类（人）的 K 个图像，从而产生一批 PK 图像。给定这样一个 mini-batch $X = \\{x_i\\}_{i=1}^PK$，我们可以为每个选定的 anchor 图像 xi 组成一个 triplet $T_i = \\{x_i^0,x_i^+,x_i^-\\}$。由于 hard triplet mining 策略对于基于 triplet loss 的学习至关重要，因此每个 anchor 样本仅选择最小的最困难的正负样本以形成三元组进行损失计算：  \n",
    "\n",
    "$L_{trp} = \\frac {1}{PK} \\sum_{i=1}^{PK} [m + {max}_+ + D(f_m(x_i^0),f_m(x_i^+)) - {min}_- D(f_m(x_i^0),f_m(x_i^-))]_+$\n",
    "\n",
    "其中$[\\cdot]_+ = max(\\cdot,0)$，m 是在正负对之间强制执行的余量，$D(a,b)$ 被称为特征向量 a 和 b 之间的距离函数。本文中，我们使用欧几里得距离作为距离度量。这个目标函数鼓励在学习特征空间中的正对的特征比负对在预定边界 m 处更小。  \n",
    "\n",
    "### 3.5. Model Training  \n",
    "\n",
    "正如在第二节中讨论的，不同分支机构对学习区分性行人描述符具有互补的优势。为了利用这些互补性，我们联合训练整个网络以预测行人身份， 同时满足 part-based 和全局特征学习以及triplet 目标。对于多类别人员识别任务，有两个用 Softmax 损失构建的识别子网络，定义为：  \n",
    "\n",
    "$L_{cls} = - \\frac{1}{N} \\sum_{i=1}^N \\log \\frac{\\exp(W_{y_i}^T f_i + b_{y_i})} {\\sum_{j+1}^{N_c} \\ \\exp(W_j^T f_i + b_j)}$\n",
    "\n",
    "其中 $f_i$ 是第 i 个样本的分类特征，$y_i$ 是第 i 个样本的身份，N 是样本数量，$W_j$ 和 $b_j$ 分别是第 j 个身份的分类器的权重和偏差。为了简化符号并没有任何含糊性，我们用相应的 part-based（global-based）分类损失函数 $L_{cls\\_p}$（$L_{cls\\_g}$）替换 $f_p$（$f_g$）。然后最后的损失是由以下给出的：  \n",
    "\n",
    "$L = \\lambda_1 L_{trp} + \\lambda_2 L_{cls\\_p} + \\lambda_3 L_{cls\\_g}$\n",
    "\n",
    "其中 $\\lambda_i$ 表示不同分支的损失权重。  \n",
    "\n",
    "## 4. Experiments  \n",
    "\n",
    "### 4.1. Implementation Details  \n",
    "\n",
    "建议的 Deep-Person 模型建立在 PyTorch 框架之上。骨干网络是在 ImageNet 上预先训练的 ResNet-50 模型，其中 global average pooling 和全连接的层被丢弃。$f_b$ 的通道数C（见图2）设置为 2048，BLSTM 中隐藏单元的数量 U 设置为 256。  \n",
    "\n",
    "我们遵循具有不同比例和长宽比的常用数据集增强策略来训练 Deep-Person 模型。具体来说，所有的训练图像首先调整大小为 256×128。然后，我们随机地按照 [0.64,1.0] 中的缩放比例和 [2,3] 中的宽高比随机裁剪每个调整大小的图像。最后，裁剪后的 patch 再次调整到 256×128。随机水平翻转的概率为 0.5 也适用。在测试阶段，图像被简单地调整为 256×128。距离排序分支的输入 fm 被用作用于检索的行人描述符。在将输入图像馈送到网络之前，我们通过减去平均值然后除以标准偏差来归一化图像。  \n",
    "\n",
    "如 3.4 所述，PK 采样用于形成 batch，其中 P 和 K 分别表示类的数量和实例的数量。为了训练我们的 Deep-Person 模型，我们重新定义了 epoch 的概念，以便每个 $[N_c/P]$ 训练过批次形成一个 epoch。这种重新定义意味着 epoch 几乎涵盖了所有的身份。在训练阶段开始时，联合训练这样的网络并不是微不足道的。我们应用 gradient clipping 来避免梯度爆炸。  \n",
    "\n",
    "### 4.2. Datasets and Evaluation Protocol  \n",
    "\n",
    "**数据集** 我们在三个广泛使用的大规模数据集上评估我们提出的方法 DeepPerson：Market-1501，CUHK03 和 DukeMTMC-reID 数据集。简要说明如下： \n",
    "\n",
    "Market-1501：该数据集由超市前面六台摄像机捕获的 1506 个身份的 32,668 张图像组成。提供的行人边界框由 DPM 给出，并通过手动标注进一步验证。 该数据集训练组包含 751 个身份的 12,936 个图像，测试组包含剩下的750个身份的 19,732个图像组成。对于测试集中的每个身份，仅选择每个摄像头的一个图像作为查询，从而总共获得 3,368 个查询图像。  \n",
    "\n",
    "CUHK03：此数据集包含 14,097 张由六部摄影机在中大校园拍摄的 1,467 个身份的图像。它提供两种类型的注释：手动标记的行人边界框和由 DPM 检测器给出的自动检测。我们分别对两种标注数据集进行了实验，分别命名为标记数据集和检测数据集。该数据集还提供 20 个随机分组，每个分组选择 100 个测试身份，并使用其他 1367 个人进行训练。报告所有 split 的平均性能用于对此数据集进行评估。  \n",
    "\n",
    "DukeMMC-reID：该数据集是 DukeMTMC 的一个子集，用于基于图像的重新识别，格式为 Market-1501 数据集格式。它由 8 个高分辨率相机拍摄的 1,812 个不同身份的 36,411 张图像组成，其中 1,404 个身份标识出现在两个以上的相机中，其他 408 个身份标识被视为干扰标识。在 1404 个身份中，使用了702 个身份的 16,522 个图像进行训练，其他 702 个身份分为 2,228 个查询图像和 17,661 个图库图像。  \n",
    "\n",
    "**评估协议** 我们遵循标准评估协议。具体来说，我们采用 rank-1 的累积匹配特征（CMC）和 mean average precision （mAP）来评估在 Market-1501 和 DukeMTMC-reID 数据集上的性能。CUHK03 报告了 rank-1，rank-5 和 rank-10 的精度。与大部分相关工作一样，对 CHUK03 和 DukeMTMC-reID 的评估是在单个查询条件下进行的。 Market-1501 数据集都使用单个和多个查询条件。  \n",
    "\n",
    "### 4.3. Comparison with Related Methods  \n",
    "\n",
    "我们将所提出的方法 Deep-Person 在 Market-1501，CUHK03 和 DukeMMC-reID 数据集中与最新方法进行了比较。提议的 Deep-Person 在所有三个数据集上始终优于最先进的方法。详情如下：  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/3/4.jpg?raw=true)\n",
    "\n",
    "**Evaluation on Market-1501** 如表 1 所示，提议的 Deep-Person 在 Market-1501 数据集上获得了令人信服的结果。具体来说，我们的 Deep-Person 在单一查询模式下显着提高了最先进的结果大约 7% 的 mAP 和 3.7% 的 rank-1 匹配率。结合有效的重新排序方法，表现进一步提升，达到90.84%。据我们所知，这是首次在 Market-1501 数据集上使用单一查询条件实现了高于 90％ 的 MAP。我们还观察到使用此数据集上的多个查询条件可以获得类似的性能改进， mAP 和 rank-1 分别获得 4.7% 和 2.3% 的提升。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/3/5.jpg?raw=true)\n",
    "\n",
    "**Evaluation on CUHK03** 表 2 中给出了对于 CUHK03 数据集上的 Deep-Person 的评估，它们的 rank-1, rank-5, and rank-10 匹配率在表 2 中给出。使用手动注释的行人边界框，我们 DeepPerson 的 rank-1 精度提高 2.8%。使用自动方法 DPM 提取行人时，实现了 4.8% 的提升。后面的设置与实际应用相一致，这证明了 Deep-Person 在实践中的潜力和稳健性。Deep-Person 在 rank-5, and rank-10 匹配率下也优于现有技术方法。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/3/6.jpg?raw=true)\n",
    "\n",
    "**Evaluation on DukeMTMC-reID** 表 3 描述了与 DukeMMC-reID 数据集的最先进方法的比较。我们的 Deep-Person 也超越了所有最先进的方法，实现了1.7% 的 rank-1 准确性提高和 4.2% mAP 提高。值得注意的是，先前最先进的 DPFL 将多尺度人物图像作为输入。采用这种设计，性能预计会进一步提高。  \n",
    "\n",
    "### 4.4. Ablation Study  \n",
    "\n",
    "我们进一步评估 Deep-Person 的几个变体，以验证每个组件的有效性。在不失一般性的情况下，消融研究是在单个查询设置下使用 Market-1501 数据集进行的，使用的是第 4.1 节中所述的相同设置。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/3/8.jpg?raw=true)\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/3/7.jpg?raw=true)\n",
    "\n",
    "**Effectiveness of LSTM-based Parts** 拟议的 Deep-Person 利用全局表征与基于 LSTM 的新型局部表征之间的互补信息。我们评估这种互补优势的贡献以及基于 LSTM 的新型局部表征的效果。为此，我们放弃 Deep-Person 中的排序分支。如表 4 所示，全局和局部分支的组合优于单独的分支结构。此外，采用的 LSTM 在 mAP 中的性能提升达到 3.1%。为了获得关于改进的一些见解，我们分析了 Deep-Person 三个变体学习到的特征图 $f_b$：单独的$B_g^I$ 分支，没有 LSTM 的 $B_g^I + B_p^I$ 和具有 LSTM 的 $B_g^I + B_p^I$。图 4 给出了一些插图。仅使用全局 $B_g^I$ 仅捕获具有少量局部细节的粗略区域。结合不使用 LSTM 的 part-based 的表示方式丰富了一些细节。在 global and part-based 分支机构中采用 LSTM 更注重行人的细节，忽略不相关的区域（例如，图4（d）中的顶角和图4（b）中的摩托车）。结果，使用 LSTM 的 $B_g^I + B_p^I$ 学习到的特征对于人员 Re-ID 更加健壮。  \n",
    "\n",
    "我们还将 Deep-Person 的一个变体与一些最先进的 part-based 模型进行比较。表 5 描述了比较结果。由于使用了 LSTM 建模空间背景信息，与最佳的 part-based 方法相比，实现了3.9% 的 mAP 和 2.1% 的 rank1 的提高。值得注意的是，Spindle 和 PDC 使用额外姿态注释来检测身体部位。与也使用 LSTM 来模拟身体部位之间空间关系的方法相比，在 Deep-Person 中以端到端的方式使用 LSTM 实现了显着的改善。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/3/9.jpg?raw=true)\n",
    "\n",
    "**Effectiveness of Multi-loss** 利用识别和排序损失的互补优势是我们 Deep-Person 的另一个重要方面。为了简化对这种互补优势的评估，我们放弃了 part-based 分支。如表 6 所示，结合识别损失和排序损失比单独使用它们中的每一个表现要好得多。这揭示了识别和排序信息对于学习 person Re-ID 的区分特征是重要的。  \n",
    "\n",
    "**Choice of final pedestrian descriptor** 如第 3 节所述，Deep-Person 使用主干特征 fb 的 global average pooled 特征 fm 作为最终的行人描述符。global and part-based representation 之间的补充信息隐含地融入到 fm 中，这要归功于 global and part-based representation 分支在训练过程中的反向传播误差。在最近使用 global and part-based 分支的工作之后，这两个分支的倒数第二个全连接层的连接表示为 fc 似乎是合理的选择。我们使用 Deep-Person 的双分支 $B_g^I + B_p^I$ 变体来评估这两种选择。如表 7 所示，隐式融合特征 fm 明显优于直接连接特征 fc。一个可能的原因是识别损失使得靠近分类层的特征更多地关注训练身份的差异。这种特征可能对训练图像中的身份具有区别性，但对测试过程中看不见的身份不具有区别性。然而，骨干特征可能更加健壮，并且能够更好地推广到看不见的测试类别。这种比较激励我们在 fm 之后附加 triplet loss 的排序分支，这被认为是 Deep-Person 中的最终行人描述符。  \n",
    "\n",
    "## 5. Conclusion  \n",
    "\n",
    "在本文中，我们引入了一个名为 Deep-Person 的新型三分支框架来学习 person Re-ID 的高判别深度特征。与大多数专注于特征表征或单独特征学习的现有方法不同，Deep-Person 会考虑两方面的互补优势。具体而言，共同使用局部身体部分和全局全身特征。识别损失和排序损失被应用于同时学习 ID 识别嵌入和相似性测量。此外，与通常丢弃身体结构的空间情境的现有 part-based 方法相比，我们使用 LSTM 来增强具有上下文信息的 part-based 表示的区分能力，从而减轻与序列级行人表示的错位。对三种流行且具有挑战性的数据集进行广泛的评估，证明了所提议的 Deep-Person 相对于最先进的方法的优越性。将来，我们希望将空间上下文建模与注意机制结合起来，为人员 Re-ID 自动选择更具区分性的部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisiting Temporal Modeling for Video-based Person ReID  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "基于视频的人员 reID 是一项重要的任务，近年来，由于监控和摄像机网络需求的增加，这种重要任务受到了很多关注。典型的基于视频的人员 reID 系统由三部分组成：图像级特征提取器（例如 CNN），聚合时间特征的时间建模方法和损失函数。虽然已经提出了许多时间建模方法，但很难直接比较这些方法，因为特征提取器和损失函数的选择对最终性能也有很大影响。我们全面研究并比较了基于视频的人员 reID 的四种不同的时间建模方法（temporal pooling, temporal attention, RNN and 3D convnets）。我们还提出了一种新的注意力生成网络，它采用时间卷积来提取帧间的时间信息。评估是在 MARS 数据集上完成的，我们的方法大大超过了最先进的方法。我们的源代码在 [https://github.com/jiyanggao/Video-Person-ReID发布](https://github.com/jiyanggao/Video-Person-ReID发布)。  \n",
    "\n",
    "## 1 Introduction  \n",
    "\n",
    "人员重新识别（重新识别）解决了检索不同图像或视频中特定人员（即查询）的问题，这些问题可能来自不同环境中的不同摄像机。 近年来，由于公共安全和快速增长的监控摄像机网络的需求不断增加，它得到了越来越多的关注。具体而言，我们专注于基于视频的人员重新识别，即给出一个人的查询视频，系统会尝试在一组图库视频中识别此人。  \n",
    "\n",
    "目前大多数基于视频的人员 reID 方法都是基于深度神经网络。通常，三个重要部分对基于视频的人员 reID 系统有很大影响：图像级特征提取器（通常是卷积神经网络，CNN），聚合图像级特征的时间建模模块和训练网络的损失函数。在测试期间，使用上述系统将查询视频和图库视频编码为特征矢量，然后计算它们之间的差异（通常为 L2 距离）以检索前 N 个结果。最近有关基于视频的人员 reID 的工作主要集中在时间建模部分，即如何将一系列图像级特征聚合到 clip-level 特征中。  \n",
    "\n",
    "以前关于基于视频的人员 reID 的时间建模方法的工作分为两类：基于递归神经网络（RNN）和 temporal attention based。在基于 RNN 的方法中，McLanghlin 等人提出使用 RNN 来模拟帧间的时间信息；Yan 等人也使用 RNN 编码序列特征，其中最终隐藏状态被用作视频表示。在 temporal attention based 的方法中，Liu 等人设计了一个质量认知网络（QAN），它实际上是一个 attention 加权平均值，用于聚合时间特征；Zhou 等人提出用时间 RNN 和注意力来编码视频。此外，Hermans 等人采用了 triplet 损失函数和简单的 temporal pooling 方法，并在 MARS 数据集上实现了最先进的性能。  \n",
    "\n",
    "尽管已经对上述方法进行了广泛的实验报道，但是很难直接比较时间建模方法的影响，因为他们使用不同的图像级特征提取器和不同的损失函数，这些变化会显着影响性能。例如，[13]采用 3 层 CNN 编码图像；[20]使用手工制作的特征；QAN [12]将 VGG 特征提取为图像表示。  \n",
    "\n",
    "在本文中，我们通过固定图像级特征提取器（ResNet-50）和损失函数（triplet loss and softmax cross-entropy loss）来研究不同的时间建模方法对基于视频的人员重识别的有效性。具体而言，我们测试了四种常用的时间建模架构：temporal pooling, temporal attention, Recurrent Neural Network (RNN) and 3D convolution neural networks。三维卷积神经网络直接将图像序列编码为特征向量；我们保持网络深度与 2D CNN 相同以进行公平比较。 我们还提出了一种新的注意力生成网络，它采用时间卷积来提取时间信息。我们在 MARS 数据集上进行实验，这是目前可用的最大的基于视频的人员 reID 数据集。实验结果表明，我们的方法大大超过了最先进的模型。  \n",
    "\n",
    "总之，我们的贡献有两方面：首先，我们全面研究了 MARS 上基于视频的人员 reID 的四种常用时间建模方法。其次，我们提出了一种新颖的基于时间转移的注意力生成网络，它在所有的时间建模方法中都达到最佳性能；借助强大的特征提取功能和有效的损失函数，我们的系统大幅超越了最先进的方法。  \n",
    "\n",
    "在下面，我们首先讨论第二节中的相关工作，然后在第三节中展示整个人员 reID 系统体系结构，并且详细描述时间建模方法。在第 4 节中，我们将展示实验并讨论结果。  \n",
    "\n",
    "## 2 Related Work  \n",
    "\n",
    "在本节中，我们将讨论相关工作，包括基于视频和基于图像的 person reid 和视频时间分析。  \n",
    "\n",
    "**基于视频的人员reID** 以前关于基于视频的人员 reID 的时间建模方法的工作分为两类：基于递归神经网络（RNN）和 temporal attention based。 McLanghlin 等人首先提出通过 RNN 对帧之间的时间信息进行建模，将 RNN 信元输出的平均值用 clip level 表示。与[13]类似，Yan 等人也使用 RNN 编码序列特征，最终的隐藏状态被用作视频表示。Liu 等人设计了一个 Quality Aware Network（QAN），它基本上是一个注意力加权平均值，用于汇总时间特征；注意分数是从帧级特征图生成的。Zhou 等人和 Xu 等人提出用时间 RNN 和注意力编码视频编码。Chung 等人提出了一个双流网络模型用于对 RGB 图像和光流进行建模，简单的 temporal pooling 用于聚合的功能。最近，Zheng 等人为基于视频的人员 reID 构建了一个新的数据集 MARS，该数据集成为此任务的标准基准。  \n",
    "\n",
    "**基于图像的人员 reID** 最近有关基于图像的人员 reID 的工作主要通过两个方向来提高性能：图像空间建模和度量学习的损失函数。在空间特征建模方面，Su 等人和 Zhao 等人使用人体关节解析图像并融合空间特征。Zhao 等人提出了一个部分对齐的表示方法来处理身体部位错位问题。至于损失函数，通常使用 Siamese 网络中的 hinge 损失和 identity softmax cross-entropy loss 函数。为了学习有效的 metric embedding，Hermans 等人提出了一个修改的 triplet 损失函数，它为每个样本选择最难的正样本和负样本，并且他们达到了最先进的性能。  \n",
    "\n",
    "**视频时间分析** 除了人员 reID 工作外，其他领域的时态建模方法，如视频分类，时间动作检测也是相关的。Karpathy 等人设计了 CNN 网络来提取帧级特征，并使用 temporal pooling 方法来聚合特征。Tran 等人提出了一个 3D CNN 网络从视频剪辑中提取时空特征。Hara 等人用3D卷积探索 ResNet 架构。Gao 等人提出了一个时间边界回归网络来定位长视频中的动作。  \n",
    "\n",
    "## 3 Methods  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/4/1.jpg?raw=true)\n",
    "\n",
    "在本节中，我们将介绍整个系统流水线和时间建模方法的详细配置。整个系统可以分为两部分：从视频剪辑中提取可视化表示的视频编码器，优化视频编码器的损失函数以及将查询视频与图库视频相匹配的方法。首先将视频剪切成连续的非重叠的剪辑$\\{c_k\\}$，每个剪辑包含 T 帧。剪辑编码器将剪辑作为输入并输出每个剪辑的 D 维特征向量 $f_c$。视频级别特征是所有剪辑级别特征的平均值。  \n",
    "\n",
    "### 3.1 Video Clip Encoder  \n",
    "\n",
    "为了构建视频剪辑编码器，我们考虑两种卷积神经网络（CNN）：（1）3D CNN 和（2）具有时间聚合方法的 2D CNN。3D CNN 直接将包含 n 帧的视频剪辑 c 作为输入并输出特征矢量 $f_c$，而 2D CNN 首先提取图像级特征序列 $\\{f_c^t\\},t \\in [1,n]$，然后 $\\{f_c^t\\}$ 通过时间建模方法被聚合成单个向量 $f_c$。  \n",
    "\n",
    "**3D CNN** 对于 3D CNN，我们采用了 3D ResNet 模型，该模型采用了带有 ResNet 架构的 3D 卷积核，并被用作动作分类。我们用人身份输出替换最终的分类层，并使用预先训练的参数（在 Kinetics 上）。该模型将 T 个连续帧（即视频剪辑）作为输入，并且将最终分类层之前的层用作人的特征。  \n",
    "\n",
    "对于二维 CNN，我们采用标准 ResNet-50 模型作为图像级特征提取器。给定一个图像序列（即一个视频剪辑），我们将每个图像输入到特征提取器中，并输出一系列图像级特征 $\\{f_c^t\\},t \\in [1,n]$，它是一个 T×D 矩阵，n 是剪辑序列长度，D 是图像级特征尺寸。然后，我们应用时间聚合方法将特征聚合成单个剪辑级特征 $f_c$，这是一个 D 维向量。具体而言，我们测试了三种不同的时间建模方法：（1）temporal pooling，（2） temporal attention，（3）RNN；这些方法的架构如图 1 所示。  \n",
    "\n",
    "**Temporal pooling** 在 Temporal pooling 模型中，我们考虑 max pooling 和 average pooling。对于 max pooling，$f_c = max_t(f_c^t)$；对于average pooling，$f_c = \\frac {1}{T} \\sum_{t=1}^n f_c^t$。  \n",
    "\n",
    "**Temporal attention** 在 Temporal attention 模型中，我们对图像特征序列应用注意力加权平均。假设 clip c 的注意力是 $a_c^t,t \\in [1,T]$，那么  \n",
    "\n",
    "$f_c = \\frac {1}{T} \\sum_{t=1}^n a_c^t f_c^t$\n",
    "\n",
    "Resnet-50 中最后一个卷积层的张量大小为 [w，h，2048]，w 和 h 取决于输入图像的大小。注意力生成网络将一系列图像层次特征 [T，w，h，2048] 作为输入，并输出 T 注意力分数。我们设计两种类型的注意力网络。（1）“空间 conv + FC”：我们应用空间 conv 层（kernel width = w，kernel height = h，输入信道数= 2048，输出信道数= dt，{w，h，2048 ，dt}）和上述输出张量上的全连接（FC）（输入信道= dt，输出信道= 1）层；conv 层的输出是标量向量 $s_c^t,t \\in [1,T]$，这些将作为剪辑视频中帧 t 的分数。（2）“spatial + temporal conv”：首先应用形状为{w，h，2048，dt}的 conv 层，然后对于剪辑的每个帧我们得到一个 dt 维特征，我们在这些帧级别特征上应用 temporal conv layer {3，dt，1} 以生成时间注意力 $s_c^t$。这两个网络如图 1（C）所示。  \n",
    "\n",
    "一旦我们有了 $s_c^t$，有两种方法计算最终的注意力分数 $a_c^t$：（1）Softmax函数，$a_c^t = \\frac {e^{s_c^t}}{\\sum_{i=1}^T \\ e^{s_c^t}}$，和（2）Sigmoid函数+ L1 正则，$a_c^t = \\frac {\\sigma(s_c^t)}{\\sum_{i=1}^T \\ \\sigma(s_c^t)}$，其中 $\\sigma$ 表示 Sigmoid 函数。  \n",
    "\n",
    "**RNN** RNN cell 在序列中按顺序对一个图像特征进行编码，并将隐藏状态 $h_t$ 传递到下一个时间步。我们考虑两种将一系列图像特征聚合成单个剪辑特征 $f_c$ 的方法。第一种方法在最后一个时间步直接采用隐藏状态 $h^T, f_c = h_c^T$，如图 1（B）所示。第二种方法计算 RNN 输出的平均值 $\\{o^t\\},t \\in [1,n]$，即 $f_c = \\frac{1}{T} \\sum_{t=1}^T o_c^t$。我们测试两种类型的 RNN cell：长短时记忆网络（LSTM）和门控循环单元（GRU）。其他设置可以在 4.1 节中找到。  \n",
    "\n",
    "### 3.2 Loss Functions  \n",
    "\n",
    "我们使用 triplet 损失函数和 Softmax 交叉熵损失函数来训练网络。我们使用的 triplet  损失函数最初是在[7]中提出的，并且被命名为 Batch Hard\n",
    "triplet loss function。为了形成一个批次，我们随机抽样 P 个身份并为每个身份随机抽样 K 个剪辑（每个剪辑包含 T 个帧）；一个 batch 中有 PK 个剪辑。对于批次中的每个样本 a，选择batch中的 the hardest positive and the hardest negative samples 来形成用于计算 triplet loss 的三元组。  \n",
    "\n",
    "$L_{triplet} = \\sum_{i=1}^P\\sum_{a=1}^K \\ [m + {max}_{p=1...K} D(f_{i,a},f_p^i) - {min}_{j=1...P,n=1...K,j \\neq i}D(f_{i,a},f_{j,n})]_+$\n",
    "\n",
    "Softmax 交叉熵损失函数 $L_{softmax}$ 鼓励网络将 PK 片段分类为正确的身份。  \n",
    "\n",
    "$L_{softmax} = - \\frac{1}{PK} \\sum_{i=1}^P \\sum_{a=1}^K p_{i,a}\\log q_{i,a}$\n",
    "\n",
    "其中 $p_{i,a}$ 和 $q_{i,a}$ 是样本 {i,a} 的真实身份和预测身份。总损失 L 是这两个损失的组合。  \n",
    "\n",
    "$L = L_{softmax} + L_{triplet}$\n",
    "\n",
    "### 3.3 Similarity Calculation for Testing  \n",
    "\n",
    "如前所述，视频被剪切成连续的非重叠剪辑{ck}，每个剪辑包含 T 帧。在测试过程中，我们提取视频中每个剪辑的剪辑级别表示，视频级别表示是所有剪辑级别表示的平均值。L2 距离用于衡量视频之间的相似度。  \n",
    "\n",
    "## 4 Evaluation  \n",
    "\n",
    "在本节中，我们列出评估设置并讨论实验结果。  \n",
    "\n",
    "### 4.1 Evaluation Settings  \n",
    "\n",
    "我们介绍评估指标，数据集和图像基准模型。还显示了实施细节。  \n",
    "\n",
    "**评估标准** 我们使用标准评估指标：mean average precision 评分（mAP）和在 rank-1, rank-5, rank-10 and rank-20 的累积匹配曲线（CMC）评分。  \n",
    "\n",
    "**数据集** 我们在 MARS 数据集上测试所有模型，这是迄今为止最大的基于视频的 person reID 数据集。MARS 由“tracklets”组成，其中包含 1261 个ID 和大约 20000 个tracklets，训练和测试均匀分割。  \n",
    "\n",
    "**实现** 在 ImageNet 上预训练的标准 ResNet-50 被用作 2D CNN，在 Kinectics 上预训练的 3D ResNet-50 被用作 3D CNN 视频编码器。视频帧的大小调整为 224×112。Adam 用于优化网络。批量大小设置为 32；如果总内存使用量超过 GPU 内存限制，我们会相应地将批量大小降至最大可能大小。在一批中，我们为每个身份选择 P = 4 个样本。我们使用区间在 0.0001 到 0.0003 的学习率来得到在不同模型上最好的效果。  \n",
    "\n",
    "**基于图像的基准模型** 我们提供基于图像的基准模型来测试时间建模的有效性。这个模型与[7]类似，但是使用了一个额外的 Softmaxcross-entropy 损失。具体而言，剪辑的序列长度被设置为 T = 1，不使用时间建模方法。采用相同的损失函数（triplet 损失和交叉熵损失）和 ResNet-50 网络。在测试过程中，使用与 3.3 节中所述相同的相似度计算程序。  \n",
    "\n",
    "### 4.2 Experiments on MARS  \n",
    "\n",
    "在这部分中，我们分别报道了 3D CNN, temporal pooling, temporal attention and RNN 的性能，然后讨论了实验结果。  \n",
    "\n",
    "**3D CNN** ResNet3D-50 被用作测试架构，它与 ResNet-50 具有相同的层数。ResNet3D-50 是一个全卷积网络，所以我们可以改变输入图像的序列长度。输入图像的高度和宽度保持为 224 和 112。由于 GPU 内存有限，我们只测试 T = 4 和 T = 8 的序列长度。我们将学习速率设置为 0.0001。结果显示在表 1 中。我们可以看到 T = 4 比 T = 8 表现更好。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/4/2.jpg?raw=true)\n",
    "\n",
    "**Temporal pooling** 用不同的序列长度测试了 average pooling and max pooling 模型。学习率设置为 0.0003，因为我们发现这个率达到了最佳效果。首先，我们用相同的序列长度 T = 8 比较 average pooling and max pooling 模型，如表 2 所示。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/4/3.jpg?raw=true)\n",
    "\n",
    "我们可以看到，average pooling 始终如一地优于 max pooling。接下来，我们测试不同序列长度 T = 1,2,4,8 的 average pooling T = 1是基于图像的基准模型，并且需要使用时间聚合方法。结果如表 3 所示，可以看出 T = 4 达到了最佳性能。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/4/4.jpg?raw=true)\n",
    "\n",
    "**Temporal attention** 我们评估了第 3 节中提到的两个 Temporal attention 模型：“spatial conv + FC”和“spatial conv + temporal conv”以及两个注意力生成函数（Softmax 和 Sigmoid）。序列长度被设置为 T = 4（根据以上实验，T = 4 实现最佳结果），学习速率都被设置为0.0003，并且 dt 被设置为 256。注意力生成函数之间的比较如表 4 所示，我们可以看到，softmax 和 sigmoid 的表现相似。注意力生成网络之间的比较如表 5 所示，softmax 用作注意力生成函数。可以看出，“spatial conv + temporal conv”比“spatial conv + FC”表现更好，这表明使用时间积分来捕获帧间信息的有效性。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/4/5.jpg?raw=true)\n",
    "\n",
    "**RNN** 我们首先测试具有不同输出选择的 RNN 模型：最终隐藏状态 $f_c = h_c^T$ 和 cell output average pooling $f_c = o_c^T$，如第 3 节所述。序列长度设置为 8，学习速率设置为 0.0001。LSTM 被用作基本的 RNN cell。测试不同隐藏状态大小（512,1024,2048）。final hidden state and cell output average pooling 的结果分别显示在表 6 和表 7 中。 我们可以看到“cell outputs average”一般比“final hidden state”更好。在两种模型中，Dh = 512 达到最佳性能。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/4/6.jpg?raw=true)\n",
    "\n",
    "为了测试不同类型的 RNN cell（LSTM 和 GRU），我们将隐藏状态大小修改为 Dh = 512，序列长度为 T = 8 并且使用“cell outputs average”作为剪辑表示。结果示于表 8 中 ，可以看出 LSTM 的表现一直超过 GRU。  \n",
    "\n",
    "为了用不同的序列长度 T = 2,4,8,16 测试 RNN 性能，我们将隐藏状态大小修改为 Dh = 512，使用 LSTM 单元和“cell outputs average”作为剪辑表示。结果示如表 9 所示，我们可以看出，T = 4 比 T = 2 和 T = 8 给出了稍微精确的结果。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/4/7.jpg?raw=true)\n",
    "\n",
    "**与最先进的方法进行比较** 我们选择每个模型中的最佳设置（temporal pooling, temporal attention and RNN），并比较它们的性能；结果显示在表10 中。对于 temporal pooling，我们选择 mean pooling 并将序列长度设置为 T = 4。对于 temporal attention，我们选择“Softmax”+“spatial conv + temporal conv”并设置 T = 4；对于 RNN，我们设置 Dh = 512，T = 4，并使用“cell outputs average”，我们还列出了基于图像的基准模型。可以看到，RNN 的性能甚至低于基于图像的基准模型，这意味着使用 RNN 做 temporal aggregation 的效果较差，temporal attention 比 mean pooling 能略好，并且优于基于图像的基线模型的性能 3%，这表明了时间建模的有效性。如表 10 所示，可以看出，我们基于图像的基线模型（mAP = 74.1%）已经大大超过了现有模型（mAP = 67.7%）；我们认为这种改进主要来自于从 Softmax 交叉熵损失的使用。我们还列出了表 10 中的重新排序后的性能，这使得mAP 和 CMC-1 再提高 7.8% 和 1.7%。  \n",
    "\n",
    "**讨论** 基于实验，可以看出，mean pooling 比图像基线模型改善了 3%，这表明对剪辑级别时间信息建模对基于视频的人员 reID 是有效的。通过比较RNN，mean pooling 和 图像基线模型，我们可以看到 RNN 的性能比基于图像的基线模型更差，这表明 RNN 要么捕获的时间信息是无效的，要么在这个数据集上难以训练。以前的工作显示通过使用 RNNs 改善的原因可能是由于使用的图像特征是来自浅层 CNN，与专门设计的预先训练的 CNN 相比，它本身就是劣质模型，并且 RNN 表现得像用一个额外的层来提取更具代表性的特征。我们 temporal attention 的效果比 mean pooling 效果稍好一些，原因可能是 mean pooling 已经足够捕捉剪辑中的信息，因为剪辑仅包含几帧，即 1/4 到 1/2 秒，很难在这么短的时间内观察任何图像质量变化。然而，剪辑中的质量差异可能非常大，因为整个视频可能非常长，因此未来可能的方向是如何聚合剪辑级别信息（我们当前的解决方案平均所有剪辑）。  \n",
    "\n",
    "## 5 Conclusion  \n",
    "\n",
    "基于视频的人类身份识别是一项重要任务，近年来备受关注。我们全面研究并比较了基于视频的人类 reID 的四种不同的时间建模方法。为了直接比较这些方法，我们固定了基础网络架构（ResNet-50）和损失函数（三元损失和 softmax 交叉熵损失）。我们还提出了一种新的注意力生成网络，它采用时间卷积层来提取帧间的时间信息。评估是在 MARS 数据集上完成的。实验结果表明，RNN 的性能较差，甚至低于图像基线模型；temporal pooling 可以比基线带来 2%-3% 的改善；我们的 temporal-conv-based attention 模型在所有时间建模方法中实现了最佳性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-ranking Person Re-identification with k-reciprocal Encoding  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "当将人员重新识别（re-ID）作为检索过程时，重新排序是提高准确性的关键步骤。然而在人员重新识别社区中，很少有工作一直致力于重新排序，特别是那些全自动，无监督的解决方案。在本文中，我们提出了一个 k-reciprocal 编码方法来重新排列 re-ID 结果。我们的假设是，根据 probe 搜索出来的图库图像选择 k-reciprocal 最近邻，如果包含搜索的 probe，那么它更可能是真正的匹配。具体而言，给定图像，通过将其 k-reciprocal 最近邻编码成单个向量来计算 k-reciprocal 特征，该向量用于在 Jaccard 距离下重新排序。最终距离按照原始距离和 Jaccard 距离的组合计算。我们的重新排序方法不需要任何人员交互或任何标记数据，因此它适用于大规模数据集。大规模的 Market-1501，CUHK03，MARS 和 PRW 数据集上的实验证实了我们方法的有效性。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "人员重新识别（re-ID）在计算机视觉中是一项具有挑战性的任务。一般来说，重新识别可被视为检索问题。给定一个 probe，我们希望在图库中搜索包含同一个人的图像。获得初始排名表后，一个好的做法是增加一个重新排序步骤，并期望相关图像可以获得更高的排名。在本文中，我们将重点放在重新排序问题上。  \n",
    "\n",
    "重新排序主要在通用实例检索中进行研究。许多重新排序方法的主要优点是可以在不需要额外训练样本的情况下实施，并且可以应用于任何初始排名结果。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/5/1.jpg?raw=true)\n",
    "\n",
    "重新排序的有效性在很大程度上取决于最初排名表的质量。以前的一些作品利用了排名最高的图像（如最近邻）之间的相似关系。一个基本的假设是，如果返回的图像在 probe 的邻近邻内，则它可能是真正的匹配，可以用于随后的重新排序。尽管如此，可能会偏离最佳情况：错误匹配可能包含在 probe 的最近邻中。例如，在图 1 中，P1，P2，P3 和 P4 与 probe 是四个真正的匹配，但是它们都不包括在前四名中。我们观察到一些错误的匹配（N1-N6）获得高排名。结果，直接使用 top-k 排列的图像可能会在重新排序的系统中引入噪声并损害最终结果。  \n",
    "\n",
    "在文献中，k-reciprocal 最近邻是解决上述问题的有效方法，即对 top-k 图像进行错误匹配的污染。当两个图像被称为 k-reciprocal 最近邻居时，当另一个图像作为 probe 时，它们都被排名为 top-k。因此，k-reciprocal 最邻居作为一个更严格的规则，无论两个图像是否真实匹配。在图 1 中，我们观察到 probe 是真实匹配图像的 reciprocal 邻居，但不是虚假匹配。这一观察确定了初始排名列表中的真实匹配，以改善重新排序结果。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/5/2.jpg?raw=true)\n",
    "\n",
    "鉴于上述考虑，本文介绍了一种用于 reID 的重新排序的 k-reciprocal 编码方法。我们的方法由三个步骤组成。首先，我们将加权 k-reciprocal 邻居集合编码成向量形成 k-reciprocal 特征。然后，两个图像之间的 Jaccard 距离可以通过它们的 k-reciprocal 特征来计算。其次，为了获得更强大的 k-reciprocal 特征，我们开发了一种本地查询扩展方法，以进一步提高重新识别性能。最后，最终距离计算为原始距离和 Jaccard 距离的加权聚合。它随后用于获取重新排列的列表。该方法的框架如图 2 所示。总之，本文的贡献是：  \n",
    "\n",
    "- 我们通过将 k-reciprocal 编码为单个向量来提出 k-reciprocal 特征。重新排序过程可以通过向量比较轻松完成。  \n",
    "\n",
    "- 我们的方法不需要任何人员交互或注释数据，并且可以以自动和无监督的方式应用于任何人员重新排序的结果。  \n",
    "\n",
    "- 所提出的方法有效地提高了几个数据集（包括Market-1501，CUHK03，MARS 和 PRW）上的人员重新识别性能。尤其是，我们在 Market-1501 中的 rank-1 和 mAP 都达到了最新的准确度。  \n",
    "\n",
    "## 2. Related Work  \n",
    "\n",
    "我们将感兴趣的读者引荐给[4,52]，以便对人员重新识别（re-ID）进行详细审查。在这里，我们专注于重新排列对象检索方法的研究，特别是针对重新识别的研究。  \n",
    "\n",
    "**Re-ranking for object retrieval** 已经成功地对重新排序方法进行了研究，以提高对象检索的准确性。许多作品利用 k 近邻探索相似关系来解决重排序问题。Chum 等人提出了average query expansion（AQE）方法，其中通过平均 top-k 返回结果中的向量来获得新的查询向量，并用于重新查询数据库。为了利用远离查询图像的负样本，Arandjelovic 和 Zisserman 开发了 discriminative query expansion（DQE）以使用线性 SVM 来获得权重向量。采用距决策边界的距离来修改初始排名表。Shen 等人利用初始排名列表的 k-近邻作为新的查询来产生新的排名列表。每个图像的新分数根据其在产生的排名列表中的位置来计算。最近，稀疏上下文激活（SCA）提出将邻居集合编码成矢量，并用广义 Jaccard 距离表示样本相似度。为了防止对 top-k 图像的错误匹配的污染，[15,35]中采用了 k-reciprocal 最近邻的概念。在[15]中，提出了上下文相异性度量（CDM），通过迭代调整每个点到其邻域的平均距离来改进相似性。Qin 等人正式提出 k-reciprocal 最近邻的概念。k-reciprocal 最近邻居被认为是高度相关的候选人，并且用于构造闭集以对数据集的其余部分进行重新排序。我们的工作与这两部作品中有几个方面不太一样。我们不使用最近邻居关系来改进相似性[15]，或者直接将 k-reciprocal 最近邻居视为排名最高的样本。相反，我们通过比较它们的 k-reciprocal 最近邻居来计算两幅图像之间的新距离。  \n",
    "\n",
    "**Re-ranking for re-ID** 大多数现有的人员重新识别方法主要集中在特征表示或度量学习。最近，一些研究人员已经关注了基于重新排序的 re-ID 方法。与[25,40]和[3]不同，这需要人机交互或标签监督，我们专注于自动和无监督的解决方案。Li 等人通过分析每对图像的邻近邻居的相关信息和直接信息来开发重新排序模型。在[12]中，通过联合考虑排序列表中的内容和上下文信息来学习无监督重排序模型，该模型有效地去除了模糊样本以改进重标识的性能。Leng 等人提出了一种双向排序方法来修正初始排序列表，其中新的相似度被计算为内容和上下文相似度的融合。最近，不同基线方法的共同最近邻居被利用来重新排序。Ye等人将全局和本地局部特征的共同最近邻居组合为新查询，并通过汇总全局和本地局部特征的新排名列表来修改初始排名列表。在文献[46]中，利用 k 最近邻计算不同基线方法的相似度和相异度，然后进行相似度和相异度的聚合，以优化初始排序列表。这些提到的方法在重新排列承诺方面继续取得进展，使未来的贡献能够发现来自最近邻居的进一步信息。但是，由于通常包含错误匹配，因此使用 k-最近邻居直接执行重新排序可能会限制整体性能。为了解决这个问题，在本文中，我们研究了 k-reciprocal neighbours 在个人身份识别中的重要性，并因此设计了一个简单而有效的重新排序方法。  \n",
    "\n",
    "## 3. Proposed Approach  \n",
    "\n",
    "### 3.1. Problem Definition  \n",
    "\n",
    "给定一个 probe p 和设有 N 个图像的图库 $G = \\{g_i|i = 1,2,...,N\\}$，两个人 p 和 gi 之间的原始距离可以用 Mahalanobis 距离  \n",
    "\n",
    "$d(p,g_i) = (x_p - x_{g_i})^T M (x_p - x_{g_i})$\n",
    "\n",
    "其中 xp 和 xgi 分别代表 probe p 和图像 gi 的外观特征，M 是一个正半定矩阵。  \n",
    "\n",
    "可以根据探测器 p 与图库 gi 之间的成对原始距离获得初始排序表 $L(p,G) = \\{g_1^0,g_2^0,...,g_N^0\\}$，其中 $d(p,g_i^0) < d(p,g_{i+1}^0)$。 我们的目标是对 $L(p,G)$ 进行重新排序，以使更多的正面样本出现在列表的前面，从而提高人员重新识别（re-ID）的性能。  \n",
    "\n",
    "### 3.2. K-reciprocal Nearest Neighbors  \n",
    "\n",
    "在[35]之后，我们将 $N(p,k)$ 定义为探测器 p 的 k 个最近邻居（即排名列表的前 k 个样本）:  \n",
    "\n",
    "$N(p,k) = \\{g_1^0,g_2^0,...,g_N^0\\}, |N(p,k)| = k$\n",
    "\n",
    "其中|·| 表示集合中的候选人数量。K-reciprocal 最近邻 $R(p,k)$可以定义为，  \n",
    "\n",
    "$R(p,k) = \\{g_i|(g_i \\in N(p,k)) ∧ (p \\in N(g_i,k))\\}$\n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/5/3.jpg?raw=true)\n",
    "\n",
    "根据前面的描述，k-reciprocal 最近邻居比 k-最近邻居更接近于 p。然而，由于照明，姿势，视图和遮挡的变化，positive images 可能被排除在 k-最近的邻居之外，并且随后不被包括在 k-reciprocal 最近的邻居中。为了解决这个问题，我们根据下面的条件将 $R(p,k)$ 中每个候选项的 1/2 k-reciprocal 最近邻居增量地增加到一个更鲁棒的集合 $R^*(p,k)$中。  \n",
    "\n",
    "$R^*(p,k) \\leftarrow R(p,k) \\bigcup R(q,1/2k)$  \n",
    "$s.t |R(p,k) \\bigcap R(q,1/2k)| \\geq 2/3 |R(q,1/2k)|$\n",
    "$q \\ in R(p,k)$\n",
    "\n",
    "通过这个操作，我们可以在 $R^*(p,k)$ 中加入更多与 $R(p,k)$ 中的候选相比更接近于 probe p 的正样本。与[35]相比，这样做更严格。在图 3 中，我们展示了一个扩展过程的例子。最初，$R(Q,20)$ 中遗漏了正样本 G。有趣的是，G 包含在 $R(C,10)$ 中，这是将正样本 G 带回的有益信息。那么，我们可以应用上述方程将 G 添加到 $R^*(Q,20)$ 中。因此，扩展过程之后，可以将更多的正样本添加到 $R^*(p,k)$ 中。与[35]不同，我们不直接将 $R^*(p,k)$ 中的候选作为排名最高的图像。相反，我们将 $R^*(p,k)$ 作为背景知识来重新计算 probe 和图库之间的距离。  \n",
    "\n",
    "### 3.3. Jaccard Distance  \n",
    "\n",
    "在这一小节中，我们通过比较它们的 k-reciprocal 最近邻集来重新计算 probe p 和图库 gi 之间的成对距离。如前所述，我们认为如果两幅图像相似，则它们的 k-reciprocal 最近邻集合重叠，即在集合中存在一些重复样本。而且重复的样本越多，两幅图像越相似。p 和 gi 之间的新距离可以通过它们 k-reciprocal 的 Jaccard 度量来计算：  \n",
    "\n",
    "$d_J(p,g_i) = 1 - \\frac{R^*(p,k) \\bigcap R^*(g_i, k)}{R^*(p,k) \\bigcup R^*(g_i, k)}$\n",
    "\n",
    "其中|·| 表示集合中的候选人数量。我们采用 Jaccard 距离来命名这个新距离。虽然上述方法可以捕捉到两幅图像之间的相似关系，但仍然有三个明显的缺点：  \n",
    "\n",
    "- 在很多情况下，获得两个邻居集合 $R^*(p,k)$ 和 $R^*(g_i, k)$ 的交集和并集非常耗时，而且需要对所有图像对计算 Jaccard 距离时它变得更具挑战性。另一种方法是将邻居集编码成更简单但等价的向量，大大减少计算复杂度，同时保持邻居集中的原始结构。  \n",
    "\n",
    "- 距离计算方法平均衡量所有邻居，导致简单但不具有区别性的邻居集。事实上，更接近 probe p 的邻居更可能是真正的正样本。因此，基于原始距离重新计算权重并将较大权重分配给较近的样本是令人信服和合理的。  \n",
    "\n",
    "- 考虑到上下文信息会在试图测量两个人之间的相似性时造成相当大的障碍，因为不可避免的变化使得难以区分足够的上下文信息。因此，结合原始距离和Jaccard 距离对于稳健的距离变得重要。  \n",
    "\n",
    "受到[2]的启发，通过将 k-reciprocal 最近邻集编码为一个向量 $V_p = [V_{p,g_1},v_{p,g-2},...,V_{p,g_N}]$ ,这个向量可以解决上述前两个问题，其中 $V_{p,g_i}$ 最初由二元指示函数定义为  \n",
    "\n",
    "$V_{p,g_i} = \\begin{cases} 1,  & \\text{if $g_i$ $\\in R^*(p,k)$} \\\\ 0, & \\text{otherwise} \\end{cases}$\n",
    "\n",
    "这样，k-reciprocal 邻居集合可以被表示为 N 维向量，向量的每个项目指示对应图像是否被包括在 $R^*(p,k)$中。但是，这个函数仍然认为每个邻居都是平等的。直观地说，更接近 probe p 的邻居应该与 probe p 更相似，因此，我们根据 probe 与其邻居之间的原始距离重新分配权重，我们重新定义了上述公式，高斯内核的成对距离定义为  \n",
    "\n",
    "$V_{p,g_i} = \\begin{cases} e^{-d(p,g_i)},  & \\text{if $g_i$ $\\in R^*(p,k)$} \\\\ 0, & \\text{otherwise} \\end{cases}$\n",
    "\n",
    "这样，硬加权（0或1）被转换成软加权，较近的邻居分配较大的权重，而较远的邻居分配较小的权重。基于上述定义，交集和并集合中的候选数量可以计算为  \n",
    "\n",
    "$\\mid R^*(p,k) \\bigcap R^*(g_i,k)\\mid = \\mid\\mid min(V_p,v_{g_i}) \\mid\\mid_1$\n",
    "\n",
    "$\\mid R^*(p,k) \\bigcup R^*(g_i,k)\\mid = \\mid\\mid max(V_p,v_{g_i}) \\mid\\mid_1$\n",
    "\n",
    "其中 min 和 max 操作两个输入向量的基于元素的最小化和最大化。 $\\mid\\mid \\cdot \\mid\\mid$ 是 L1 范数。因此我们可以重写方程式中的 Jaccard 距离  \n",
    "\n",
    "$d_J(p,g_i) = 1 - \\frac {\\sum_{j=1}^N min(V_{p,g_j},v_{g_i},v_{g_j})}{\\sum_{j=1}^N max(V_{p,g_j},v_{g_i},v_{g_j})}$\n",
    "\n",
    "通过 Jaccard 公式的转变，我们成功地将设定的比较问题转换为纯矢量计算，这实际上更容易实现。  \n",
    "\n",
    "### 3.4. Local Query Expansion  \n",
    "\n",
    "为了模拟来自同一类的图像可能共享相似特征的想法，我们使用 probe p 的 k 最近邻居来实现本地扩展查询。 本地扩展查询定义为  \n",
    "\n",
    "$V_p = \\frac{1}{\\mid N(p,k) \\mid} \\sum_{g_i \\in N(p,k)} V_{g_i}$\n",
    "\n",
    "结果， k-reciprocal 特征 Vp 被 probe p 的 k 个最近邻居扩展。请注意，我们在 probe p 和 gallery gi 上实现了这个查询扩展。由于在 k 个最近的邻居中会有噪声，所以我们将在本地查询扩展中使用的 $N(p,k)$ 的大小限制为较小的值。  \n",
    "\n",
    "### 3.5. Final Distance  \n",
    "\n",
    "在这一小节中，我们关注 Jaccard 方程式的第三个缺点。虽然大多数现有的重排方法忽略了原始距离在重排中的重要性，但我们共同汇总了原始距离和Jaccard 距离以修改初始排序列表，最终距离 $d^*$ 被定义为  \n",
    "\n",
    "$d^*(p,g_i) = (1-\\lambda) d_J(p,g_i) + \\lambda d(p,g_i)$\n",
    "\n",
    "其中$\\lambda \\in [0,1] $ 表示惩罚因子，它惩罚远离探测点 p 的图像。最后，修改后的排序列表 $L^*(p,G)$ 可以通过升序排列的最终距离获得。  \n",
    "\n",
    "### 3.6. Complexity Analysis  \n",
    "\n",
    "在所提出的方法中，大部分计算成本集中于所有图库的成对距离计算。假设图库的大小为 N，距离度量和排序过程所需的计算复杂度分别为 $O(N^2)$ 和$O(N^2\\log N)$。然而，在实际应用中，我们可以计算两两距离，并提前离线获取图库的排序列表。因此，给定一个新的探测器 p，我们只需要计算 p 和图库之间的成对距离，其计算复杂度为 $O(N)$，并用计算复杂度 $O(N\\log N)$ 对所有最终距离进行排序。  \n",
    "\n",
    "## 4. Experiments  \n",
    "\n",
    "### 4.1. Datasets and Settings  \n",
    "\n",
    "**数据集** 由于我们的重新排序方法基于两个人之间的相似邻居的比较，我们对四个大型人员重新识别（re-ID）基准数据集进行了实验，这些基准数据集包含多个针对该库中每个 probe 的正面样本：包括两个 基于图像的数据集 Market-1501，CUHK03，基于视频的数据集 MARS 和端到端数据集 PRW (概述见表 1 )。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/5/4.jpg?raw=true)\n",
    "\n",
    "**Market-1501**是目前最大的基于图像的 re-ID 基准数据集。它包含 32,668 个标记的包围盒，其中包含从 6 个不同视点捕获的 1,501 个身份。使用可变形零件模型（DPM）检测边界框。该数据集分为两部分：12,936 张带有 751 个身份的图像用于训练和 19,732 张带有 750 个身份的图像用于测试。 在测试中，使用 3368 个具有 750 个身份的手绘图像作为 probe 集，以确定测试集上的正确身份。我们报告此数据集的单个查询评估结果。  \n",
    "\n",
    "**CUHK03**包含 1,496 个身份的 14,096 张图片。每个身份都是从中大校园内的两台摄像机拍摄的，每台摄像机每个身份平均有 4.8 张图像。该数据集提供了手动标记的边界框和 DPM 检测的边界框。在这篇文章中，我们给出了'标记'和'检测'数据的实验结果。  \n",
    "\n",
    "**MARS**是迄今为止最大的基于视频的 re-ID 基准数据集，包含 1,261 个身份和约 20,000 个视频序列。这些序列从 6 个不同的相机收集，每个身份平均具有 13.2 个序列。每个序列由 DPM 作为行人检测器和 GMMCP 作为跟踪器自动获得。另外，该数据集还包含 3,248 个干扰序列。数据集被固定分为训练集和测试集，分别有 631 和 630 个身份。在测试中，选择 2,009 个 probe 进行查询。  \n",
    "\n",
    "**PRW**是一个端到端的大规模数据集。它由六个不同摄像机捕获的 11,816 帧 932 个身份组成。从这些帧中生成总共 43,110 个带注释的人边界框。给定查询边界框，数据集的目标是首先对原始帧执行行人检测以生成图库，并从图库中识别正确的边界框。该数据集被分成具有 5,704 个 482 个身份的训练集和具有 450 个身份的 6,112 个帧的测试集。在测试中，选择 2,057 个查询图像用于 450 身份验证。如果其与真实值的 IOU 值大于0.5，则检测到的边界框被认为是正确的。  \n",
    "\n",
    "**评估指标** 我们使用两个评估指标来评估所有数据集上的重新识别方法的性能。第一个是累积匹配特征（CMC）。考虑到 reID 问题作为排序问题，我们在 rank-1 上报告累积匹配的准确性。另一个是考虑到重新识别作为对象检索问题，我们报告了平均精度（mAP）。  \n",
    "\n",
    "**特征表达** Local Maximal Occurrence（LOMO）特征用于表示人的外貌，它对照明变化和其他变化比较鲁棒。另外，也使用了[54]中提出的 ID-discriminative Embedding（IDE）特征。IDE 提取器经过有效的分类模型训练，包括 CaffeNet 和ResNet-50 。它为每个图像生成一个 1024-dim（或2048-dim）向量，这在大规模重新识别数据集中很有效。为了便于描述，我们分别将经过 CaffeNet 和 ResNet-50 训练的 IDE 缩写为IDE（C）和IDE（R）。我们使用这两种方法作为我们的 re-ID 框架的基准。  \n",
    "\n",
    "### 4.2. Experiments on Market-1501  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/5/5.jpg?raw=true)\n",
    "\n",
    "我们首先在最大的基于图像的重新识别数据集上评估我们的方法。在这个数据集中，除了使用 LOMO 和 IDE 特征外，我们还使用 BOW 特征。我们在CaffeNet 和 ResNet-50 上训练了 IDE 特征。我们设 k1 为 20，k2 为 6，$\\lambda$ 设为 0.3。我们的方法在各种方法中的结果如表 2 所示。我们的方法不断提高具有所有特征的 rank-1 精度和 mAP，即使是在强大的 ResNet-50 模型上训练的 IDE（R）也是如此。我们的方法在 rank-1 准确性方面获得3.06% 的提升，IDE（R）的 mAP 提高13.99%。此外，用两个度量指标 KISSME 和 XQDA 进行的实验验证了我们的方法对不同距离度量的有效性。与平均查询扩展（AQE）和上下文相异度量（CDM）两种流行的重排序方法相比，我们的方法在 rank-1 准确度和 mAP 方面都优于它们。许多现有的人员重新排序方法是用于单次设置或需要人机交互。因此，这些方法不能直接与我们的方法相媲美。  \n",
    "\n",
    "表 3 将我们最佳方法 IDE（R）+ KISSME + ours 与其他最先进的方法进行了比较。我们最好的方法胜过了以前的工作，与 rank-1 精度的最新结果相比，尤其是在 mAP 中，实现了巨大的增长。  \n",
    "\n",
    "### 4.3. Experiments on CUHK03  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/5/6.jpg?raw=true)\n",
    "\n",
    "在[23]中的 single-shot 设置协议之后，我们将数据集分成包含 1,160 个身份和包含 100 个身份的训练集和训练集。测试过程重复 20 次随机分割。我们将 k1 设置为 7，k2 设置为 3，$\\lambda$ 设置为 0.85。single-shot 设置的结果显示在表 4 中。我们可以看到，使用 IDE 特征时，我们的重新排序结果几乎等于原始结果，我们的方法无效是合理的。由于图库中的每个身份只有一个正样本，我们的方法无法获得足够的上下文信息。即便如此，我们的方法在“标记”和“检测”设置上应用 LOMO 特征时的 rank-1 精度和 mAP 提高近 1%。实验表明，在 single-shot 的情况下，我们的方法不会对结果造成伤害，并且有机会提高性能。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/5/7.jpg?raw=true)\n",
    "\n",
    "除了以前的评估方法外，我们还使用与 Market-1501 类似的新训练/测试协议报告结果。新协议将数据集分解为训练集和测试集，分别由 767 个身份和 700 个身份组成。在测试中，我们从每个相机中随机选择一个图像作为每个身份的查询，并使用其余图像构建图库集。新协议有两个优点：1）对于每个身份，图库中都有多个基本事实。这更符合实际应用场景。2）将数据集均匀分为训练集和测试集，有助于避免多次重复训练和测试。分开的培训/测试集和评估代码可在我们的源代码中找到。我们设 k1 为 20，k2 为 6，$\\lambda$ 设为 0.3。表 5 中的结果表明，在所有情况下，我们的方法显着提高了 rank-1 准确性和 mAP。特别是对于 IDE（R）+ XQDA，我们的方法在“标记”设置中的 rank-1 准确度提高了 6.1%，mAP 提高了 10.7%。  \n",
    "\n",
    "### 4.4. Experiments on MARS  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/5/8.jpg?raw=true)\n",
    "\n",
    "我们还评估了基于视频数据集的方法。在这个数据集上，我们使用了两个特征作为基准方法，LOMO 和 IDE。对于每个序列，我们首先提取每个图像的特征，并使用 max pooling 将所有特征组合成一个固定长度的向量。我们将 k1 设置为 20，k2 设置为 6，并将 $\\lambda$ 设置为 0.3。我们的方法在不同特征和度量上的表现如表 6 所示。我们可以看到，我们的重新排序方法不断提高了两个不同特征的 rank-1 精度和 mAP。结果与平均查询扩展（AQE）和上下文相异性度量（CDM）相比，我们的方法在 rank-1 准确度和 mAP 中均优于它们。此外，我们的方法甚至可以提高所有情况下的 rank-1 准确性和 mAP，同时使用判别性度量。特别是，我们的方法将 IDE（R）+ XQDA 的 rank-1 准确度从 70.51% 提高到 73.94%，mAP 从 55.12% 提高到 68.45%。实验结果表明，我们的重新排序方法对基于视频的重新识别问题也是有效的。我们相信，通过将更复杂的特征模型与我们的方法相结合，这个问题的结果将得到进一步改善。  \n",
    "\n",
    "### 4.5. Experiments on PRW  \n",
    "\n",
    "我们还在端到端的 re-ID 数据集上评估我们的方法。该数据集比基于图像和基于视频的数据集更具挑战性，因为它需要从原始图像中检测人并从检测到的图库中识别出正确的人。[54]之后，我们首先使用 DPM 来检测大原始图像上的候选边界框，然后查询检测到的边界框。我们使用 LOMO 和 IDE 为每个边界框提取特征，并将这两种方法作为基线。我们设 k1 为 20，k2 为 6，$\\lambda$ 设为 0.3。实验结果如表 7 所示。可以看出，我们的方法不断提高 LOMO和 IDE 特性的 rank-1 准确度和 mAP，证明我们的方法在端到端的 re-ID 任务中是有效的。  \n",
    "\n",
    "### 4.6. Parameters Analysis  \n",
    "\n",
    "本小节分析了我们方法的参数。基准方法是 LOMO 和在 CaffeNet 上训练的 IDE。我们评估 k1，k2 和 $\\lambda$ 对 Market-1501 数据集的 rank-1 准确性和 mAP 的影响。为了进行实验分析，我们将原始训练集随机分为训练集和验证集，分别有 425 和 200 个身份。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/5/9.jpg?raw=true)\n",
    "\n",
    "图 4 显示了设置在 rank-1 精度和 mAP 上的 k-reciprocal 邻居大小的影响。可以看出，我们方法的 rank-1 准确度和 mAP 在不同的 k1 值上始终由于基线方法。MAP 首先随 k1 的增长而增加，然后在 k1 超过阈值后开始缓慢下降。同样，随着 k1 的增长，rank-1 的准确度首先随着波动而上升；到达 k1 = 20 左右的最佳点后，它开始下降。如果 k1 的值太大，则 k-reciprocal 集合中将包含更多的错误匹配，导致性能下降。  \n",
    "\n",
    "k2 的影响如图 5 所示。当 k2 等于 1 时，不考虑本地查询扩展。显然，随着 k2 在合理范围内增加，表现会增长。请注意，为 k2 分配太大的值会降低性能。由于它可能会导致在本地查询扩展中包含错误匹配的指数级，这无疑会损害该特性，从而影响性能。事实上，当为 k2 设置适当的值时，本地查询扩展对于进一步提高性能非常有利。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/5/10.jpg?raw=true)\n",
    "\n",
    "参数 $\\lambda$ 的影响如图 6 所示。注意，当 $\\lambda$  设置为 0 时，我们只考虑 Jaccard 距离作为最终距离；相反，当 $\\lambda$  等于1时，Jaccard 距离被排除，并且结果恰好是使用纯原始距离获得的基线结果。可以观察到，当只考虑 Jaccard 距离时，我们的方法始终优于基线。这表明提议的 Jaccard 距离对于重新排序是有效的。此外，在同时考虑原始距离和 Jaccard 距离的情况下，当 $\\lambda$ 的值大约为 0.3 时，性能得到进一步改进，表明原始距离对于重新排序也是重要的。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/5/11.jpg?raw=true)\n",
    "\n",
    "在图 7 中，显示了四个示例结果。所提出的方法 IDE + Ours 在排名列表顶部排列更多的真实人物，这些人在 IDE 的排名列表中被错过了。  \n",
    "\n",
    "## 5. Conclusion  \n",
    "\n",
    "在本文中，我们解决人员重新识别中的重新排序问题（重新识别）。我们通过将 k-reciprocal 最近邻居编码成单个向量来提出 k-reciprocal 特征，因此通过向量比较可以容易地执行重排序过程。为了捕获来自相似样本的相似关系，本地扩展查询被提出来获得更强大的 k-reciprocal 特征。基于原始距离和 Jaccard 距离组合的最终距离可以在几个大型数据集上有效改善重新识别性能。值得一提的是，我们的方法是全自动和无监督的，可以轻松实现任何排序结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Deep Metric for Person Re-identification: A Study Against Large Variations  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "由于姿态，照明，遮挡和摄像头视图的变化很大，因此人员重新识别具有挑战性。由于这些变化，尽管目前的卷积神经网络（CNN）具有特征提取能力，但行人数据在特征空间中分布是 highly-curved manifolds 的。然而，数据分布是未知的，因此在比较两个样本时难以使用 geodesic 距离。实际上，目前的深度嵌入方法使用欧几里德距离进行训练和测试。另一方面，manifold 学习方法建议使用局部范围内的欧氏距离，并结合样本之间的图形关系来近似 geodesic 距离。从这个角度来看，在局部范围内选择合适的郑（即类内）训练样本对于训练 CNN embedding 是至关重要的，特别是当数据具有较大的类内变化时。在本文中，我们提出了一种新的中等正样本挖掘方法来训练鲁棒的 CNN 用于人员重新识别，处理大变化的问题。另外，我们通过一个 metric 权重约束来改进学习，使得学习的度量具有更好的泛化能力。实验表明，这两种策略在学习用于人员重新识别的健壮深度度量方面是有效的，因此，我们的深度模型在人员重新识别的几个基准上明显优于最先进的方法。因此，本文提出的研究可能有助于激发 person 再识别深层模型的新设计。  \n",
    "\n",
    "## 1 Introduction  \n",
    "\n",
    "给定一组行人图像，人员重新识别旨在识别通常由不同摄像机捕获的探测图像。现在，人员重新识别对于监视和安全系统变得越来越重要，例如，取代手动视频筛选和其他重负载。由于身体姿势，照明，视角，时间和摄像机场景的巨大变化，人员重新识别是一项具有挑战性的任务。  \n",
    "\n",
    "现有方法的框架通常由两部分组成：（1）从行人图像中提取区分特征；（2）通过特征比较来计算样本的距离。关于这两方面有很多着作。传统的方法着重改善手工制作的特征，或提高测量指标或两者都使用。第一个方面考虑在保留身份信息的同时找到对挑战性因素（照明，姿势等）稳健的特征。第二个方面涉及到度量学习问题，该问题通常使类内距离最小化，同时使类间距离最大化。  \n",
    "\n",
    "最近，深度学习方法逐渐在个人重新识别中获得普及。深度学习的重新识别方法将人员重新识别的上述两个方面（特征提取和度量学习）纳入综合框架。特征提取和度量学习分别由深度神经网络中的两个组件完成：从图像中提取特征的 CNN 部分，以及将特征进行比较的度量学习部分。FPNN 算法首次为 CNN 部分引入了 patch matching 层。Ahmed 等人提出了一种改进的深度学习架构（IDLA），具有交叉输入邻域差异和 patch summary 特征。这两种方法都致力于改进 CNN 体系结构。他们的目的是在 CNN 阶段早期评估图相对的相似性，以便它可以利用特征图的空间对应关系。对于度量学习部分，DML 采用余弦相似和二项偏差。DeepFeature 采用欧几里得距离和三重损失。其他一些使用逻辑损失直接形成输入图像对是否属于同一身份的二元分类问题。  \n",
    "\n",
    "以下是我们的贡献。  \n",
    "\n",
    "- 为了训练 CNN，hard negative mining 策略已经在[1,27,30]中使用。考虑到行人数据中的类内差异较大，我们认为，由于行人数据是作为特征空间中高度弯曲的流形分布的，所以在进行个人重新识别时，也应仔细采样正样本训练对。正如在一些流形学习方法中所论述的那样，使用局部欧几里得距离，结合样本间的图形关系来近似 geodesic 距离是有效的。因此，选择局部范围中选择 moderate positive pairs 对训练网络至关重要。这是一个重要的问题，但很少被人注意。在本文中，我们提出了一种新的训练策略，名为 moderate positive mining，用于自适应地搜索 moderate positives for training。 这种新颖的训练方法显着提高了识别精度。  \n",
    "\n",
    "- 此外，我们通过度量图层的权重约束来改善网络。权重约束规范了 metric 学习部分，缓解了过拟合问题。  \n",
    "\n",
    "## 2 Related Work  \n",
    "\n",
    "**Positive Sample Mining** The hard negative mining 策略已被用于人脸识别。在人员重新识别方面，IDLA 也采用了 hard negative mining 的方式进行训练。通过迫使模型专注于决策边界附近的 hard negatives，提高了训练效率和模型性能。在本文中，我们发现如何选择 moderate positive samples 也是学习重新识别模型的基本问题。moderate positives 与训练网络的 hard negative 一样重要，特别是当数据具有较大的类内变化时。然而，这方面几乎没有任何学习深度嵌入的尝试。在我们的方法中，我们提出了 moderate positive mining  的新策略来解决这个问题。我们对 moderate positives 的训练样本进行抽样，并避免使用来自行人数据极端类内差异的困难样本。凭经验发现，这一策略有效地提高了识别的准确性（见第 4.2节）。   \n",
    "\n",
    "**Weight Constraint for Metric Learning** 一种常用的深度学习方法是欧氏距离。然而，欧氏距离对尺度敏感，并且对于尺寸间的相关性是盲目的。在实践中，我们无法保证 CNN 学习的特征具有相似的尺度和跨维度的去相关性。因此，使用 Mahalanobis 距离是多变量度量的更好选择。在人脸识别领域，DDML 在其网络中实现了 Mahalanobis 度量，但没有任何约束。我们的度量是以相似的方式学习的，并且通过所提出的权重约束来改进，这有助于获得更好的泛化能力。  \n",
    "\n",
    "## 3 Proposed Method  \n",
    "\n",
    "在本节中，我们首先介绍 moderate positive mining 方法。然后，我们重新访问 DDML 并引入权重约束。  \n",
    "\n",
    "### 3.1 Moderate Positive Mining  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/6/1.jpg?raw=true)\n",
    "\n",
    "**Large Intra-class Variations** 有许多因素导致行人数据中的大的类内变化，例如照明，背景，错位，遮挡，人员共同出现，外观变化等。其中许多因素都与行人数据有关。图1（a）显示了 CUHK03 的数据集中的一些 hard positive 样本。他们中有些人甚至难以认出。  \n",
    "\n",
    "虽然 CNN 具有很强的提取特征的能力，但由于变化较大，行人数据遵循特征空间中非常不规则的分布，如图1（b）所示的 highly-curved manifold 的例子。这反映在与人脸识别任务相比较，几个人员重新识别基准上的最新性能相对较差，这是由于较小的类内变化比较容易。  \n",
    "\n",
    "**Moderate Positive Mining Method** 考虑到图1（b）中的分布是未知的，难以应用 geodesic 距离来比较两个样本。通常的方法是使用Mahalanobis 距离（或特殊情况 Euclidean），这是理想条件下的合适度量（图1（c））。  \n",
    "\n",
    "另一方面，流形学习方法建议在局部范围内使用欧氏距离，并结合样本之间的图形关系来近似 geodesic 距离。这是一个可行的方法，可以沿着监督学习的方向尽量减少类内差异。然而，当用 contrastive 或者 triplet loss训练 CNN 来进行 embedding 时，现有的 deep embedding 方法对所有正样本不加区分地使用欧几里德距离。  \n",
    "\n",
    "在这里，我们认为选择局部范围内的正样本（图1（b）中的黄线对）对于训练网络至关重要；用大距离的正样本（交叉的黄线）进行训练可能会扭曲流形并损害流形学习。  \n",
    "\n",
    "基本思想是我们通过挖掘局部范围内的 moderate positive pairs 来保留行人数据的内在图形结构，从而减少类内差异。  \n",
    "\n",
    "我们通过以下方法引入 moderate positive mining：我们每次在同一 subject 范围内选择 moderate positive pairs。例如，假设一个 subject 有 6 个图像，其中 3 个来自一个相机，另外 3 个来自另一个相机。我们可以对这个 subject 可以有 9 个positive pairs。如果我们使用九个中最简单的 positive pair，那么收敛速度将非常缓慢。如果我们使用最困难的 positive pair，学习将会受到损害。因此，我们选择两个极端情况之间的 moderate positive pairs。  \n",
    "\n",
    "给定来自两个不相交的摄像机的两组行人图像 $\\cal I_1$ 和 $\\cal I_2$ ，将 $I_1 \\in \\cal I_1$ 和 $I_2^p \\in \\cal I_2$ 作为一个正对（来自同一身份），$I_1 \\in \\cal I_1$ 和 $I_2^n \\in \\cal I_2$作为一个负对（来自不同的身份）。将 $\\Psi (\\cdot)$ 表示为 CNN，$d(\\cdot,\\cdot)$ 是马氏距离或欧氏距离。mining 方法描述如下：  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/6/2.jpg?raw=true)\n",
    "\n",
    "首先，我们随机选择一个 anchor 样本及其正样本和负样本（数目相同）形成一个 mini-batch；然后，我们挖掘 hardest negative sample，并选择比hardest negative 距离更小的 positive sample；最后，我们将这些选定正样本中最难的一个作为我们的 moderate positive sample。这样做的原因是我们对每个 subject 适应性地定义了“moderate positive”，而他们的 hard negatives 考虑在内了。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/6/3.jpg?raw=true)\n",
    "\n",
    "图 2 给出了一个例子。在实验中，这个动态挖掘策略显着提高了性能，并且显示出很好的稳定性，因为在每个 subject 中考虑了所有的正样本，并且通过随机变化来增加数据。  \n",
    "\n",
    "### 3.2 Weight Constraint for Deep Metric Learning  \n",
    "\n",
    "一旦 CNN 从一对图像中提取特征，随后执行度量层以计算距离，如图 3 所示。度量学习层类似于 DDML 中提出的结构，并且其学习通过权重约束得到改进。  \n",
    "\n",
    "回顾上述两组行人图像 $\\cal I_1$ 和 $\\cal I_2$，$\\cal X_1$和 $\\cal X_2$ 表示是由 CNN 提取的对应特征集合。$x_1 = \\Psi(I_1), x_2^p = \\Psi(I_2^p)$ 和 $x_2^n = \\Psi(I_2^n)$ 是anchor,，正样本和负样本的相应特征。  \n",
    "\n",
    "**Revisiting DDML** Mahalanobis 距离被表述为  \n",
    "\n",
    "$d(x_1,x_2) = \\sqrt {(x_1 - x_2)^T M (x_1 - x_2)}$\n",
    "\n",
    "其中 $x_2 \\in \\{x_2^p,x_2^n\\}$，M 是对称正半定矩阵。在正半定的约束下学习 M 是困难的。我们利用它的分解 $M = WWT^$。学习 W 要容易得多，我们定义的距离如下  \n",
    "\n",
    "$d(x_1,x_2) = \\sqrt {(x_1 - x_2)^T WW^T (x_1 - x_2)} = \\mid\\mid W^T(x_1-x_2)\\mid\\mid_2$\n",
    "\n",
    "内积 $W^T(x_1-x_2)$ 可以通过线性全连接（FC）层来实现。FC 层的输出由下式计算  \n",
    "\n",
    "$y = f(W^x + b)$\n",
    "\n",
    "其中 b 是偏差项，身份函数用作线性 FC 层的激活 $f(\\cdot)$。如图 3 所示，特征向量 x1 和 x2 被馈送到 subtraction 层。然后，该差值由线性 FC 层与权重矩阵 $W^T$ 进行变换。对于距离的对称性，我们在整个训练和测试中将偏差项 b 固定为零。最后，将 L2 范数计算为输出距离 $d(x1,x2)$。切换减法层和 FC 层的位置时，此结构保持等效。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/6/4.jpg?raw=true)\n",
    "\n",
    "**Weight Constraint** 目标是尽量减少类内距离，并最大化类间距离。训练损失定义为  \n",
    "\n",
    "$L = d(\\Psi(I_1),\\psi(I_2^p)) + [m - d(\\Psi(I_1),\\psi(I_2^n))]_+$\n",
    "\n",
    "其中 $I_1, I_2^p, I_2^n$ 是对应于特征 $x_1, x_2^p, x_2^n$ 的输入图像，并且 m 是在实现中被设置为 2 的余量。在正向传播的每一次，无论是上述方程的第一项还是第二项都被计算。然后通过组合这两个项来获得损失，并且计算梯度。  \n",
    "\n",
    "与 Mahalanobis 距离相比，欧氏距离具有较小的可区分性，但具有较好的泛化能力，因为它没有考虑尺度和跨维度的相关性。在这里，我们施加一个约束，使得矩阵 M 在对角线上有很大的值，在其他地方有很小的值，所以我们可以在无约束的马氏距离和欧几里德距离之间取得平衡。该约束被表述为 $WW^T$ 和单位矩阵 I 之间差异的 Frobenius 范数，  \n",
    "\n",
    "$L = d(\\Psi(I_1),\\psi(I_2^p)) + [m - d(\\Psi(I_1),\\psi(I_2^n))]_+$  \n",
    "$s.t. \\mid\\mid WW^T - I \\mid\\mid_F^2 \\leq C $\n",
    "\n",
    "其中 C 是一个常数。我们进一步将约束条件与损失函数结合为一个正则化项：  \n",
    "\n",
    "$\\hat L = L + \\frac{\\lambda}{2} \\mid\\mid WW^T - I \\mid\\mid_F^2 $\n",
    "\n",
    "$\\lambda$ 是正则化的相对权重，$\\hat L$ 是新的损失函数。为了更新权重矩阵 W，通过下面 W 的梯度 \n",
    "\n",
    "$\\frac {\\partial \\hat L}{\\partial W} = \\frac {\\partial L}{\\partial W} + \\lambda(WW^T - I)W$\n",
    "\n",
    "当 $\\lambda$ 较小时，Mahalanobis 距离将考虑跨维度的相关性。然而，由于从训练集中学习度量矩阵，其通常在个人重新识别中通常很小，所以它可能在训练集中过拟合。另一方面，当 $\\lambda$ 很大时，矩阵 WWT 变得接近单位矩阵。在极端情况下，WWT 等于单位矩阵，并且距离减少到欧几里德距离。 在这种情况下，欧几里得距离不考虑相关性，但可能强健地推广到看不见的测试集。因此，我们将马氏距离和欧几里得距离的优点结合起来，并通过约束来平衡匹配精度和泛化性能。  \n",
    "\n",
    "## 4 Experiments  \n",
    "\n",
    "我们的方法是通过重新调整 CUDA-Convnet 框架来实现的。我们使用 one-shot 标准协议对人员重新识别的三个通用基准进行评估，即CUHK03，CUHK01 和VIPeR。  \n",
    "\n",
    "我们从描述我们用于提取特征的 CNN 体系结构开始。然后，我们报告对 CUHK03 的验证集进行评估，以分析 moderate positive mining，权重约束和 CNN 体系结构的影响。然后，我们将我们的性能与 CUHK03 和 CUHK01 上的最新方法进行比较。最后，我们展示了所提出的方法在 VIPeR 的小数据集上也表现出色，并且获得了有竞争力的结果。  \n",
    "\n",
    "### 4.1 CNN architecture  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/6/5.jpg?raw=true)\n",
    "\n",
    "CNN 由 3 个分支构建，具体细节如图 4 所示。输入图像标准化为 128×64 RGB。然后，它被分成三个 64×64 重叠色块，每个色块输入到一个分支汇总。每个分支由 3 个卷积层和 2 个 pooling 层组成。分支之间不执行参数共享。然后，3 个分支由具有 ReLU 激活的 FC 层结束。最后，输出特征向量 x 由另一个具有线性激活的 FC 层计算。为了计算的稳定性，在发送到度量标准学习层之前，要对这些要素进行标准化。CNN 和度量层通过反向传播联合学习。  \n",
    "\n",
    "与之前在 CUHK03＆01（IDLA，2.32M）和 VIPeR（DeepFeature，26M）上的最佳方法相比，我们的网络具有更少的参数（0.84M 参数）。我们用分支机构建立 CNN 架构的原因是为了从行人图像的不同人体部位学习特定的特征；同时，形态信息从人体各个部位保存下来。DML 采用了类似的架构，但分支之间的权重共享。在第 4.4 节中，实验展示了我们架构的优势。  \n",
    "\n",
    "### 4.2 Analysis of Moderate Positive Mining  \n",
    "\n",
    "CUHK03 包含 1369 个身份，每个身份有 10 个图像。默认协议随机选择 1,169 个对象用于训练，100 个用于验证，100 个用于测试。我们用训练集上的softmax 分类作为基线对 CNN 进行预训练。softmax 的输出对应于身份。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/6/6.jpg?raw=true)\n",
    "\n",
    "为了证明 moderate positive mining 的优势，我们比较了有和无 moderate positive mining 的验证集的性能。图5（a）显示了累积匹配特征（CMC）曲线和 rank-1 识别率。我们可以发现，moderate positive mining 和 hard negative mining 的合作取得了最好的结果（红线）。moderate positive mining 缺乏导致效果绩显着下降（蓝色）。这反映了如果所有正配对都不加区别地使用，那么 manifold 就会学习的不太好。  \n",
    "\n",
    "如果两种 mining 方法都不使用（洋红色），则网络在 low ranks 时的识别率非常低，甚至比基线（黑色）还差。这表明 moderate positive mining 和hard negative mining 对于训练都是至关重要的。  \n",
    "\n",
    "3 个训练网络的 CMC 曲线在排名超过 20 之后倾向于饱和，而基线网络保持相对较低的识别率。这表明，使用度量图层进行的训练是改进的基本贡献者。  \n",
    "\n",
    "损失值随着迭代不断的下降（如图5（b）所示）。图5（c）显示了一些正样本，这些正样本是在训练期间通过 moderate positive mining 得到的。与图1（a）中的那些难点相比，这些正样本的难度适中。  \n",
    "\n",
    "### 4.3 Analysis of Weight Constraint  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/6/7.jpg?raw=true)\n",
    "\n",
    "我们检查用正则化的不同相对权重 $\\lambda$ 学习的度量矩阵。在图6（a）中，我们显示了矩阵 M 的频谱。我们也在图6（b）中显示了相应的 rank-1 识别率。  \n",
    "\n",
    "当 $\\lambda = 10^2$ 时，奇异值几乎恒定在 1，这意味着度量层几乎给出欧几里德距离。这导致低方差和高偏差。随着 $\\lambda$ 的增加，矩阵在各维上具有不同的奇异值。这意味着学习的指标很适合训练数据，但更有可能过度拟合。因此，$\\lambda$ 的中等值给出了方差和偏差之间的折衷，这是良好性能的适当选择（图6（b））。  \n",
    "\n",
    "### 4.4 Analysis of Untied Branches  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/6/8.jpg?raw=true)\n",
    "\n",
    "我们在图7（a）中显示了 untied 分支学习的过滤器。该网络已经学会了显着的色彩表现，这与 IDLA 的结果一致。由于我们在分支之间应用 untied 权重，因此每个分支从它们自己的部分学习不同的过滤器。如图7（a）所示，每行显示一个分支的过滤器集，我们可以发现每个分支都有其自己的颜色重点。 例如，中间分支倾向于紫色和蓝色，而底部分支已经学习了比其他两个颜色明显更淡的过滤器。原因在于行人图像具有常规的人体外观。每个部分都有自己的颜色分布。因此，分支学习特定部分的过滤器。  \n",
    "\n",
    "我们比较了图7（b）中分支之间的有和无 tied 权重。我们增加 tied 分支网络中的过滤器数量，以便与 untied-branch 大致相等的参数数量。untied-branch 网络比 tied 分支的性能更好。它反映出，当网络具有一定的复杂性时，神经结构（tied vs untied-branch）变得非常重要。如何组织网络结构是获得良好性能的关键问题。  \n",
    "\n",
    "### 4.5 Performance on CUHK03  \n",
    "\n",
    "我们采用随机转换来增强训练数据。图像在水平和垂直方向随机裁剪（0-5像素），并拉伸以恢复大小。根据验证结果（4.3节），我们在以下所有实验中设置参数$\\lambda = 10^{-2}$。采用 moderate positive mining and hard negative mining。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/6/9.jpg?raw=true)\n",
    "\n",
    "CUHK03 有两个版本，一个有手动标记的图像，另一个检测到图像。我们在两个版本的测试集上评估我们的方法。我们将我们的表现与传统方法和深度学习方法进行比较。传统方法包括 LOMO-XQDA ，KISSME，LDM，RANK，eSDC，SDALF，LMNN，ITML，Euclid 。深度学习方法包括 FPNN 和 IDLA。IDLA 和LOMO-XQDA 在 CUHK03 上获得了以前的最佳表现。CMC 曲线和 rank-1 识别率如图 8 所示。我们的方法不仅在标签版本上，而且在检测版本上都比以前的先进方法获得了更好的性能。这表明我们的方法对检测不对准具有良好的鲁棒性。  \n",
    "\n",
    "### 4.6 Performance on CUHK01  \n",
    "\n",
    "CUHK01 数据集包含 971 个身份，每个身份有 2 个摄像头视图下的 4 个图像。根据文献中的协议，数据集被分成 871 个受试者的训练集和一个 100 的测试集。与最先进的方法 IDLA 一样，我们在 CUHK03 上训练网络，并在 CUHK01 上对其进行微调。我们比较我们的方法和前面提到的方法。CMC 曲线和 rank-1 识别率如图9（a）所示。我们的方法以 69% 的 rank-1 识别率获得最佳结果（红线）。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/6/10.jpg?raw=true)\n",
    "\n",
    "此外，为了检查数据集 CUHK01 的局限性，我们将最近发布的 Market1501 纳入训练。随着训练数据的增加，我们的网络具有更高的性能（红色虚线标记为“Ours \\*”），87% 的 rank-1 识别率。我们在图 10 中显示了某些失败的情况。在每个块中，我们从左到右给出真实的图库，探测和误报图像。我们发现大多数失败案例来自暗色图像或具有显着颜色对应关系的 negative pair。这种现象符合事实，网络中学习的过滤器主要集中在图像颜色上（如图7（a）所示）。当 true positive pairs 色彩不一致时，再次识别问题变得非常困难，而 negative pairs 具有相似的颜色（由于照明，相机设置等）。  \n",
    "\n",
    "### 4.7 Performance on VIPeR  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/6/11.jpg?raw=true)\n",
    "\n",
    "VIPeR 数据集包括 632 个身份，其中每个身份有来自两个不同相机的 2 个图像。虽然 VIPeR 是一个不适合训练 CNN 的小数据集，但我们仍然对这一具有挑战性的任务表现感兴趣。数据集被随机分成两个子集，每个子集具有相同大小的非重叠身份。这两个子集用于训练或测试。我们对 316 人训练集上的网络进行微调并在测试集上进行测试。我们还采用随机变化来增强数据。结果如图 9（b）所示。我们将我们的模型与 IDLA ，DeepFeature，视觉词（visWord），显着性匹配（SalMatch），补丁匹配（PatMatch），ELF，PRSVM，LMNNR，eBiCov，局部 Fisher 判别分析（LF），PRDC，aPRDC，PCCA ，中级滤波器（mFilter）和 mFilter 与 LADF 混合模型相比。我们的方法在 rank-1 的识别率达到 40.91%，这是 VIPeR 与现有深度学习方法相比的最佳结果。请注意，rank-1 识别率（43.39%）是通过两种方法（mFilter + LADF）的组合获得的。DeepFeature 的识别率接近于我们的 rank-1，但在更高的等级上要低得多。  \n",
    "\n",
    "## 5 Conclusion  \n",
    "\n",
    "行人数据的巨大变化对于人员重新识别方法来说是一个具有挑战性的问题。尽管 CNN 具有很强的特征提取的能力，但由于变化较大，行人数据在特征空间中非常不规则的分布。为了解决这个问题并训练强大的深度嵌入，应该有意识地选择正面的训练样本。在本文中，我们提出了一种新的 moderate positive mining 方法来 embed robust deep metric 来重新识别人员。我们发现，对 moderate positive 进行挖掘对于训练深度网络至关重要，特别是当涉及到具有较大类内变化（例如行人）的困难数据时。moderate positive 挖掘方法动态选择合适的正对，用于学习适应数据流形的鲁棒嵌入。此外，我们提出了权重约束来获得对人重新识别过度拟合问题的良好鲁棒性。  \n",
    "\n",
    "由于这些改进，我们的方法在 CUHK03 和 CUHK01 上实现了最新的效果，并在 VIPeR 上获得了有竞争力的结果。通过挖掘 moderate positive 的训练，我们可以减少类内差异，同时保留行人数据的内在图形结构；度量权重约束有助于提高网络的泛化能力，尤其是当大多数参数都在度量层中时。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Deep Neural Networks for Vehicle Re-ID with Visual-spatio-temporal Path Proposals  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "车辆重新识别是一个重要问题，在视频监控和智能交通领域有着广泛的应用。由于人员再识别技术的最新进展，它越来越受到关注。然而，与人员重新识别不同，车辆图像对之间的视觉差异通常是微妙的，并且对于人类来区分甚至是具有挑战性的。结合额外的时空信息对于解决具有挑战性的重新识别任务至关重要。现有的车辆重新识别方法忽略或使用过度简化的车辆图像之间的时空关系模型。在本文中，我们提出了一个两阶段框架，其中包含复杂的时空信息，用于有效调整重新识别结果。给定一对具有时空信息的车辆图像，首先通过具有深度函数的 chain MRF model 生成候选 visual-spatio-temporal\n",
    "path，其中每个 visual-spatiotemporal state 状态对应于实际车辆图像及其空间-时间信息。Siamese-CNN + Path-LSTM 模型采用候选路径以及成对查询来生成它们的相似度分数。大量的实验和分析表明了我们提出的方法和单个组件的有效性。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "车辆识别是计算机视觉领域的一个活跃的研究领域，其中包括车辆分类，车辆检测和车辆分割等应用。车辆重新识别（重新识别），目的在于确定两张图像是否从同一车辆拍摄的，最近引起了越来越多的关注。它在视频监控，公共安全和智能交通领域有着重要的应用。  \n",
    "\n",
    "从人员重新识别算法中推导出，大多数现有的车辆重新识别方法仅依赖于外观信息。这样的问题设置特别具有挑战性，因为不同的汽车可能具有非常相似的颜色和形状，对于属于同一制造商的那些汽车尤其如此。由于非正面摄像头视角，低分辨率或车辆图像照明不足，可能无法识别车辆的特殊线索，如车牌和特殊装饰。因此，仅使用外观信息进行准确的车辆重新识别是不实际的。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/7/1.jpg?raw=true)\n",
    "\n",
    "为了应对这些限制，初步尝试将输入图像的时空信息合并来进行更精确的车辆重新识别。Liu 等人利用每个车辆图像的时间和地理位置信息，每对图像之间计算时空亲和力。然而，它在空间和时间域中彼此接近的图像对才有用。这样的时空正则化明显过于简化。更重要的是，由数据集提供的车辆的重要时空路径信息被忽略。图 1 中说明了这种时空路径的必要性。如果在摄像机 A 和 C 都观察到车辆，则相同的车辆也必须出现在摄像机 B 处。因此，给定位置 A 和 C 处的一对车辆图像，如果在适当的时间从未在相机 B 观察到具有相似外观的图像，则它们的匹配可信度应该是非常低的。  \n",
    "\n",
    "在本文中，我们建议利用这样的时空路径信息来解决问题。我们的方法的主要贡献是有两个。（1）我们提出了一个车辆重新识别的两阶段框架。它首先提出一系列以查询图像作为起始和结束状态的候选视觉-时空路径。在第二阶段，利用 Siamese-CNN + Path-LSTM 网络来确定每个查询对是否与来自候选路径的时空正则化具有相同的车辆身份。这样，沿着候选路径的所有视觉-时空状态被有效地合并以估计路径的有效置信度。这种信息是第一次在车辆重新识别中引入。（2）为了有效地生成视觉-时空路径提议，我们通过 chain MRF 对路径进行建模，可以通过 max-sum 算法有效地对路径进行优化。提出了深度神经网络来学习成对的视觉-时空 potential function 。  \n",
    "\n",
    "## 2. Related Works  \n",
    "\n",
    "**车辆重新识别** 由于人体再识别方法的快速发展，近年来车辆识别技术开始受到关注。Feris 等人提出了一种基于属性的监控场景车辆搜索方法。车辆根据不同的属性进行分类，如车型和颜色。然后通过搜索数据库中具有相似属性的车辆进行检索。Dominik 等人利用 3D 边界框校正汽车图像，然后连接成对车辆图像的颜色直方图特征。训练二元线性 SVM 以验证这对图像是否具有相同的身份。Liu 等人提出了一个车辆重新识别数据集 VeRi-776，他们是由道路网中 20 台摄像机中捕获的大量汽车图像。车辆外观，时空信息和车牌用于独立的学习图像对之间的相似性分数。对于外观线索，使用深度神经网络来估计车辆图像之间的视觉相似性。其他车辆识别算法主要集中在细粒度汽车模型分类，而不是识别具有相同或不同身份的车辆。  \n",
    "\n",
    "**深度神经网络** 近年来，卷积深度神经网络在大规模图像分类，物体检测和视觉关系检测中表现出了有效性。对于序列数据，包括 LongShort Term Memory Network（LSTM）和门控递归神经网络在内的递归神经网络系列在包括图像字幕，语音识别，视觉问题回答，人物搜索，即时预测，视频分类和视频检测等任务中都取得了较好的效果。这些工作表明，RNN 能够捕获序列信息中的时序信息并有效的学习时间特征表示，这激励我们使用 LSTM 网络来学习用于视觉-时空路径分类的特征表示。  \n",
    "\n",
    "**Person re-identification** 人员重新识别是一个具有挑战性的问题，近年来引起越来越多的关注。最先进的人员重新识别方法采用深度学习技术。Ahmed 等人设计了一个成对验证 CNN 模型，用于将一对裁剪后的行人图像作为输入进行人员重新识别，并采用二元验证损失函数进行训练。Xiao 等人用分类损失训练 CNN 来学习人的深层特征。Ding 等人和 Cheng 等人用 triplet 样本训练 CNN 并且最小化同一人之间的特征距离，最大化不同人之间的距离。除了特征学习外，还提出了许多用于人员重新识别的度量学习方法。对于多摄像机系统中的人员重新识别，Hamdoun 等人提出了一种方法，匹配基于在多视频监控系统中用于人员识别方案的短视频序列上收集的兴趣点描述符的明显特征。  \n",
    "\n",
    "**Spatio-temporal relations** 时空关系被广泛用于多摄像机系统中的对象关联，Ellis 等人提出了一种方法来学习 trajectory 数据的拓扑和时间转换，这是从多摄像机网络中的单视点目标跟踪独立获得的。Neumann 等人提出了一种方法，将结构和运动估计结合在一个统一的框架中，以恢复对象的精确 3D 时空描述。Loy 等人提出了一种多摄像机活动相关性分析的方法，该方法估计摄像机网络的空间和时间拓扑结构。  \n",
    "\n",
    "## 3. Approach  \n",
    "\n",
    "对于利用计算机视觉进行车辆重新识别，需要在道路网络中的摄像机捕获过往车辆的图像。由于摄像机视点不合适，图像分辨率低和车辆运动模糊等因素，车牌信息可能无法一直解决任务。给定具有时空信息的一对车辆图像，需要两个车辆图像之间的相似性分数来确定两个图像是否具有相同的身份。每个图像与三种类型的信息相关联，即视觉外观，时间戳和相机的地理位置。我们称这些信息为车辆图像的视觉-时间状态。我们提出的方法以两个视觉-时空状态作为输入，并用两阶段框架输出它们的相似度得分，如图 2 所示。在阶段 1 中，我们不是仅考虑两个查询之间的简单配对关系，而是先对这两个输入生成一个候选 visual-spatio-temporal 路径。在阶段 2 中，候选 visual-spatio-temporal 路径充当正则化先验，并且利用 SiameseCNN + path-LSTM 网络来确定查询是否具有相同的身份。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/7/2.jpg?raw=true)\n",
    "\n",
    "### 3.1. Visual-spatio-temporal path proposals  \n",
    "\n",
    "给定一对查询，现有的车辆或人员重新识别算法主要考虑查询之间的配对关系，例如两个查询的视觉外观和时空状态的兼容性。如图 1 所示，这种成对关系通常过于简化，并且在实践中无法有效区分疑难案件。视觉-时空路径信息可以为实现更强大的重新识别提供重要信息。识别候选视觉-时空路径的问题被建模为马尔可夫随机场（MRF）。给定一对查询，首先将候选空间路径的地理位置标识为开始和结束位置。然后沿着空间路径的视觉-时空状态利用深度学习的成对潜在函数来优化以生成候选视觉-时空路径。  \n",
    "\n",
    "为了获得一对开始和结束位置的候选空间路径，从训练集中收集同一车辆经过的所有可能的空间路径。对于大型道路网络，可以预先收集每对位置之间的多个候选空间路径。  \n",
    "\n",
    "#### 3.1.1 Chain MRF model for visual-spatio-temporal path proposal  \n",
    "\n",
    "从候选空间路径集合中，我们的方法提出了一个候选的视觉-时空路径规则化车辆重新识别结果。沿着候选空间路径识别视觉和时间状态的问题被建模为优化 chain MRF 模型。  \n",
    "\n",
    "令 N 表示候选空间路径上摄像机的数量，其中 N 个摄像机中的每一个与 chain MRF 模型上的随机变量 X = {X1，X2，...，XN} 中的一个相关联。对于第 i 个随机变量（摄像机）Xi，其域是该摄像机中所有 k 个视觉-时空状态（具有它们的时空信息的所有 k 个图像）的集合，$S_i = \\{s_{i,1},...,s_{i,k}\\}$，其中 $s_{i,j} = \\{I_{i,j},t_{i,j},l_i\\}$ 是第 i 个摄像机的第 j 个视觉图像的三元组，其时间戳是 $_{i,j}$ 和摄像机位置 $l_i$。  \n",
    "\n",
    "设 p 和 q 表示两个查询的视觉-时空状态。基于候选空间路径获得最佳视觉-时空路径可以通过最大化以下分布来实现，  \n",
    "\n",
    "$p(x|x1 = p, xN = q) = 1/Z \\psi (p, x_2)\\psi(x_{N−1}, q) \\quad_{i=2}^{N-2} \\psi(x_i,x_{i+1})$\n",
    "\n",
    "其中 $\\psi(x_i,x_{i+1})$ 是同一辆车的 xi 和 $x_{i+1}$ 的成对势函数。理想情况下，如果xi和 $x_{i+1}$ 表示具有相同车辆身份的状态，则适当的潜在函数将具有大的值，否则为小的值。$\\psi$ 函数作为深度神经网络学习，并在下一小节详细介绍。  \n",
    "\n",
    "上述概率需要在适当的时间限制下最大化  \n",
    "\n",
    "$x^* = arg max_x p(X|x_1,x_N = q)$  \n",
    "$s.t \\ t_{i,k_i^*} \\leq t_{i+1,k_{i+1}^*} i \\in \\{1,...,N-1\\}$\n",
    "\n",
    "其中 $k_i^*$ 和 $k_{i+1}^*$ 分别代表 xi 和 $x_{i+1}$ 的最佳视觉-时空状态的指数。上述约束表明所获得的视觉-时空路径必须在时间上可行，即，车辆图像的时间戳必须沿路径保持增加。  \n",
    "\n",
    "该分布可以通过 maxsum 算法进行高效优化，这相当于 chain 模型的动态规划。概率的最大值可以写为，  \n",
    "\n",
    "$max_x p(x_1=p,x_N=q) = 1/Z max_{x_2}[\\psi(p,x_2) \\psi(x_2,x_3)[... max_{x_{N-1}}\\psi(x_{N-1},x_q)]...]$\n",
    "\n",
    "在获得候选空间路径上的每个随机变量（照相机）的最佳状态之后，生成候选视觉时空路径。  \n",
    "对于一对查询，获得多个候选视觉空间时间路径。我们定义了每条候选路径的最优解的经验平均 potential，  \n",
    "\n",
    "$S(x^*) = \\frac{1}{N-1} (\\psi(p,2) + \\sum_{1=2}^{N-2} \\psi(x_i^*,x_{i+1}^*) + \\psi(x_{N-1}^*,q))$\n",
    "\n",
    "其中 1/(N-1) 对具有不同长度的候选视觉-时空路径的整体置信度进行归一化。然后我们选择候选路径中的视觉-时空提议。  \n",
    "\n",
    "对于每一对查询，即使它们不具有相同的身份，所提出的算法总是试图在相邻摄像机之间的视觉和时空兼容性方面产生最可行的路径作为路径建议。图 3 显示了候选 visual spatio-temporal 路径的一些示例。  \n",
    "\n",
    "为了提高效率，当为数据集中的所有状态对生成视觉-时空路径时，我们的方法利用系统化的方法来避免冗余计算。例如，假设摄像机 A 和 C 具有两条可能的路径 A-B1-C 和 A-B2-C。当计算摄像机 A 和 C 上的查询之间的路径建议时，子路径 A-B1 和 A-B2 也被计算在计算过程中，可以被 A-B1 和 A-B2 上的其他查询重用。时间复杂度分析的细节将在 4.4 节中介绍。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/7/3.jpg?raw=true)\n",
    "\n",
    "#### 3.1.2 Deep neural networks as pairwise potential functions  \n",
    "\n",
    "(1) 中的成对势函数 $\\psi(x_i,x_{i+1})$ 评估相邻随机变量 xi 和 xi+1 的两个视觉时-空状态之间的兼容性。 我们将潜在函数 ψ 作为一个双分支深层神经网络学习，其结构如图 4 所示。视觉分支和时空分支估计成对视觉和时空状态之间的成对兼容性。  \n",
    "\n",
    "视觉分支（Siamese-Visual）被设计为一个共享 ResNet-50 的 Siamese 网络。它将摄像机 xi 和 xi+1 上的两个图像 $I_{i,k}$ 和 $I_{i+1,j}$，作为输入，并利用来自“global pooling”层的特征来描述它们的视觉外观。两幅图像之间的视觉相似度被计算为两个“global pooling”特征的后续 sigmoid函数的内积。  \n",
    "\n",
    "另一个分支计算时空兼容性。给定相机 i 和 i+1 处的时间戳 $\\{t_{i,k}，t_{i+1,j}\\}$ 和两个地理位置 ${l_i，l_{i+1}}$，分支的输入特征被计算为它们的时间差和空间差，  \n",
    "\n",
    "$\\Delta t_{i,i+1}(k,j) = t_{i+1,j} - t_{i,k}$  \n",
    "$\\Delta d_{i,i+1} = |l_{i+1} - l_i|$\n",
    "\n",
    "其中 $t_{i,k}$ 表示相机 i 处的第 k 个状态的时间戳。通过将连接的特征 $[\\Delta t_{i,i+1}(k,j),\\Delta d_{i,i+1}]^T$ 馈入具有两个全连接层的多层感知（MLP）、relu 层和 sigmoid 层来获得标量时空兼容性值。  \n",
    "\n",
    "将两个分支的输出连接并输入到具有 sigmoid 函数的 2×1 全连接层中，以获得两个状态之间的最终兼容性，其将所有视觉，空间和时间信息考虑在内。  \n",
    "\n",
    "为了训练 pairwise potential 网络 Siamese-CNN，我们首先预先训练 ResNet-50 网络，用分类交叉熵损失函数对车辆身份进行分类。然后收集邻居随机变量（摄像机）处的所有视觉-时-空状态对，用于微调整个网络。如果一对车辆具有相同的车辆身份，则将它们视为正样本，而具有不同身份的对视为负样本。正负抽样比率设定为 1：3。双分支网络用 0-1 交叉熵损失函数和随机梯度下降进行训练。  \n",
    "\n",
    "### 3.2. Siamese-CNN+Path-LSTM for query pair classification  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/7/4.jpg?raw=true)\n",
    "\n",
    "我们提出的候选视觉-时空路径提议算法为每个查询对生成最可行的路径，即使它们不具有相同的身份。对查询对的相似性进行排名的一种 naive 解决方案直接将它们的最大概率（方程（1））或经验平均潜力（方程（7））视为排名的最终相似度分数。但是，采取任何一种策略都存在限制。为了计算公式（1）中的最大概率，需要计算分区函数 Z，这通常是耗时的。对于方程（7）式中的经验平均的潜力，它是有偏见的，并将有利于更长的路径。图 5 说明了一个例子。给定两对具有不同路径长度的否定查询，因为路径建议算法试图为两对产生最可行的路径，所以沿每条路径可能只有一个标识开关。长路径的经验平均置信度会更高，因为低成对置信度会被更大的 N 减小。  \n",
    "\n",
    "给定一对查询，我们利用它们的候选视觉时空路径作为先验来确定查询对是否具有与 SiameseCNN + path-LSTM网络相同的身份。网络结构如图 6 所示。其中 Siamese-CNN 与 3.1.2 节中的整个网络具有相同的结构。它直接将查询对作为输入并估计查询之间的相似性。  \n",
    "\n",
    "对于候选视觉时空路径，采用 path-LSTM 来判断路径是否有效。path-LSTM 网络的框架如图 6 所示。path-LSTM 的每一步沿着候选路径处理一个步骤，因此 LSTM 的长度为 N-1。在每一步中，path-LSTM 的输入，yi 是摄像机 i 和 i+1 处视觉时空状态之间的视觉差异，空间差异和时间差异的拼接，然后是具有 32 个输出神经元和 ReLU 非线性函数的全连接层，即 $y_1 = f([\\Delta I_{i,i+1}^*, \\Delta d_{i,i+1}^*, \\Delta t_{i,i+1}^*])$，其中 f 表示全连接层的特征变换。  \n",
    "\n",
    "给定候选路径上的视觉时空状态，图像 $I_{i,k_i^*}$ 及其相关时间戳 $t_{i,k_i^*}$ 在摄像机 i 处固定，其中 $k_i^*$ 表示视觉-空间-时间的最优状态。  \n",
    "\n",
    "LSTM 由一个记忆单元 $c_t$ 和三个控制门组成：输入门 ig，输出门 og 和在每个时间步 t forget gate fg。利用输入特征 yt，LSTM 用以下等式更新记忆单元 ct 和隐藏状态 ht，  \n",
    "\n",
    "$fg_t = \\sigma (W_f \\cdot [h_{t−1}, y_t]) + b_f )$  \n",
    "$ig_t = \\sigma (W_i \\cdot [h_{t−1}, y_t]) + b_i )$  \n",
    "$og_t = \\sigma (W_o \\cdot [h_{t−1}, y_t]) + b_o )$  \n",
    "$ig_t = \\sigma (W_i \\cdot [h_{t−1}, y_t]) + b_i )$\n",
    "\n",
    "我们的 Path-LSTM 的隐藏神经元的数量被设置为 32。最后一步中的 Path-LSTM 的隐藏特征被馈送到完、全连接的层以获得有效路径置信度分数，其与由Siamese-CNN 生成的 pairwise 相似性分数来表示最终的相似性分数。通过这种方式，Path-LSTM 为估计最终匹配相似性提供了重要的正则化。  \n",
    "\n",
    "Siamese-CNN 和 Path-LSTM 分开预训练，然后联合微调。他们的训练样本的准备与第 3.1.2 节中学习成对势函数的训练样本类似。然而，这里的训练样本不再仅限于来自相邻摄像机的视觉-时空状态，而是限于整个摄像机网络中的任何摄像机对。Path-LSTM 首先用 Adam 算法预训练。整个Siamese-CNN + Path-LSTM 网络随后以端到端的方式进行微调。  \n",
    "\n",
    "## 4. Experiments  \n",
    "\n",
    "### 4.1. Dataset and evaluation metric  \n",
    "\n",
    "为了评估我们的车辆重新识别框架的有效性，我们对 VeRi-776 数据集进行了实验，该数据集是唯一提供空间和时间注释的现有车辆重新识别数据集。 VeRi-776 数据集包含超过 50,000 张带有身份标注，图像时间戳，相机地理位置，车牌，车型和颜色信息的 776 辆车图像。在 24 小时的时间段内，每辆车在 1 平方公里的城区内被 2 至 18 台摄像机拍摄。该数据集中训练组包括 576 辆车的 37,781 幅图像和测试集包括属于 200 辆车的 11,579 幅图像。 测试集中的 1,678 个查询图像的子集用于从所有其他测试图像中检索对应的图像。  \n",
    "\n",
    "mean average precision（mAP），top-1 accuracy and top-10 accuracy 被用来作为评估指标。给定测试图像子集中的每个查询图像以获取其他测试图像，每个查询 q 的平均精度由下式计算  \n",
    "\n",
    "$AP(q) = \\frac {\\sum_{k=1}^n P(k) \\cdot rel(k)} {N_{gt}}$\n",
    "\n",
    "其中 P(k) 表示 cut-off k 处的精度，如果在等级 k 处的项目是匹配的车辆图像，则 rel(k) 是等于1的指示函数，否则为零，n 是用于检索的数字，并且 $N_{gt}$ 表示查询到检索的数量。然后所有查询图像的 MAP 通过下式计算  \n",
    "\n",
    "$mAP = \\frac {\\sum_{q=1}^Q AP(1)}{Q}$\n",
    "\n",
    "其中 Q 是所有查询的数量（数据集为 1,678）。在 Liu 等人的实验设置之后，对于每个查询图像，只有来自其他摄像机的相同车辆的图像才会被考虑用于计算 mAP，top-1 和 top-5 精度。  \n",
    "\n",
    "由于我们提出的方法为每对查询图像生成一个视觉-时空候选路径，我们可以将评估度量扩展到整个图像序列。对于具有相同车辆身份的每一对查询图像，我们获得车辆的实际视觉时空路径，并使用 Jaccard 相似性将其与我们的候选路径进行比较，  \n",
    "\n",
    "### 4.2. Compared With Vehicle Re-ID methods  \n",
    "\n",
    "我们将我们提出的方法与最先进的方法进行比较，并在 VeRi-776 数据集上设计几条基线来评估我们提出的方法的有效性。  \n",
    "\n",
    "- Siamese-CNN + Path-LSTM 是我们的最终方法，它为 Siamese-CNN 提供查询视觉-时空状态对，并为 Path-LSTM 提出视觉-时空路径以生成最终相似度评分。请注意，我们的方法没有像[28]中那样利用图像中的车牌信息。  \n",
    "\n",
    "- FACT 和 FACT + Plate-SNN + STR。 Liu 等人提出的 FACT 模型结合了来自 GoogleNet 的深度学习视觉特征，BOW-CN 和 BOW-SIFT 特征来测量查询图像对之间的视觉相似性。在[28]中，他们进一步整合了车辆重新识别的视觉外观，车牌和时空信息。  \n",
    "\n",
    "  对于外观相似性，采用相同的 FACT 模型。他们利用 Siamese 神经网络（Plate-SNN）来比较车牌区之间的视觉相似性。时空相似度计算为  \n",
    "\n",
    "  $STR(i,j) = \\frac {T_i,T-j}{T_{max}} \\cdot \\frac{\\delta (C_i - c_j)}{D_{max}}$\n",
    "\n",
    "  其中 Ti 和 Tj 是两个查询的时间戳，$\\delta (C_i - c_j)$ 是两个摄像机之间的空间距离，Tmax 和 Dmax 是整个数据集中的最大时间距离和空间距离。 \n",
    "\n",
    "- Siamese-Visual。该基线通过仅使用第 3.2 节中的 SiameseCNN 的可视化分支来生成仅具有成对视觉信息的查询对之间的相似性。没有使用时空信息来获得相似性分数。  \n",
    "\n",
    "- Siamese-Visual + STR。这个基线不是通过深度神经网络学习时空关系，而是通过上面的 Siamese-Visual 和时空关系评分（STR，方程（22）中的）总结得分，两项之间的权重是手动搜索的最佳性能。\n",
    "\n",
    "- Siamese-CNN。 基线与 3.2 节中的 Siamese-CNN 相同，与上述基线相比，它使用两个查询的视觉和时空信息来确定它们的相似度得分，候选视觉时空路径在此基准中没有使用。  \n",
    "\n",
    "- Chain MRF 模型。在通过 3.1.1 节的 Chain MRF 模型获得查询对的候选视觉-时空路径之后，我们直接利用经验平均值作为查询对的相似度。  \n",
    " \n",
    "- Path-LSTM only。所提出的 Path-LSTM 估计所提出的视觉-时空路径的有效性得分。我们测试只使用 Path-LSTM 结果，而没有将它与 Siamese-CNN 结合起来。  \n",
    "\n",
    "- Siamese-CNN-VGG16。这与 SiameseCNN 相同，但仅用 VGG16 取代 ResNet50。  \n",
    "\n",
    "- Path-LSTM-VGG16。这与 Path-LSTM 相同，但仅以 Siamese-CNN-VGG16 取代了 Siamese-CNN。  \n",
    "\n",
    "- Siamese-CNN-VGG16 + Path-LSTM-VGG16。这与Siamese-CNN + Path-LSTM 相同，但仅用 VGG16 取代 ResNet50。  \n",
    "\n",
    "### 4.3. Experiment Results  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/7/5.jpg?raw=true)\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/7/6.jpg?raw=true)\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/reID/7/7.jpg?raw=true)\n",
    "\n",
    "表 1 和表 2 列出了方法的 mAP，top-1 和 top-5 精度。图 7 显示了比较方法的 CMC 曲线。图 8 显示了我们的方法实例车辆重新识别结果。  \n",
    "\n",
    "我们提出的两阶段方法 Siamese-CNN + Path-LSTM 优于最先进的方法和所有比较的基线，这证明了我们整体框架和单个组件的有效性。与只考虑成对视觉和时空信息的 Siamese-CNN 相比，我们的最终方法在 mAP 和 top-1 准确性方面有 4% 的增益。这样的性能提升表明，具有视觉-时空路径建议的 Path-LSTM 确实提供了用于鲁棒地估计车辆相似性的重要先验。与仅计算路径有效性得分的 Path-LSTM 相比，我们的最终方法在 mAP 和 top-1 准确性方面也增加了4%。这是因为要产生候选路径，我们提出的 chain MRF 模型总是试图发现最可行的视觉-时空路径。沿着路径的视觉-时空状态改变有时可能是微妙的，并且难以仅通过邻近相机处的状态的成对差异来捕捉。这个查询对之间明显的状态差异有时可以更容易地被 Siamese-CNN 捕获。因此，Path-LSTM 作为规范 Siamese-CNN 结果的强有力的先验，它们的组合显示出最佳的检索性能。  \n",
    "\n",
    "与 Chain MRF 模型相比，Path-LSTM 具有10% 的 mAP 增益，并且增加了 25% 的 top-1 精度。这样的结果表明，经验平均值不是候选路径的鲁棒路径有效性指标。我们训练有素的 Path-LSTM 能够捕获候选路径上更细微的状态变化，以估计正确的路径有效性分数。与 Siamese-Visual 相比，Siamese-CNN 在mAP（〜25%）和 top-1 准确度（〜40%）方面有显着的增长。这表明，与人重新识别不同，时空信息对于车辆重新识别至关重要，其中不同车辆之间的视觉差异对于具有相同颜色的车辆可能是微妙的。与采用[28]中的时空关系得分的 Siamese-Visual + STR 相比，我们的 Siamese-CNN 实现了更加准确的检索性能。我们的深层神经网络能够捕获查询对之间更复杂的时空关系。  \n",
    "\n",
    "Path-LSTM 显示了用候选视觉-时空路径规则化检索结果的强大能力。Path-LSTM 的有效性依赖于候选路径的正确性。如果候选路径确实对应于实际路径，则 Path-LSTM 可能对最终相似性分数具有负面影响。对于具有相同车辆身份的所有查询对，我们获得他们的实况视觉-时空路径并将其与我们提出的路线进行比较。计算平均 Jaccard 相似度。我们提出的具有深度学习潜能函数的 Chain MRF 模型实现了 96.39% 的 AJS。  \n",
    "\n",
    "我们还测试用我们管道中的 VGG16 替代 ResNet50。我们提出的总体框架（SiameseVGG16 + Path-LSTM-VGG16）和单个组件（Path-LSTM-VGG16）优于我们的 VGG16 基线（Siamese-CNN-VGG16）。  \n",
    "\n",
    "### 4.4. Time Complexity Analysis  \n",
    "\n",
    "在最坏的情况下，需要计算所有的成对 potentials，导致 $O(MK^2)$ 的复杂度，其中 M 是相机网络中边的数量（连接相邻的相机），K 是每个相机的平均状态数量。之后，所有路径建议的动态规划的时间复杂度也是 $O(MK^2)$，它利用 3.1.1 节中描述的技术来避免冗余计算。在 Q 查询对的总数上分摊，每个查询对的平均时间复杂度为 $O(MK^2/Q)$。在实践中，为了测试，在 19.4 万个查询对（图库 11579，1678 个查询）之间计算配对分数。由于我们有系统地避免冗余计算，每个查询对平均只需要 0.016s。  \n",
    "\n",
    "## 5. Conclusions  \n",
    "\n",
    "在本文中，我们提出了一个两阶段的车辆重新识别与视觉和时空信息的框架。现有方法忽略或使用有限的空间-时间信息来规范重新识别结果。我们提出的方法结合了正则化的重要视觉空间-时间路径信息。采用深度学习配对 potential 函数的 chain MRF 模型生成视觉-时空路径建议。通过 Siamese-CNN + Path-LSTM 评估此类候选路径建议，以获得各对查询之间的相似性分数。所提出的方法优于 VeRi-776 数据集中的最新技术方法。我们框架的大量组件分析显示了我们整体框架和单个组件的有效性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考  \n",
    "\n",
    "- 1 [https://arxiv.org/abs/1610.02984](https://arxiv.org/abs/1610.02984)  \n",
    "- 2 [DeepReID: Deep Filter Pairing Neural Network for Person Re-Identification](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Li_DeepReID_Deep_Filter_2014_CVPR_paper.pdf)\n",
    "- 3 [DeepReID: Deep Filter Pairing Neural Network for Person Re-Identification 解读](https://blog.csdn.net/yuanchheneducn/article/details/51005599)\n",
    "- 4 [Deep-Person: Learning Discriminative Deep Features for Person Re-Identification](https://arxiv.org/abs/1711.10658)\n",
    "- 5 [Deep-Person: Learning Discriminative Deep Features for Person Re-Identification 解读](https://blog.csdn.net/q295684174/article/details/79084632)\n",
    "- 6 [Revisiting Temporal Modeling for Video-based Person ReID](https://arxiv.org/abs/1805.02104) \n",
    "- 7 [Re-ranking Person Re-identification with k-reciprocal Encoding](https://arxiv.org/abs/1701.08398) \n",
    "- 8 [Embedding Deep Metric for Person Re-identification: A Study Against Large Variations](https://arxiv.org/abs/1611.00137) \n",
    "- 9 [Learning Deep Neural Networks for Vehicle Re-ID withVisual-spatio-temporal Path Proposals](https://arxiv.org/pdf/1708.03918.pdf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
