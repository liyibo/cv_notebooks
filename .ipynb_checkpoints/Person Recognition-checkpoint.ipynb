{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Learning Deep Features via Congenerous Cosine Loss for Person Recognition  \n",
    "\n",
    "## 1.1 Abstract\n",
    "\n",
    "人物识别目的是在复杂的场景中识别跨越时间和空间的相同身份。在本文中提出了一种新方法，通过训练一个网络来获得鲁棒且较好的特征来解决这个问题。我们直接比较和优化了两个特征之间的余弦距离 - 扩大类差异并减少了类内的差异。我们通过最小化样本与它们的质心之间的余弦距离来提出一种连续的余弦损失。这样的设计降低了复杂度，并且可以通过使用标准化输入的 softmax 来实现。我们的方法也不同于以前的人物识别工作，我们不对测试子集进行额外的训练。通过测量测试集中几个身体区域的相似性来确定人的身份。实验结果表明，所提出的方法比之前 state-of-the-arts 的方法有着更高的准确率。  \n",
    "\n",
    "## 1.2 Introduction  \n",
    "\n",
    "随着智能手机和数码相机需求的不断增长，今天的人们会拍摄更多照片来记录日常生活和故事。这种趋势迫切需要智能工具从个人数据，社交媒体或互联网上获取数以千计的图像中跨越不同的时间和空间来识别同一个人。以前的工作已经证明，由于许多因素，诸如非正面，不同的光照，外观的可变性，身份纹理等，在这种不受约束的环境中人物识别仍然是一个具有挑战性的问题。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/person/1.jpg?raw=true)  \n",
    "\n",
    "最近提出的 PIPA 数据集包含数千幅具有复杂场景和相似外观的图像。图像的亮度，大小和上下文变化很大，许多实例都有部分甚至没有面部。图 1 显示了来自训练和测试集的一些样本。以前的工作通过多线索，多层次的方式来识别同一个人，其中训练集仅用于提取特征并且后续分类器（SVM或神经网络）在 test 0 中被训练。识别系统在 test 1 组中进行评估。我们认为这样的做法在实际应用中是不可行的，因为第二次 test 0 的训练是附加的并且如果添加新样本需要重新训练。相反，我们的目标是提供一组鲁棒的，广义的特征表示，它们直接在训练集上训练，并通过测量测试集之间的特征相似度来识别人物。没有必要在测试 test 0 上进行训练，即使有新的数据进入，系统仍然运行良好。  \n",
    "\n",
    "如图 1 底部所示，green box 中的女人与 light green box 中的女人戴着类似的围巾，她的脸部部分出现或者在某些情况下脸部不会出现。为了获得鲁棒的特征表示，我们针对不同的区域训练几个不同深度模型，并结合不同区域特征的相似度得分来预测他们的身份。我们的重要发现是，在训练期间，交叉熵损失不能保证类别内样本的相似性。它放大了不同类别之间的差异，并忽略了同一类别的特征相似性。为此，我们提出了一个连续的余弦损失，即 COCO，以扩大类间差异，并缩小类内差异。它通过测量样本与其集群质心之间的余弦距离来实现。此外，我们还将每个 region patch 与预定义的基本位置对齐，以进一步使特征空间中的类别内的样本更加接近。这种对齐策略也可能使网络不太容易过拟合。  \n",
    "\n",
    "图 2 显示了我们提出的算法训练的 pipeline。图像中的每一个实例都用一个 ground truth head 标注，我们使用 RPN 框架分别训练一个脸部和人体检测器。然后使用人体姿势估计器来检测人的关键部位以定位上身区域。crop 四个区域后，我们进行仿射变换，将不同的 patch 从训练样本对齐到“基本”位置。分别在 PIPA 训练集上使用 COCO 损失训练四个深度模型来获取一系列鲁棒的特征。总而言之，这项工作的贡献如下：  \n",
    "\n",
    "- 提出一个连续余弦损失来直接优化类别内部和跨类别样本间的余弦距离。它是以简单的 softmax 实现的，具有较低的复杂度。\n",
    "- 设计了一个人体识别 pipeline，利用多个身体区域获得可区分的特征表达，而无需对测试集进行二次训练。\n",
    "- 通过仿射变换将区域 patch 与基准位置对齐，以减少样本间的差异，使网络不易过拟合。  \n",
    "\n",
    "## 1.3 Related Work  \n",
    "\n",
    "基于图像的人物识别旨在识别日常生活照片中人物的身份，这些问题可能会因杂乱的背景而变得复杂。Anguelov 等首先通过提出一个马尔科夫随机场框架来解决这个问题，以结合所有情境提示来识别人的身份。最近，Zhang 等人为这项任务引入了一个大规模数据集。他们积累了由深层模型训练的 poselet 级别人物识别器的线索，以补偿姿势变化。Li 等在 LSTM 中嵌入场景和关系上下文，并将人的识别作为序列预测任务。  \n",
    "\n",
    "人物重新识别是为了匹配摄像机中不同视角下的行人图像，并在视频中引发了许多重要的应用。现有的工作采用度量学习和中级特征学习来解决这个问题。 Yi 等用 Siamese 深度网络来学习图像对之间的相似性。人物识别与重新识别的主要区别在于数据流。前者是在不同的地点和时间识别同一个人。在大多数情况下，在不同场合中人物的外观会有很大的变化。后者是在连续的视频中检测人，这意味着外观和背景随着时间并没有很大的变化。  \n",
    "\n",
    "深层神经网络近年来很大程度地推进了计算机视觉领域的发展，提高了图像分类，目标检测和目标跟踪等任务的性能。深度学习成功背后的精髓在于其在高维空间中的非线性复杂性的优越表现力和大规模数据集，深层网络可以在很大程度上学习复杂的模式和代表性特征。  \n",
    "\n",
    "## 1.4 Algorithm  \n",
    "\n",
    "所提出的 COCO 损失是针对每个身体区域进行训练以获得鲁棒的特征。  \n",
    "\n",
    "### 1.4.1 Region detection  \n",
    "\n",
    "利用四个区域 $r \\in {1,...,4}$ 即脸部，头部，全身和上身来训练特征。先来介绍这些区域的检测。\n",
    "\n",
    "Face: 先用 Faster RCNN 中的 RPN 预训练一个人脸检测器。数据为来自互联网的大约为300,000 张图像。网络结构是浅层 ResNet 模型，移除了 res_3b 之后的层并添加两个损失层（分类和回归）。然后，我们对 PIPA 训练集中的人脸采用 COCO 损失对模型进行微调。人脸检测器识别脸部的 m 个关键点（眼睛，眉毛，嘴巴等），并通过平移，旋转和缩放将检测到的脸部 patch 进行标准化。设 $p,q \\in R^{m*2}$ 分别表示人脸模型检测的 m 个关键点和对齐结果。我们将 $P,Q$ 定义为两个仿射空间，然后仿射变换 $A:P \\mapsto Q$ 定义为：  \n",
    "\n",
    "$p \\mapsto q = Ap + b$\n",
    "\n",
    "其中 $A \\in R^{m*m}$ 是 P 中的线性变换矩阵，$b \\in R^{m*2}$是 Q 中的偏差。这样的对齐方案可以确保类别内和类别之间的样本没有很大的差异：如果网络是在没有对齐的情况下学习的，它必须区分更多的模式，例如人员之间不同的旋转，更容易出现过拟合；如果数据进行了对其，则可以更侧重于区分不同身份的功能，而不用管旋转，视角等。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/person/2.jpg?raw=true) \n",
    "\n",
    "Head, whole body and upper body：头部区域是作为每个人的 ground truth 给出的。为了检测整个身体，我们还在 RPN 框架中预先训练了一个检测器。 该模型在大型人类数据集上进行了训练，我们使用 4638717 副图像中的 87021 个人进行训练。网络结构是一个 inception 模型，最终的 pooling 被完全连接层所取代。为了确定上身区域，我们进行了人体姿态估计以识别身体的关键点，并由此确定上半身的位置。头部，全身和上半身的检测器基于 PIPA 训练集用 COCO loss 进行了微调，并做了面部部区域所述的类似的 patch 对准过程。图 2（c）显示了四个区域的对齐 patch。  \n",
    "\n",
    "### 1.4.2  Congenerous cosine loss for training  \n",
    "\n",
    "设计 COCO 损失是为了直接比较和优化两个特征之间的余弦距离。令 $f^{i,f} \\in R^{D*1}$ 表示第 i 个样本在区域 r 的特征向量，其中 D 是特征维数。为简洁起见，我们去掉上标 r，因为每个区域模型都经历了相同的 COCO 训练。我们首先定义一个小批次 $B$ 中两个特征的余弦相似性，如下所示：  \n",
    "\n",
    "$C(f^{(i)},f^{(j)}) = \\frac {f^{(i)}*f^{(j)}} {\\parallel f^{(i)} \\parallel \\parallel f^{(j)} \\parallel} $\n",
    "\n",
    "余弦相似度衡量两个样本在特征空间中的距离。理想的损失是增加类内样本的相似度，并扩大类间质心的距离。令 $l_i,l_j \\in {1,...,K}$ 为样本 $i,j$ 的标签，其中 K 为类别总数，我们最大化下面的损失：  \n",
    "\n",
    "$ L^{naive} = \\sum_{i,j \\in B} \\frac {\\delta (l_i,l_j) C(f^{(i)},f^{(j)})}{1 - \\delta (l_i,l_j) C(f^{(i)},f^{(j)}) + \\epsilon}$\n",
    "\n",
    "其中$\\delta(.,.)$ 是一个指示函数。这样的设计在理论上是合理的，但仍然存在计算效率低下的问题。此外，如果我们直接计算来自小批次的两个任意样本的损失，网络会遭受不稳定的参数更新，并且很难收敛。  \n",
    "\n",
    "受中心损失的启发，我们将类别 k 的质心定义为小样本 B 上特征的平均值：  \n",
    "\n",
    "$c_k = \\frac {\\sum_{i \\in B} \\delta (l_i,k) f^{(i)}}{\\sum_{i \\in B} \\delta (l_i,k) + \\epsilon} \\in R^{D*1}$\n",
    "\n",
    "结合上面两个公式，我们有以下样本 i 的输出需要去最大化：  \n",
    "\n",
    "$p_{l_i}^{(i)} = \\frac {exp C(f^{(i)},c_{l_i})}{\\sum_{k \\neq l_i} exp C(f^{(i)},c_k)} \\in R$\n",
    "\n",
    "公式通过类别质心来测量一个样本与其他样本的距离，而不是上面一样直接进行两两比较。确保样本 i 与其自己的类别 $l_i$ 足够接近。指数运算是将余弦相似度转换为归一化概率输出，范围从 0 到 1。  \n",
    "\n",
    "为此，我们提出了cogenerous cosine（COCO）损失，这是为了增加类内的相似性，并以合作的方式扩大跨类别的变化：  \n",
    "\n",
    "$ L^{COCO} = \\sum_{i \\in B} L^{(i)}  = -\\sum_{k,i} t_k^{(i)} log p_k^{(i)} = - \\sum_{i \\in B} log p_{l_i}^{(i)}$\n",
    "\n",
    "在实践中，可以通过 softmax 操作以一种简洁的方式实施 COCO loss：  \n",
    "\n",
    "$p_{k}^{(i)} = \\frac {exp (\\hat{c}_k^T \\cdot \\hat{f}^{(i)})}{\\sum_m exp (\\hat{c}_m^T \\cdot \\hat{f}^{(i)})} = softmax(z_k^{(i)})$\n",
    "\n",
    "其中 $z_k^{(i)} = \\hat{c}_k^T \\cdot \\hat{f}^{(i)}$ 是softmax的输入。 $\\hat{c}_k$ 可以看作是分类层中的权重，其中偏差项为零。  \n",
    "\n",
    "请注意，这些特征和集群质心都是端对端训练的。这些特征是从预训练模型初始化的，并且 $c_k$ 的初始值是通过公式计算的。  \n",
    "\n",
    "### 1.4.3  Relationship of COCO with counterparts  \n",
    "\n",
    "COCO 损失被表述为特征空间中的度量学习方法，其使用余弦距离中的集群质心作为度量来扩大类间变化以及缩小类内差异。它可以通过几个限制条件下的softmax 操作来实现。图 3 显示了不同损失方案下的特征集群的可视化。对于 softmax 损失，它只强制跨类别的样本很远，而忽略一个类中的相似性。在COCO 损失中，我们用 softmax 之前的分类层来替换权重，并且有明确定义和可学习的集群质心。中心损失与我们的某些方面相似，但是，它需要一个外部存储器来存储类的中心，因此计算量是我们的两倍。Liu 提出了一个广义的 large-margin 损失，它也通过强化类内紧致性和类间可分性来学习判别性特征。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/person/3.jpg?raw=true) \n",
    "\n",
    "### 1.4.4   Inference  \n",
    "\n",
    "在测试阶段，我们测量两个测试分片之间的特征的相似性，以基于 test_0 中的标签来识别 test_1 中的每个实例的身份。test_1 和 test_0 中的两个分片 i 和 j 之间的相似性由 $s_{i,j}^(r) = C(f^{(i,r)},f^{(j,r)})$，其中 r 表示特定的区域模型。关键问题是如何合并来自不同区域的相似度分数。 我们首先对初步结果$s^(r)$进行归一化处理，以使不同区域的分数具有可比性：  \n",
    "\n",
    "$\\hat{s}_{i,j}^{(r)} = (1 + exp [-(\\beta_0 + \\beta_1 s_{i,j}^{(r)})])^{-1}$\n",
    "\n",
    "其中 $\\beta_0, \\beta_1$ 是逻辑回归的参数。最终得分 $S_{i,j}$ 是每个区域的归一化得分 $s_{ij}^{(r)}$ 的加权平均值。test_1 中的 patch i的身份由与参考集中的最大分数相对应的标签决定：$l_i = arg max_{j*} S_{i,j}$。 这样的方案保证了当新的训练数据被添加到 test_0 中时，不需要在测试集上训练第二个模型或SVM，这与以前的工作非常不同。\n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/person/4.jpg?raw=true) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Person Recognition in Social Media Photos  \n",
    "\n",
    "## 2.1 Abstract  \n",
    "\n",
    "现今人们通过社交媒体分享他们个人生活的大部分内容。通过简化相册管理，能够自动识别个人照片中的人物可能会大大提高用户的便利性。然而，对于人类识别任务来说，计算机视觉的传统焦点是人脸识别和行人重新识别。社交媒体照片中的人物识别为计算机视觉带来了新的挑战，包括复杂的主题（靠后的视角，不寻常的姿势）和外观上的巨大变化。为了解决这个问题，我们利用来自多个图像区域（头部，身体等）的卷积特征构建了一个简单的人物识别框架。我们提出新的识别方案，重点关注训练和测试样本之间的时间和外观的差距。我们根据时间和视角的一般性对不同特征的重要性进行深入分析。在此过程中，验证了我们的方法实现了 PIPA 基准测试集上最先进结果，这是迄今为止最大的基于社交媒体的人物识别基准，具有多种姿势，视角，社交群组和事件。  \n",
    "\n",
    "与本文的会议版本相比，本文增加了(1)人脸识别器（DeepID2+）的分析；(2)结合会议版本方法 naeil 和 DeepID2 + 的新方法 naeil2，实现了 state of the art 的效果；(3)讨论自会议版本以来的相关工作，...。  \n",
    "\n",
    "## 2.2 Introduction  \n",
    "\n",
    "随着社交媒体的出现和图像捕捉模式从数码相机向智能手机和生活记录设备的转变，用户现在在线共享大量个人照片。通过简化相册管理，能够自动识别个人照片中的人物可能会大大提高用户的便利性。识别自然环境中的人物是一个有趣的挑战，人们可能将注意力集中在看不见人脸的活动上，或者可以改变服装或发型。这些挑战在很大程度上是新的 - 用于人类识别的传统计算机视觉研究的焦点是人脸识别（正面，完全可见的人脸）或行人重新识别（没有衣服变化，站立姿势）。  \n",
    "\n",
    "表面上看，识别自然环境中的人脸是这个任务中的重要因素。然而，当人们参与一项活动（即不摆姿势）时，他们的面部只会变得部分可见（非正面，遮挡）或完全不可见（背面）。因此，需要额外的信息来可靠地识别人员。我们探索其他线索，包括（1）包含形状和外观信息的人的身体;（2）性别和年龄等人的属性;（3）场景上下文。图 1 给出了几个需要更多线索才能正确识别的例子。  \n",
    "\n",
    "本文对社交媒体类型照片中的人物识别任务进行了深入分析：给每个人一些标注的训练图像，这个人在测试图像中是谁？ 本文的主要贡献总结如下：  \n",
    "\n",
    "- 提出具有挑战性的 PIPA 数据集人物识别方案。\n",
    "- 详细分析不同线索的信息，特别是人脸识别模块DeepID2 +。\n",
    "- 确认我们的期刊版本最终模型 naeil2 在 PIPA 上实现了最好的性能。\n",
    "- 根据外观和视点变化分析线索的贡献。\n",
    "- 在 openworld 识别设置下讨论我们的方法的性能。\n",
    "- 开放代码和数据：https://goo.gl/DKuhlY \n",
    "\n",
    "## 2.2.1 Related work  \n",
    "\n",
    "#### Data type  \n",
    "\n",
    "以前关于人物识别的大部分工作都集中在人脸上。室外标记人脸（LFW）一直是实验室环境以外的许多关于人脸识别和验证工作的极好测试平台。近年来，测试基准的准确率已经接近饱和，归因于在大规模人脸数据库训练的深层特征传统的方法比基于手工特征和度量学习方法的复杂分类器上有很大的提高。然而，LFW 对于社交媒体照片并不具有代表性：数据主要由未被遮挡的正面（面部检测）组成，并且数据集集中在公众人物上。事实上，最近的基准数据已经引入了更多复制类型的数据。IARPA Janus Benchmark A (IJB-A) 包含了人物的侧脸等，但仍限于公众人物。  \n",
    "\n",
    "不仅人脸，身体也被作为人物识别的线索进行了研究。例如，行人重新识别（re-id）解决了在不同摄像机视图中匹配行人检测的问题。标准 benchmarks 包括VIPeR，CAVIAR，CUHK 和 Caltech Roadside Pedestrians等，以前主要研究设计良好的手工特征，现在更多地关注于开发有效的深度特征。然而，典型的 re-id benchmarks 不能完全覆盖社交媒体图片的三个方面：（1）人物主要以站立姿势出现；（2）分辨率低；（3）匹配仅在短时间跨度内评估。  \n",
    "\n",
    "日常自然环境中的人体识别首先由“Gallagher收集的人物数据集”涵盖。 然而，与典型的社交媒体帐户的大小相比，该数据集很小（〜600个图像，32个身份），并且只有正面被标注。 MegaFace 也许是社交媒体照片上最大的已知开源脸谱数据库。 但是，MegaFace 不包含任何背影数据（由面部检测器修剪），并且由于数据处理步骤而不保留每个帐户统计信息（例如，每个帐户的照片数量）。我们在 PIPA 数据集上构建论文，也从 Flickr 爬取，规模相当大（约40k图像，约2k个身份），具有不同的外观、所有视角和遮挡水平的人体。头部用带有身份标签的边界框进行标注释。我们在下一章中更详细地描述了 PIPA。  \n",
    "\n",
    "#### Recognition tasks  \n",
    "\n",
    "存在多个与人物识别相关的任务，主要在训练和测试数据量方面存在差异。面部识别和监控重新识别通常通过验证完成：给定一个参考图像（图库）和一个测试图像，它们是否是同一个人？。在本文中，我们考虑两个识别任务。(1) 封闭的识别任务：给定一个测试图像，这个人是训练集中的哪个人？(2) 开放的识别任务：给出测试图像，这个人是否是训练集中的人，如果是，是谁？  \n",
    "\n",
    "其他相关的任务是人脸聚类，寻找重要人物，或者将文本中的名字与图像中的人脸相关联。  \n",
    "\n",
    "#### Prior work with the same data type and task  \n",
    "\n",
    "自从引入 PIPA 数据集以来，多个作品提出了不同的方法来解决社交媒体照片中的人物识别问题。 Zhang 等人提出了姿态不变人识别（PIPER），通过结合三个要素来获得有希望的结果：DeepFace（在大型私人数据集上训练的人脸识别模块）；poselets (在 2K 图片中 19 个关键点上训练的姿势估计模块)，以及 19 个关键点的卷积特征。  \n",
    "\n",
    "Oh 等人，即本文的会议版本提出了一个简单的模型 naeil 从多个固定图像区域提取 AlexNet 特征。这个模型不需要数据需求量大的 DeepFace 或费时的Poselet，同时实现了比 PIPER 略好的识别性能。  \n",
    "\n",
    "会议版本之后又有很多工作。 Kumar 等人通过使用姿态估计来标准化身体姿势从而改善性能。Li 等人考虑利用人物共现统计。Liu 等人已经提出训练一个度量空间中人物的特征向量，而不是在一组固定的身份上训练一个分类器，从而使得该模型更适应于陌生样本。一些作品利用了相册元数据，允许模型对不同的照片进行推理。  \n",
    "\n",
    "在这个期刊版本中，我们使用 naeil 和 DeepID2+ 构建了 naeil2，以实现关于 PIPA 已发表的工作成果。 我们根据时间和视角变化提供线索的进行了更多的分析。  \n",
    "\n",
    "## 2.3  Dataset and experimental setup  \n",
    "\n",
    "#### Dataset  \n",
    "\n",
    "据我们所知，PIPA 数据集（“人像摄影集”）是第一个用于注释人们身份的数据集，即使图片只是人物的背影。注释者标记了即使对于人类也可能被认为很难的实例。PIPA 拥有 Flickr 中个人相册图像，共有 63188 个头部标注，共 2356 个身份。数据集被划分为 train，val，test 和剩余集，粗略比例为45：15：20：20。本文不使用剩余集合。  \n",
    "\n",
    "#### Task  \n",
    "\n",
    "在测试时，给一张测试图片和该图像中人物头部的 bounding box，任务是在给定的一组身份（图库集，200〜500个身份）中选择出该测试样本中人物的身份。  \n",
    "\n",
    "在附录 B 中，我们评估测试实例可能是背景人员的方法（例如旁观者 - 没有给出训练图像）。然后系统还需要确定给定的实例是否在可见的身份(图库集)之中。  \n",
    "\n",
    "#### Protocol  \n",
    "\n",
    "我们遵循 PIPA 的协议进行数据训练和模型评估。训练集用于卷积特征训练，测试集包含测试身份的示例。对于每个身份，样本被分为 test0 和 test1。 为了评估，我们通过对其中一个分割进行训练并对另一个进行测试来执行双重交叉验证。验证集同样分为 val0 和 val1，用于探索不同的模型和调整超参数。  \n",
    "\n",
    "#### Evaluation  \n",
    "\n",
    "我们使用识别率（或准确率），即测试实例中正确的身份预测率。对于每个实验，我们平均从（训练，测试）对（val0，val1）和（val1，val0）获得的两个识别率 - 类似于测试。  \n",
    "\n",
    "### 2.3.1 Splits  \n",
    "\n",
    "我们考虑四种不同的方式来分割每个身份的训练和测试样本（val0 / 1和test0 / 1），目的是评估不同级别的泛化能力。 第一个来自以前的工作，我们介绍三个新的。 数据统计参见表1，图3用于可视化。  \n",
    "\n",
    "Original split O [1]  \n",
    "\n",
    "原始分割在整个分割中为每个标识分享许多相似的例子 - 例如 连续拍摄的照片。 原始分割因此很容易 - 即使最近的原始RGB像素也能工作（第4.1节）。 为了评估长期外观变化的泛化能力，我们在下面介绍三个新的分割。  \n",
    "\n",
    "Album split A [2]  \n",
    "\n",
    "专辑分割根据相册元数据为每个身份划分训练和测试样本。 在尝试匹配每个身份的样本数量以及整个分组样本的总数时，每个分组都会获取相册。 为了匹配样本数量，在分组之间共享几张专辑。 由于Flickr相册是用户定义的，并不总是严格聚集事件和场合，所以拆分可能并不完美。  \n",
    "\n",
    "Time split T [2]  \n",
    "\n",
    "时间分割根据拍摄照片的时间分割样本。 对于每个身份，样本都根据其“照片拍摄日期”元数据进行排序，然后根据最新与最旧的基础进行划分。 没有时间元数据的实例均匀分布。 这个分割评估识别器的时间泛化。 然而，“照片拍摄日期”元数据非常嘈杂，大量缺失数据。  \n",
    "\n",
    "Day split D [2]  \n",
    "\n",
    "日间分割通过视觉检查来分割实例，以确保公司在分裂中“外观变化”。 我们定义了两个划分标准：（1）日期变化的确切证据，例如{变化的季节，大陆，事件，共同发生的人}和/或（2）{发型，化妆，头部或 身体穿着}。 我们放弃无法进行此类划分的身份。 分割后，对于每个身份我们随机丢弃较大分组中的样本，直到大小匹配。 如果较小的分割≤4个实例，则完全丢弃标识。 日间分组使清洁实验能够评估整个外观和事件变化的泛化性能。  \n",
    "\n",
    "### 2.4.2 Face detection  \n",
    "\n",
    "PIPA中的实例由头部周围的人注释（紧挨着头骨）。 我们另外计算了PIPA上的面部检测，其目的有三个：（1）比较头部与面部的身份信息量（§3），（2）获得头部方向信息以供进一步分析（§5）和（3） 在测试时模拟没有地面真实头盒的场景（附录§B）。 我们使用开源的DPM人脸检测器[48]。  \n",
    "\n",
    "给定一组检测到的面孔（高于特定检测分数阈值）和地面真值头，根据重叠（交集超过联合）进行匹配。 对于匹配的头部，相应的面部检测告诉我们哪个DPM部件被触发，从而允许我们推断头部方向（正面或侧面视图）。 更多细节见附录§A。  \n",
    "\n",
    "使用DPM组件，我们在PIPA中分割实例如下：（1）检测到和正面（FR，41.29％），（2）检测到和非正面（NFR，27.10％）和（3）未检测到面部（NFD ，31.60％）。 我们将检测结果表示为未将地面真相头与背景相匹配。 见图2的可视化。  \n",
    "\n",
    "## 3 CUES FOR RECOGNITION  \n",
    "\n",
    "在本节中，我们调查了在社交媒体照片中识别人物的线索。 我们首先概述我们的模型。 然后，我们通过实验回答以下问题：固定身体区域（无姿势估计）（§3.4）有多丰富？ 场景上下文有多大帮助（§3.5）？ 它是头部还是脸部（头部减去头发和背景），更具信息性（§3.6）？ 我们通过使用扩展数据获得了多少（第3.7节和第3.8节）？ 专业人脸识别器（§3.10）的效果如何？ 本节中的研究仅基于val集。  \n",
    "\n",
    "### 3.1 Model overview  \n",
    "\n",
    "在测试时间，给定一个地面真实头部边界框，我们估计图4中描绘的五个不同区域。每个区域被馈送到一个或多个边界以获得一组线索。 这些提示连接起来形成一个描述该实例的特征向量。 在整篇论文中，我们写+来表示向量级联。 线性SVM分类器通过这个特征向量进行训练（一个与其余部分相比）。 在我们的最终系统中，除了DeepID2 + [3]以外，所有特征都使用AlexNet [41]的第七层（fc7）进行预先训练，用于ImageNet分类。 除了DeepID2 + [3]特性之外，线索在图像区域和所使用的微调（数据类型或代理任务）之间仅彼此不同，以改变AlexNet。  \n",
    "\n",
    "### 3.2 Image regions used  \n",
    "\n",
    "我们根据基础真实头部注释选择五个不同的图像区域（在测试时间给出，参见§2中的协议）。 头部矩形h对应于地面真实注释。 全身矩形b定义为（3×头宽，6×头高），头部位于全身顶部中心。 上半身矩形u是b的上半部分。 场景区域是包含头部的整个图像。  \n",
    "\n",
    "使用§2.2中讨论的DPM人脸检测器获得人脸区域f。 对于没有匹配检测的头盒（例如背部视图和遮挡脸部），我们使用列车组上的脸部位移统计数据从头部回归脸部区域。 图4中示出了五个相应的图像区域。  \n",
    "\n",
    "请注意，这些区域彼此重叠，并且取决于人物的姿势，它们可能完全关闭。 例如，对于说谎者而言，b可能包含比实际身体更多的背景。  \n",
    "\n",
    "### 3.3 Fine-tuning and parameters  \n",
    "\n",
    "除非另有说明，否则AlexNet会使用PIPA训练集（〜30k实例，〜1.5k身份）进行微调，在五个不同的图像区域裁剪，并进行30万次小批量迭代（批量为50）。 我们将这样得到的基本线索称为f，h，u，b或s，这取决于裁剪区域。 在val集上，我们发现微调提供了比非微调AlexNet系统增加约10％（pp）的系统增益（图5）。 我们对每个提示使用AlexNet的第七层（fc7）（4 096尺寸）。  \n",
    "\n",
    "我们针对每个身份训练具有正则化参数C = 1的一对一SVM分类器; 它在我们的初步实验中变成了一个不敏感的参数。 作为替代，天真的最近邻分类器也被考虑过了。 然而，在val集上，SVMs的表现一直比神经网络好10 pp左右。  \n",
    "\n",
    "### 3.4 How informative is each image region?  \n",
    "\n",
    "表2显示了每个地区单独和组合的val set结果。 头h和上身u是最强的个人暗示。 上半身比全身b更可靠，因为下半身通常被遮挡或从框架中切出，因此通常是牵引物。 不出意外，场景s是最弱的个人暗示，但它仍然是人物识别的有用信息（远高于偶然级别）。 重要的是，尽管像素重叠，但我们看到所有线索都相互补充。 总的来说，我们的功能和组合策略是有效的。  \n",
    "\n",
    "### 3.5 Scene (s)  \n",
    "\n",
    "除了精调的AlexNet，我们考虑了多种功能类型来编码场景信息。 sgist：使用Gist描述符[49]（512维）。 s0位置：我们不考虑使用在ImageNet上预先训练的AlexNet，而是考虑在“Places数据库”[50]（约250万幅图像）的205个场景类别上预先训练AlexNet（PlacesNet）。 splaces 205：我们也考虑使用每个场景类别（205个维度）的得分向量来代替4 096维度的PlacesNet特征向量。 s0，s3：最后我们考虑以与身体或头部相同的方式使用AlexNet（在PIPA人员识别训练集上进行零或300k次微调迭代）。 s3个地方：s0个地方进行了人员识别的微调。  \n",
    "\n",
    "Results  \n",
    "\n",
    "表3比较了val集的不同备选方案。 Gist描述符sgist只在略低于convnet选项的地方执行（我们也尝试了Gist的4 608维度版本，获得更差的结果）。 使用s0位置的原始（和更长）特征向量优于splaces 205的类别分数。有趣的是，在这种情况下，位置分类的预训练比针对对象分类的预训练（s0位置对s0）更好。 经过微调s3达到与s0相似的表现。  \n",
    "\n",
    "尝试不同组合的实验表明这些特征之间几乎没有互补性。 由于s0和s3之间没有很大的差别，为了简单起见，我们在所有其他实验中使用s3作为场景提示。  \n",
    "\n",
    "Conclusion  \n",
    "\n",
    "场景本身尽管很弱，但可以获得远高于机会水平的结果。 经过微调后，场景识别作为预训练替代任务[50]不能提供明显的（ImageNet）物体识别增益。  \n",
    "\n",
    "### 3.6  Head (h) or face (f)?  \n",
    "\n",
    "大部分面部识别工作都专注于脸部区域。 在相册的背景下，我们的目标是量化头部与脸部区域的信息量。 如§2.2所述，我们从DPM人脸检测器获得人脸区域f [48]。  \n",
    "\n",
    "Results  \n",
    "\n",
    "表2中f和h之间的差距大约为10％，突出了将头发和背景包括在脸上的重要性。  \n",
    "\n",
    "Conclusion  \n",
    "\n",
    "使用h比f更有效，但f结果仍然表现出公平的表现。 与其他身体线索一样，h和f之间存在互补性; 我们建议一起使用它们。  \n",
    "\n",
    "### 3.7 Additional training data (hcacd, hcasia)  \n",
    "\n",
    "众所周知，深度学习架构可从其他数据中受益。 PIPER [1]使用的DeepFace [5]被训练超过4·103人（私人SFC数据集[5]）的4.4·106面孔。 相比之下，我们的线索是通过ImageNet和PIPA的29·103面孔超过1.4·103人进行训练的。 为了测量训练对大数据的影响，我们考虑使用两个开源人脸识别数据集进行微调：CASIA-WebFace（CASIA）[51]和“跨时代参考编码数据集”（CACD）[52]。  \n",
    "\n",
    "CASIA包含0.5·106张10.5·103人的图片（主要是演员和公众人物）。 当对这些身份（使用头部区域h）进行微调AlexNet时，我们获得了hcasia提示。  \n",
    "\n",
    "CACD包含160.103面2,103人不同年龄的人。 尽管图像总数量少于CASIA，但CACD的每个身份（~2×）具有更多的样本数量。 hcacd提示是通过与hcasia相同的程序构建的。  \n",
    "\n",
    "Results  \n",
    "\n",
    "结果见表4的上半部分。 h + hcacd和h + hcasia改善超过h（分别为1.0和2.2 pp）。 额外的定点培训数据似乎有所帮助。 然而，由于数据分布不匹配，hcacd和hcasia本身比h差约5 pp  \n",
    "\n",
    "Conclusion  \n",
    "\n",
    "即使是来自不同类型的照片，额外的网点培训数据也可以提供帮助。  \n",
    "\n",
    "### 3.8  Attributes (hpipa11, upeta5)  \n",
    "\n",
    "虽然整体外观可能每天都在变化，但人们可以预期，稳定的长期属性提供了识别手段。 我们通过微调AlexNet功能来构建属性线索，而不是针对人员识别任务（像所有其他线索），而是针对属性预测代理任务。 我们考虑两个属性，一个在头部区域，另一个在上身区域。  \n",
    "\n",
    "我们在PIPA训练和训练集（1409 + 366）中注明了身份，包含五个长期属性：年龄，性别，眼镜，头发颜色和头发长度（详见表5）。 我们通过微调AlexNet功能来构建头部属性预测任务的hpipa11。  \n",
    "\n",
    "为了对属性提示hpipa11进行微调，我们考虑两种方法：针对所有属性训练单个网络作为具有S形交叉熵损失的多标签分类问题，或者分别调整每个属性的一个网络并连接特征向量。 val组的结果表明后者（hpipa11）比前者（hpipa11m）表现更好。  \n",
    "\n",
    "对于上半身属性特征，我们使用“PETA行人属性数据集”[53]。 该数据集最初对于19·103个全身行人图像具有105个属性注释。 我们选择了五项长期属性作为研究对象：性别，年龄（年轻成人，成年人），黑发和短发（详见表5）。 我们选择使用上身u而不是全身b来进行属性预测 - 庄稼的嘈杂程度要低得多。 我们使用属性预测任务对PETA图像上半部分的AlexNet特征进行训练，以获得提示upeta5。  \n",
    "\n",
    "Results  \n",
    "\n",
    "见表4中的结果.PIPA（hpipa11）和PETA（upeta5）注释的行为相似（相对于h和u，增益约为1 pp），并显示互补性（相对于h + u约5 pp增益）。 在考虑的属性中，性别对提高识别准确性（对于两个属性数据集）贡献最大。  \n",
    "\n",
    "Conclusion  \n",
    "\n",
    "添加属性信息可提高性能。  \n",
    "\n",
    "### 3.9  Conference version final model (naeil) [2]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1702.06890"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
