{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图片风格迁移就是将一幅图片的内容与另外一幅图片的风格融合在一起，形成一幅独特的照片，先来感受一下效果：  \n",
    "\n",
    "图片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 摘要：  \n",
    "\n",
    "1 输入：content picture + style picture，输出：style transfer picture\n",
    "\n",
    "2 使用预训练好的VGG网络提取图片特征，低层特征用于重建图像风格，高层特征用于重建图片内容  \n",
    "\n",
    "3 创建一幅随机噪声组成的图片，通过构建的 content loss 和 style loss 迭代优化这幅图片，最终作为输出结果  \n",
    "\n",
    "Note：content 和 style 图像的特征只计算一次，新建空的图片之后，迭代计算空图片在content 和 style 层的特征，计算当前的loss，反向更新空的图片像素值，最终该图片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优缺点  \n",
    "优点：可以端到端的分离和重组任意图像的内容和风格，用Gram矩阵来作为图像的风格表示，从而可以量化两幅图片的风格差异，具有创造力  \n",
    "\n",
    "缺点：每转换一次，都需要重新训练整个模型，速度有待优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法  \n",
    "\n",
    "通过在图像内容和风格之间形成复杂的相互作用，人类在绘画中掌握了创造独特视觉体验的技巧。到目前为止，这个过程的算法基础是未知的，并且不存在具有相似能力的人造系统。然而，在视觉感知的其他关键领域，如目标和人脸识别，人类的表现最近被一类生物学启发的视觉模型（深度神经网络）所证实。这里我们介绍一个基于深度神经网络创造高感知质量的艺术形象。该系统使用神经表示来分离和重组任意图像的内容和风格，为创造艺术图像提供神经算法。  \n",
    "\n",
    "论文中展示的结果是在 VGG 网络基础上产生的，VGG 是一个卷积神经网络，它可以在一个物体识别任务上与人类的表现相媲美，并且被广泛的介绍和描述.论文使用了由 16 个卷积层和 5 个 pooling 层组成的 19 层 VGG Network，没有使用任何全连接层。该模型是公开可用的，可以在 caffe-framework 中找到。对于图像合成，文章发现用平均池化代替最大池化操作改善了梯度流，可以生成更吸引人的效果。\n",
    "\n",
    "通常，网络中的每一层都定义了一个非线性滤波器组，其复杂度随着网络中层的位置而增加。因此，通过对该图像的滤波器响应，在CNN的每一层中对给定的输入图像 $x$ 进行编码。具有 $N_l$ 个不同滤波器的层包含 $N_l$ 个大小为 $M_l$ 的特征图，其中 $M_l$ 是特征图的长乘宽。因此，层 $l$ 中的响应可以存储在矩阵 $F^l\\in R^{N_l\\times M_l}$ 中，其中，$F_{ij}^l$ 是在层$l$中位置 $j$ 处的第 $i$ 个滤波器的激活。为了可视化在层次结构的不同层编码的图像信息，我们在白噪声图像上执行梯度下降以找到与原始图像的特征响应相匹配的另一图像。所以令 p 和 x 作为原始图像和生成的图像，$P^l$ 和 $F^l$ 是它们在图层 $l$ 中各自的特征表示。然后我们定义两个特征表示之间的平方误差损失:  \n",
    "\n",
    "$L_{content}(p,x,l) = \\frac{1}{2}\\sum_{i,j}(F_{ij}^l - P_{ij}^l)$\n",
    "\n",
    "从中可以使用标准误差反向传播来计算相对于图像 x 的梯度。因此，我们可以改变最初的随机图像 x，直到它在CNN的某一层产生与原始图像 p 相同的响应。FIg1 中用于内容重建的5个层分来来自于VGG中的\"conv1_1\"(a),\"conv2_1\"(b),\"conv3_1\"(c),\"conv4_1\"(d),\"conv5_1\"(e)。  \n",
    "\n",
    "在网络每一层的CNN响应之上，我们建立了一个风格表示方法，用来计算不同滤波器响应之间的相关性，其中期望值取决于输入图像的空间范围。这些特征相关性由 Gram 矩阵 $G^l\\in R^{N_l\\times N_l}$ 给出，其中，$G_{ij}$ 是第 l 层中矢量化特征映射 i 和 j 之间的内积：  \n",
    "\n",
    "为了生成与给定图像的样式相匹配的纹理，我们使用白噪声图像的渐变下降来找到与原始图像的样式表示匹配的另一个图像。这是通过最小化来自原始图像的 Gram 矩阵的条目与要生成的图像的格拉姆矩阵之间的均方距离来完成的。所以令 $\\vec{a}$ 和 $\\vec{x}$ 是原始图像和生成的图像，$A^l$ 和 $G^l$ 分别表示层 l 中的风格。那么该层对全部损失的贡献就是:  \n",
    "\n",
    "$E_l = \\frac{1}{4N_l^2M_l^2}\\sum_{i,j}(G_{ij}^l - A_{ij}^l)^2$  \n",
    "\n",
    "总的损失就是：  \n",
    "\n",
    "$L_{style}(\\vec{a},\\vec{x}) = \\sum_{l=0}^{L}w_lE_l$  \n",
    "\n",
    "其中$w_l$是每层对总损失的贡献的权重因子。\n",
    "\n",
    "为了生成将照片的内容与绘画风格相混合的图像，我们共同最小化白噪声图像与网络的一层中照片的内容表示以及在CNN的许多层次上图像的风格表示的距离。所以，让$\\vec{p}$ 是照片，$\\vec{a}$ 是艺术品。我们最小化的损失函数是  \n",
    "\n",
    "$L_{total}(\\vec{p},\\vec{a},\\vec{x}) = \\alpha L_{content}(\\vec{p},\\vec{x}) + \\beta L_{style}(\\vec{a},\\vec{x})$\n",
    "\n",
    "其中α和β分别是内容和风格重构的权重因子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验：  \n",
    "\n",
    "### 1 定义用于内容和风格重建的特征层  \n",
    "\n",
    "使用 image net数据预训练好的 VGG 网络来提取特征，论文指出高层特征可以用于重建图片内容，低层特征可以用于重建图片风格，用多层特征可以提取图片的多尺度特征，更好的进行特征提取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONTENT_LAYERS = ('relu4_2', 'relu5_2')\n",
    "STYLE_LAYERS = ('relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 前向提取content feature  \n",
    "\n",
    "content：输入的 content 图片  \n",
    "\n",
    "通过前向运行 VGG 网络，在定义好的 CONTENT_LAYERS 中提取图片特征，用于后面的 loss 计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute content features in feedforward mode\n",
    "g = tf.Graph()\n",
    "with g.as_default(), g.device('/cpu:0'), tf.Session() as sess:\n",
    "    image = tf.placeholder('float', shape=shape)\n",
    "    net = vgg.net_preloaded(vgg_weights, image, pooling)\n",
    "    content_pre = np.array([vgg.preprocess(content, vgg_mean_pixel)])\n",
    "    for layer in CONTENT_LAYERS:\n",
    "        content_features[layer] = net[layer].eval(feed_dict={image: content_pre})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 前向提取 style feature  \n",
    "\n",
    "styles：输入的 style 图片  \n",
    "\n",
    "文章引入Gram矩阵计算风格的差异,风格的差异就是两幅图的 Gram 矩阵的差异，从而可以量化图像的风格，进行后续 loss 的计算。  \n",
    "\n",
    "可以将Gram矩阵认为是同一层中不同的 feature map 之间的协方差矩阵，同一层中不同的 feature map 可以认为是“同等非线性复杂程度的feature map”。这种特征对物体的特征表达能力是相近的，可以进行互相关操作，进而综合利用这些特征从而表示图像的风格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute style features in feedforward mode\n",
    "for i in range(len(styles)):\n",
    "    g = tf.Graph()\n",
    "    with g.as_default(), g.device('/cpu:0'), tf.Session() as sess:\n",
    "        image = tf.placeholder('float', shape=style_shapes[i])\n",
    "        net = vgg.net_preloaded(vgg_weights, image, pooling)\n",
    "        style_pre = np.array([vgg.preprocess(styles[i], vgg_mean_pixel)])\n",
    "        for layer in STYLE_LAYERS:\n",
    "            features = net[layer].eval(feed_dict={image: style_pre})\n",
    "            features = np.reshape(features, (-1, features.shape[3]))\n",
    "            gram = np.matmul(features.T, features) / features.size\n",
    "            style_features[i][layer] = gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 新建空的图片，迭代优化  \n",
    "\n",
    "image ： 新建待优化的图片，也就是最终的融合后的图片  \n",
    "content_weight_blend ： 不同层的loss在总content loss 中的权重  \n",
    "tf.nn.l2_loss(net[content_layer] - content_features[content_layer]) ：content图片和待优化图片在相同feature map 上的平方误差损失  \n",
    "tf.nn.l2_loss(gram - style_gram) ： style图片和待优化图片在相同feature map 上的平方误差损失  \n",
    "total variation denoising ： 用于使输出的图片看起来更平滑  \n",
    "loss ： 最终待优化的损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make stylized image using backpropogation\n",
    "with tf.Graph().as_default():    \n",
    "    noise = np.random.normal(size=shape, scale=np.std(content) * 0.1)\n",
    "    initial = tf.random_normal(shape) * 0.256\n",
    "    \n",
    "    image = tf.Variable(initial)\n",
    "    net = vgg.net_preloaded(vgg_weights, image, pooling)\n",
    "\n",
    "    # content loss\n",
    "    content_layers_weights = {}\n",
    "    content_layers_weights['relu4_2'] = content_weight_blend\n",
    "    content_layers_weights['relu5_2'] = 1.0 - content_weight_blend\n",
    "\n",
    "    content_loss = 0\n",
    "    content_losses = []\n",
    "    for content_layer in CONTENT_LAYERS:\n",
    "        content_losses.append(content_layers_weights[content_layer] * content_weight * (2 * tf.nn.l2_loss(\n",
    "                net[content_layer] - content_features[content_layer]) /\n",
    "                content_features[content_layer].size))\n",
    "    content_loss += reduce(tf.add, content_losses)\n",
    "\n",
    "    # style loss\n",
    "    style_loss = 0\n",
    "    for i in range(len(styles)):\n",
    "        style_losses = []\n",
    "        for style_layer in STYLE_LAYERS:\n",
    "            layer = net[style_layer]\n",
    "            _, height, width, number = map(lambda i: i.value, layer.get_shape())\n",
    "            size = height * width * number\n",
    "            feats = tf.reshape(layer, (-1, number))\n",
    "            gram = tf.matmul(tf.transpose(feats), feats) / size\n",
    "            style_gram = style_features[i][style_layer]\n",
    "            style_losses.append(style_layers_weights[style_layer] * 2 * tf.nn.l2_loss(gram - style_gram) / style_gram.size)\n",
    "        style_loss += style_weight * style_blend_weights[i] * reduce(tf.add, style_losses)\n",
    "\n",
    "    # total variation denoising\n",
    "    tv_y_size = _tensor_size(image[:,1:,:,:])\n",
    "    tv_x_size = _tensor_size(image[:,:,1:,:])\n",
    "    tv_loss = tv_weight * 2 * (\n",
    "            (tf.nn.l2_loss(image[:,1:,:,:] - image[:,:shape[1]-1,:,:]) /\n",
    "                tv_y_size) +\n",
    "            (tf.nn.l2_loss(image[:,:,1:,:] - image[:,:,:shape[2]-1,:]) /\n",
    "                tv_x_size))\n",
    "    # overall loss\n",
    "    loss = content_loss + style_loss + tv_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考  \n",
    "\n",
    "1 [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576)  \n",
    "2 [neural-style tensorflow 源码](https://github.com/anishathalye/neural-style)  \n",
    "3 [DeepLearning-风格迁移](https://zhuanlan.zhihu.com/p/26646211)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
