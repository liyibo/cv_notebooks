{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本摘要简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、关键词提取  \n",
    "\n",
    "### 1.1 TF-IDF 算法关键词提取  \n",
    "\n",
    "- 输入文本进行分词，统计每个词语在该文本中的词频数（可以限制使用特定词性的词语）\n",
    "- 在较大语料上预先统计出全量(常用词)词的 idf 值，得到输入文本中每个词语的 tf_idf 值\n",
    "- 将 tf_idf 值作为权重，对统计得到的词语进行排序\n",
    "\n",
    "  TF=（词语在文章中出现次数）/ （文章总词数）  \n",
    "  IDF=log (语料库文档总数/(包含该词的文档数+1))  \n",
    "  TF-IDF = TF * IDF  \n",
    "    \n",
    "- 选取前 K 个词语作为关键词，就完成了关键词的抽取\n",
    "\n",
    "\n",
    "TF-IDF 方法虽然简单，但经典有效，速度也快，具有较强的普适性，该算法基本能应付大部分关键词抽取的场景。有些场景中，可以提高文本首段和标题中的词的权重。分词和词性标注的性能对关键词抽取的效果影响很大。\n",
    "\n",
    "\n",
    "下面是 jieba 抽取关键词的代码。   \n",
    "\n",
    "```\n",
    "def extract_tags(self, sentence, topK=20, withWeight=False, allowPOS=(), withFlag=False):\n",
    "    \"\"\"\n",
    "    Extract keywords from sentence using TF-IDF algorithm.\n",
    "    Parameter:\n",
    "        - topK: return how many top keywords. `None` for all possible words.\n",
    "        - withWeight: if True, return a list of (word, weight);\n",
    "                      if False, return a list of words.\n",
    "        - allowPOS: the allowed POS list eg. ['ns', 'n', 'vn', 'v','nr'].\n",
    "                    if the POS of w is not in this list,it will be filtered.\n",
    "        - withFlag: only work with allowPOS is not empty.\n",
    "                    if True, return a list of pair(word, weight) like posseg.cut\n",
    "                    if False, return a list of words\n",
    "    \"\"\"\n",
    "    if allowPOS:\n",
    "        allowPOS = frozenset(allowPOS)\n",
    "        words = self.postokenizer.cut(sentence)\n",
    "    else:\n",
    "        words = self.tokenizer.cut(sentence)\n",
    "    freq = {}\n",
    "    for w in words:\n",
    "        if allowPOS:\n",
    "            if w.flag not in allowPOS:\n",
    "                continue\n",
    "            elif not withFlag:\n",
    "                w = w.word\n",
    "        wc = w.word if allowPOS and withFlag else w\n",
    "        if len(wc.strip()) < 2 or wc.lower() in self.stop_words:\n",
    "            continue\n",
    "        freq[w] = freq.get(w, 0.0) + 1.0\n",
    "    total = sum(freq.values())\n",
    "    for k in freq:\n",
    "        kw = k.word if allowPOS and withFlag else k\n",
    "        freq[k] *= self.idf_freq.get(kw, self.median_idf) / total\n",
    "\n",
    "    if withWeight:\n",
    "        tags = sorted(freq.items(), key=itemgetter(1), reverse=True)\n",
    "    else:\n",
    "        tags = sorted(freq, key=freq.__getitem__, reverse=True)\n",
    "    if topK:\n",
    "        return tags[:topK]\n",
    "    else:\n",
    "        return tags\n",
    "```\n",
    "\n",
    "### 1.2 TextRank 算法关键词提取  \n",
    "\n",
    "- 输入文本进行分词\n",
    "- 以固定窗口大小(默认为5，通过span属性调整)，统计词之间的共现关系，共现一次就加 1，构建图\n",
    "- 计算图中节点的 PageRank\n",
    "- 排序，权重最大的前 K 个节点作为关键词输出\n",
    "\n",
    "TextRank 从图模型的角度提取文本的关键词，由于涉及网络构建和迭代计算，效率较低，实际应用效果并不比TFIDF有明显优势。好处是不用预先在较大的语料上计算 idf 值。  \n",
    "\n",
    "- pagerank 简介\n",
    "\n",
    "基本思想来源于 pagerank 算法，pagerank 算法有两个基本思想，如果一个网页被很多其他的网页链接到，就说明该网页很重要；如果一个网页被一个权值很高的网页链接到，则也要增加该网页的权重，pagerank 迭代计算公式如下所示：\n",
    "\n",
    "$S(V_i) = (1-d) + d * \\sum_{j \\in In(V_i)} \\frac {1}{|Out(V_j)|} S(V_j)$  \n",
    "\n",
    "该公式中，$V_i$ 表示某个网页，$V_j$ 表示链接到 $V_i$ 的网页（即Vi的入链），$S(V_i)$ 表示网页 $V_i$ 的 $PR$ 值，$In(V_i)$ 表示网页 $V_i$ 的所有入链的集合，$Out(V_j)$ 是网页 $V_j$ 链接指向的网页的集合。$|Out(V_j)|$ 是集合中元素的个数。$d$ 表示阻尼系数，如果仅仅有求和的部分，而没有阻尼系数部分，那么该公式将无法处理没有入链的网页的 $PR$ 值，因为根据该公式这些网页的 $PR$ 值为 $0$，但实际情况却不是这样，所以加入了一个阻尼系数来确保每个网页都有一个大于 $0$ 的 $PR$值，根据实验的结果，在 $0.85$ 的阻尼系数下，大约 $100$ 多次迭代 $PR$ 值能收敛到一个稳定的值，而当阻尼系数接近 $1$ 时，需要的迭代次数会陡然增加很多，且排序不稳定。公式中 $S(V_j)$ 前面的分数指的是 $V_j$ 所有出链指向的网页应该平分 $Vj$ 的 $PR$ 值，这样才算是把自己的票分给了自己链接到的网页。  \n",
    "\n",
    "- textrank 简介  \n",
    "\n",
    "textrank 迭代计算公式如下所示：\n",
    "\n",
    "$WS(V_i) = (1-d) + d * \\sum_{j \\in In(V_i)} \\frac {w_{i,j}}{\\sum_{v_k \\in Out(v_j)}w_{j,k}} WS(V_j)$  \n",
    "\n",
    "可以看到该公式比 pagerank 多了一个权重项，这里使用两个节点之间的共现次数作为权重  \n",
    "\n",
    "- jieba textrank 排序过程  \n",
    "\n",
    "1、使用文本中词语个数的倒数作为每个词语的初始权重，计算每个词语的 $\\sum_{v_k \\in Out(v_j)}w_{j,k}$\n",
    "\n",
    "```\n",
    "wsdef = 1.0 / (len(self.graph) or 1.0)\n",
    "for n, out in self.graph.items():\n",
    "    ws[n] = wsdef\n",
    "    outSum[n] = sum((e[2] for e in out), 0.0)\n",
    "\n",
    "```\n",
    "2、迭代计算 $WS(V_i)$  \n",
    "\n",
    "```\n",
    "sorted_keys = sorted(self.graph.keys())\n",
    "for x in xrange(10):  # 10 iters\n",
    "    for n in sorted_keys:                          # n 就是公式中的 V_i\n",
    "        s = 0                                      # s 是公式中的 \\sum In 部分\n",
    "        for e in self.graph[n]:                    # 遍历每个与 V_i 共现的词语 V_j\n",
    "            s += e[2] / outSum[e[1]] * ws[e[1]]    # 计算出 V_j 权重对 V_i 权重的攻陷程度\n",
    "        ws[n] = (1 - self.d) + self.d * s          # 更新 V_j 的权重\n",
    "```\n",
    "\n",
    "3、对词语权重进行修正，具体原因不详、可能是为了平滑的作用  \n",
    "\n",
    "4、按照词语权重从大到小排列词语，输出前 K 个作为关键词  \n",
    "\n",
    "\n",
    "### 1.3 TextRank 关键短语提取  \n",
    "\n",
    "提取关键词短语的方法基于关键词提取，可以简单认为：如果提取出的若干关键词在文本中相邻，那么构成一个被提取的关键短语。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、摘要提取  \n",
    "\n",
    "### 2.1 TextRank 摘要提取  \n",
    "\n",
    "与提取关键词的算法一样，只是这里将文本中每个句子看成图中的一个节点，计算两个句子之间有相似度，作为无向加权图中的权重。通过 textrank 公式计算得到权重最高的若干句子作为摘要。 \n",
    "\n",
    "句子相似度使用公式 $S(S_i,S_j) = \\frac {\\{w_k|w_k \\in S_i \\ and \\ w_k \\in S_j \\}}{\\log(|S_i|) + \\log(|S_j|)}$ 计算，其中分子是在两个句子中都出现的单词的数量，$|S_i|$是句子 $i$ 的单词数。  \n",
    "\n",
    "句子相似度理论上也可以使用其他方式计算，如余弦距离，DNN等方法。  \n",
    "\n",
    "### 2.2 Feature-Based 摘要提取  \n",
    "\n",
    "#### 2.2.1 TextTeaser 算法简介  \n",
    "\n",
    "算法主要考虑以下几个方面：\n",
    "\n",
    "1）句子长度，长度为某个长度的句子为最理想的长度，依照距离这个长度的远近来打分。\n",
    "\n",
    "2）句子位置，根据句子在全文中的位置，给出分数。（比如每段的第一句是核心句的比例大概是70%）\n",
    "\n",
    "3）句子关键词打分，文本进行预处理之后，按照词频统计出排名前10的关键词，通过比较句子中包含关键词的情况，以及关键词分布的情况来打分。 \n",
    "\n",
    "综合上述 3 步的打分做累加，然后倒排得到每个句子的重要性得分，此时要考虑到摘要的可读性，通俗的做法是按照句子在文章中出现的顺序来输出。  \n",
    "\n",
    "计算过程如下所示：  \n",
    "\n",
    "1、对输入文本分句  \n",
    "\n",
    "2、对文本分词，统计文本中词频数前十的词(过滤停用词)，并且给没个词计算一个权重，作为文本关键词  \n",
    "\n",
    "3、对文本标题分词，得到标题中的词汇(过滤停用词)  \n",
    "\n",
    "4、遍历所有的句子并分词，计算句子与标题词汇的相似度得分，句子长度得分，句子与文本关键词的相似度得分，句子位置得分(越靠前得分越高)  \n",
    "\n",
    "5、将上述计算的各个得分合并起来，计算句子的最终得分，并排序，去前 K 个句子作为文本的摘要  \n",
    "\n",
    "#### 2.2.2 SummaRuNNer 算法简介  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/summary/SummaRuNNer.jpg.jpg?raw=true)\n",
    "<center> **SummaRuNNer 模型** </center >\n",
    "\n",
    "模型计算过程如下所示：\n",
    "\n",
    "1、模型由一个两层的 bi-directional GRU-RNN 组成，第一层 RNN 操作在单词级，计算每个单词的隐藏层状态表示  \n",
    "\n",
    "2、第二层RNN操作在句子级，输入为 word-level 层的隐藏层向量经平均池化（average pooling）、首尾拼接而成的向量，得到的隐藏层向量作为文档中句子的表示  \n",
    "\n",
    "3、sentence-level 层隐藏层向量同样先经过平均池化、首尾拼接，然后再经过一个非线性变换，最终的结果作为整个文档的表示  \n",
    "\n",
    "4、将文档中句子的表示与文档的表示结合起来，计算当前句子和文章表示的相似度，计算出每个句子是摘要的概率，在最终选取摘要的时候并不是简单的分类，而是根据每个句子的概率高低排序，选择概率最高的前几句即可。"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
