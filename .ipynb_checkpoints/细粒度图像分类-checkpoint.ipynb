{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-based R-CNNs for Fine-grained Category Detection  \n",
    "\n",
    "\n",
    "**首先利用 Selective Search 等算法在细粒度图像中产生物体或物体部位可能出现的候选框（object proposal）。之后类似于 R-CNN 做物体检测的流程，借助细粒度图像中的 object bounding box 和 part annotation 可以训练出三个检测模型（detection model），一个对应细粒度物体级别检测，一个对应物体头部检测，另一个则对应躯干部位检测。然后，对三个检测模型得到的检测框加上位置几何约束，例如，头部和躯干的大体方位、以及位置偏移不能太离谱等。这样便可得到较理想的物体／部位检测结果。接下来将得到的图像块（image patch）作为输入，分别训练一个CNN，则该CNN可以学习到针对该物体／部位的特征。最终将三者的全连接层特征级联（concatenate）作为整张细粒度图像的特征表示。显然，这样的特征表示既包含全部特征（即物体级别特征），又包含具有更强判别性的局部特征（即部位特征：头部特征，躯干特征），因此分类精度较理想。但在Part-based R-CNN中，不仅在训练时需要借助bounding box和part annotation，为了取得满意的分类精度，在测试时甚至还要求测试图像提供bounding box。这便限制了Part-based R-CNN在实际场景中的应用。**\n",
    "\n",
    "## Abstract  \n",
    "\n",
    "通过特定对象局部区域细微外观差异得到的局部的语义可以进行细粒度分类。用于姿态标准化的表示的方法已经提出，但由于对象检测的困难，通常在测试时假定边界框标注。我们提出了一个细粒度分类模型，通过利用由下至上区域提议计算的深度卷积特征来克服这些限制。我们的方法学习整体和局部检测器，在它们之间强制学习几何约束，并从姿态标准化表示预测细粒度类别。对 Caltech UCSD 鸟类数据集的实验证实，我们的方法在端到端评估中胜过了最先进的细粒度分类方法，而无需在测试时间使用边界框。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "由于相关类别中特定区域外观的细微差异，视觉细粒度分类问题可能非常具有挑战性。与常见的识别相比，细粒度分类旨在区分不同品种或物种或产品模型，并且必须以物体姿态为条件进行可靠识别。面部识别是细粒度识别的经典案例，值得注意的是最好的面部识别方法都会去发现面部特征点并从这些位置提取特征。  \n",
    "\n",
    "因此，定位目标的局部区域对于建立目标之间的对应关系以及避免因为姿势和相机视角变化带来的影响是很重要的。已经有工作研究了基于部件的方法来解决这个问题。局部定位确实是许多姿态归一化表示方法的瓶颈。之前已经使用了 Poselet 和 DPM 方法再局部定位上取得了一定的成功;只有在测试时给出已知的边界框时，方法通常才能得到较好的局部定位。通过开发一种新颖的深度局部检测方案，我们提出了一种端到端的细粒度分类系统，该系统在测试时不需要给定对象边界框，并且可以实现与先前报道的方法相媲美的性能，这些方法需要测试时给出真实标注框来过滤误报的检测结果。  \n",
    "\n",
    "卷积网络最近在 ImageNet 挑战上的成功激发了将深度卷积特征应用于相关图像分类和检测任务的进一步工作。Girshick 等人通过将 CNN 应用于一组自下而上的候选区域提议，突破了对象检测的性能，与以前的最佳方法相比，PASCAL 检测性能提高了 30％ 以上。OverFeat 提出使用 CNN 进行目标位置的回来来进行检测。但是，利用深度卷积特征的进展并不局限于基本的目标检测。在许多应用中，如细粒度识别，属性识别，姿态估计等，合理的预测需要准确的区域定位。  \n",
    "\n",
    "特征学习已被用于细粒度识别和属性估计，但受限于局部区域的工程特征。DPD-DeCAF 使用 DeCAF 作为特征描述符，但依靠基于 HOG 的 DPM 进行局部定位。PANDA 学习了特定部分深度卷积网络，其位置取决于基于 HOG 的 poselet 模型。这些模型缺乏 R-CNN 检测的鲁棒性。在这项工作中，我们探索了一种统一的方法，它使用相同的深度卷积表示进行检测以及局部描述。  \n",
    "\n",
    "我们推测自下而上区域提议方法取得的进展，如 selective search，可以提高使目标及局部检测的性能。正如我们后面显示的那样，Caltech-UCSD 鸟类数据集使用 selective search proposals 的平均召回率为 95％。  \n",
    "\n",
    "在本文中，我们提出了一个局部定位模型，它通过利用自下而上 selective search proposals 上计算的深度卷积特征来克服先前的细粒度识别系统的局限性。我们的方法学习局部外观模型并强化局部之间的几何约束。我们的方法概述如图 1 所示。我们已经研究了不同的几何约束，包括在语义外观空间中以最近邻为条件的联合区域定位的非参数模型。用广泛使用的细粒度基准 Caltech-UCSD 鸟类数据集来评估我们的方法，并得到 state-of-the-art 结果。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/1/1.jpg?raw=true) \n",
    "\n",
    "## 2.  Related work  \n",
    "\n",
    "### 2.1 Part-based models for detection and pose localization  \n",
    "\n",
    "以前的工作已经提出了对对象局部外观和位置的显式建模，以便更精确地识别和定位。从图像结构开始，通过 poselets 和相关工作，许多方法已经联合定位了一系列的几何相关的区域。可变形零件模型（DPM），直到最近还是 PASCAL 物体检测最先进的方法，在与整个对象边界框相结合的位置上添加附加学习滤波器的模型部分，允许部件以已学会的变形成本对 anchor 进行偏移。DPM 将这种方法用于强监督训练，其中区域位置在训练时进行标注。这些方法的局限性在于它们使用弱特征（通常是 HOG）。  \n",
    "\n",
    "### 2.2 Fine-grained categorization  \n",
    "\n",
    "最近，大量的计算机视觉研究集中在许多领域的细粒度分类问题，如动物品种或物种，植物物种和人造物体。  \n",
    "\n",
    "有几种方法基于检测和提取物体某些部分的特征。Farrell 等人提出了一种使用 poselets 的姿态归一化表示法。可变形零件模型在也被用于局部定位。基于定位脸部基准特征点的工作，Liu等人提出了一种基于样本的几何方法来检测狗脸并从关键点提取高度局部化的图像特征以区分狗的品种。此外，Berg 等人通过学习每对关键点的描述符，学习了一组高度区分的中间特征。此外，还有文章通过强化姿态和子类一致性来扩展以前的非参数样本法。Yao 等人和Yang 等人研究了模板匹配方法以降低滑动窗口方法的成本。 Goring 等人最近的工作将局部标注从具有类似全局形状的对象转换为非参数局部检测。但是，所有这些 part-based 的方法在测试时都需要真实边界框来进行局部定位或关键点预测。  \n",
    "\n",
    "Human-in-the-loop 的方法要求人们命名对象的属性，点击某些部分或标记最具区分性的区域以提高分类的准确性。基于分割的方法对于细粒度识别也是非常有效的。有方法使用 region-level 线索来推断前景分割 mask 并丢弃背景中的噪声视觉信息。Chai 等表明联合学习局部定位和前景分割可以有益于细粒度分类。与大多数先前的 part-based 的方法类似，这些方法需要真实边界框来初始化分割种子。相比之下，我们的工作目标是在测试时不知道真实标注框的情况下进行端到端的细粒度分类。我们的局部检测器使用基于自底向上区域提议的卷积特征，以及学习的非参数几何约束来更精确地定位对象区域，从而实现强大的细粒度分类。  \n",
    "\n",
    "### 2.3  Convolutional networks  \n",
    "\n",
    "近年来，卷积神经网络（CNN）已被纳入各种领域的许多视觉识别系统中。这些模型的强度至少有一部分在于它们能够从原始数据输入（例如图像像素）中学习辨别特征，这与作为初始预处理步骤计算图像上的手工设计特征的更传统的物体识别流水线相反。LeCun 及其同事最初将这些模型应用于数字识别和OCR，后来应用于通用对象识别任务，CNN 得到了推广。随着大型标签图像数据库和 GPU 实现的引入，这些网络已经成为通用对象分类中最准确的方法。  \n",
    "\n",
    "最近，通用对象检测方法已经开始利用深度 CNN，并且胜过基于传统特性的任何竞争方法。 OverFeat 使用 CNN 在粗滑动窗口检测框架中回归对象位置。对我们工作特别有启发的是 R-CNN 方法，该方法利用区域提案框架中 CNN 的深度特征，在 PASCAL VOC 数据集上实现前所未有的目标检测结果。我们的方法通过将 R-CNN 应用于除了整个对象之外的局部检测来推广 R-CNN，我们的实证结果证明这对精确的细粒度识别是至关重要的。  \n",
    "\n",
    "## 3.  Part-based R-CNNs  \n",
    "\n",
    "虽然证明了 R-CNN 方法在通用物体检测任务（PASCAL VOC）上的有效性，但没有探索这种方法同时在定位和细粒度识别中的应用。因为我们的工作是在这个机制中运作的，所以我们扩展了 R-CNN 来检测物体并将它们的局部定位并对定位进行几何约束。关于感兴趣对象的单个语义部分位置的假设（例如，用于动物类的头部位置），对于倾向于出现在相对于这些粗略固定的位置中的微妙外观差异进行建模变得合理部分。  \n",
    "\n",
    "在 R-CNN 方法中，对于特定对象类别，具有 CNN 特征描述符 $\\phi (x)$ 的候选检测 x 被赋予 $w_0^T \\phi (x)$，其中 $w_0$ 是对象类别的 SVM 学习向量的权重。在我们的方法中，我们假设一个强有力的监督设置，在训练时间，我们不仅为整个对象设定了真实标注框，而且对一组固定的局部区域也进行了标注 ${p_1,p_2,...,p_n}$。  \n",
    "\n",
    "在给定这些局部标注的情况下，在训练时，所有对象及其每个区域都被视为独立的对象类别：我们在区域提议中提取的特征描述符上训练一对多线性 SVM，其中与标注框重叠超过 0.7 的 proposal 被标注为该区域的正例，并且与任何标注框区域重叠小于 0.3 的区域被标记为负样本。因此，对于单个对象类别，我们为整体和局部 ${p_1,p_2,...,p_n}$ 分别学习了 SVM 权重 $w_0$ 和 ${w_1,w_2,...,w_n}$。在测试时，对于每个 proposal，我们计算来自所有SVM 的分数。当然，这些分数并不包含任何有关物体及其局部几何约束的知识；例如，在没有任何额外限制的情况下，鸟头检测器可以在检测器检测区域之外被检测到。因此，我们的最终联合对象和局部假设是使用下面几节中详述的几何打分函数来计算的，它强化了姿势预测与训练时观察姿势的统计数据一致的直观理想属性。  \n",
    "\n",
    "### 3.1 Geometric constraints  \n",
    "\n",
    "设 $X = {x_0，x_1，...，x_n}$ 表示对象 $p_0$ 和 n 个局部区域 ${p_i}_{i=1}^n$ 的位置（边界框），它们在训练数据中被标注，但在测试时是未知的。我们的目标是在先前未见过的测试图像中推断出物体位置和局部区域位置。给定目标和局部区域的 R-CNN 权重 ${w_0，w_1，...，w_n}$，我们将有相应的检测器 ${d_0，d_1，...，d_n}$，其中每个检测器得分为 $d_i(x)= \\sigma (w_i^T \\phi (x))$，其中 $\\sigma (\\cdot)$ 是 sigmoid 函数，$\\phi (x)$ 是在位置 x 处提取的 CNN 特征描述符。我们通过解决以下优化问题来推断对象和局部区域的联合配置：  \n",
    "\n",
    "$X^* = arg \\ max_X \\ \\Delta(X) \\prod_{i=0}^n d_i(x_i)$\n",
    "\n",
    "其中 $\\Delta(X)$ 定义了边界框在联合配置上的评分函数。我们考虑并报告了几个配置评分函数 $\\Delta$ 的定量结果，在以下段落中详细介绍。  \n",
    "\n",
    "框约束。 对物体和零件进行局部化的一个直观的想法是考虑每个可能的物体窗口和物体内的所有窗口，并选择具有最高分数的窗口。 在这种情况下，我们定义评分函数  \n",
    "\n",
    "**边界框约束**。对物体和局部区域进行定位的一个直观的想法是考虑每个可能的物体窗口和物体内的所有窗口，并选择具有最高分数的窗口。在这种情况下，我们定义评分函数  \n",
    "\n",
    "$\\Delta_{box}(X) = \\prod_{i=0}^n c_{x_0}(x_i)$\n",
    "\n",
    "其中  \n",
    "\n",
    "$ c_x(y) = \\begin{cases} 1,&\\text{if region y falls outside region x by at most $\\epsilon$ pixels} \\\\ 0,&\\text{otherwise} \\end{cases} $\n",
    "\n",
    "在我们的实验中，$\\epsilon = 10$。  \n",
    "\n",
    "**几何约束**。由于单个的区域检测器不完善，单个区域检测器得分最高的窗口并不总是正确的，特别是在有遮挡的情况下。因此，我们考虑几个评分函数，以限制检测结果相对于目标位置的布局从而过滤掉不正确的结果。我们定义  \n",
    "\n",
    "$\\Delta_{geometric}(X) = \\Delta_{box}(X) \\prod_{i=0}^n \\delta_i (x_i)$\n",
    "\n",
    "其中 $\\delta_i$ 是给定训练数据的区域 $p_i$ 的位置的评分函数。在之前关于区域定位的工作中，我们试验了$\\delta$的三个定义：  \n",
    "\n",
    "- $\\delta_i^{MG}(x_i)$ 将训练数据中区域 $p_i$ 的 $N_g$ 个成分拟合一个高斯混合模型。实验中，我们设定 $N_g = 4$。  \n",
    "\n",
    "- $\\delta_i^{NP}(x_i)$ 找出 $\\tilde{x}_0$ 相似的 K 个最近邻，其中 $\\tilde{x}_0 = arg \\ max d_0(x_0)$ 是检测器检测结果评分最高的窗口，然后将这 K 个 窗口拟合一个高斯模型。实验中，设置 K = 20。图 2 展示了最近邻的一些例子。  \n",
    "\n",
    "DPM 用每个分量的高斯先验来模拟 deformation costs。 R-CNN 是一个单成分模型，需要定义$\\delta^{MG}$ 或 $\\delta^{NP}$。我们的 $\\delta^{NP}$ 定义受 Belhumeur 等人的启发，但不同之处在于我们在外观上而不是几何上定义最近邻。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/1/2.jpg?raw=true)\n",
    "\n",
    "### 3.2 Fine-grained categorization  \n",
    "\n",
    "我们从局部区域以及整个对象中提取语义特征。最终的特征表示是 $ [\\phi (x_0),...,\\phi (x_n)] $ 其中 $x_0$ 和 $x_{1...n}$ 是使用上一节中的模型推导的整体和局部检测结果，$ \\phi (x_i) $ 是区域 $x_i$ 的特征表示。  \n",
    "\n",
    "在一组实验中，我们从 ImageNet 预先训练的 CNN 中提取深卷积特征 $ \\phi (x_i) $，类似于 DeCAF。为了使 CNN 特征在细粒度鸟类分类的目标任务具有更强去区分性，我们还将从 CUB 数据集根基标注框切分的 200 类鸟类分类任务对 预训练的 CNN 网络进行 fune-tune。特别是，我们用一个新的 200 路 fc8 分层替换了原始的 1000 路 fc8 分类层，并随机初始化这层的权重。我们根据 R-CNN 的建议设置微调的学习率，将全局速率初始化为初始ImageNet 学习速率的十分之一，并在整个训练过程中将其降低 10 倍，但在新的 fc8 中学习速率这是全局学习率的 10 倍。对于整个对象边界框和每个区域边界框，我们独立地对 ImageNet 预先训练的 CNN 进行微调，以对每个区域进行分类，对每个区域进行调整为 227×227 网络输入大小。测试时，我们使用针对特定整个对象或局部区域进行微调的网络，为预测的整个对象或局部区域提取特征。  \n",
    "\n",
    "为了对分类器进行训练，我们采用了一个一对多的线性 SVM 来使用最终的特征表示。对于新的测试图像，我们将全部和局部检测器与几何打分函数一起应用，以获取检测到的局部区域位置并使用特征进行预测。如果在测试图像中没有检测到特定区域 i（由于所有提议低于局部检测器的阈值），我们设置其特征 $ \\phi (x_i) = 0$（零矢量）。  \n",
    "\n",
    "## 4. Evaluation  \n",
    "\n",
    "在本节中，我们对提出的方法进行了评估。具体而言，我们在广泛使用的细粒度基准 Caltech-UCSD 鸟类数据集上进行了实验。分类任务是区分 200 种鸟类，并且由于类别之间的高度相似性从而对计算机视觉系统具有挑战性。它包含 200 种鸟类的 11,788 幅图像。每个图像都用边界框和 15 个关键点的图像坐标标注：喙，背部，胸部，腹部，额头，冠，左眼，左腿，左翼，右眼，右腿，右翼，尾巴，颈背 和喉咙。我们对包含在数据集中的分组进行训练和测试，其中每个物种约 30 个训练样本。遵循之前的协议，我们使用鸟类数据集的两个语义部分：头部和身体。  \n",
    "\n",
    "我们使用开源软件包 Caffe 来提取深度特征并微调我们的 CNN。对于目标和局部区域检测，我们使用 Caffe 参考模型，它与 Krizhevsky 等人使用的模型几乎相同。我们将 CNN 第 n 层的深层特征称为 convn，pooln 或 fcn，分别是卷积层，汇聚层或完全连接层的输出。我们使用 fc6 来训练 R-CNN 目标和局部区域检测器以及用于分类的图像表示。对于 $\\delta^{NP}$，使用 pool5 和余弦距离度量来计算最近邻。  \n",
    "\n",
    "### 4.1 Fine-grained categorization  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/1/3.jpg?raw=true)\n",
    "\n",
    "我们首先展示 Caltech-UCSD 鸟类数据集的标准细粒度分类任务上的结果。表 1 中的第一组结果是在测试时整个鸟类的真实标注框已知的情况下实现的，因为大多数 state-of-art 方法假设这样会使分类任务更容易一些。在这种情况下，我们具有局部非参数几何约束 $\\delta^{NP}$ 的 part-based 方法在没有微调的情况下效果最好，无需微调即可实现 68.1％ 的分类精度。 Finetuning 大幅改善了这一结果，达到 76％ 以上。我们将结果与三种最先进的基准方法进行比较。我们使用深度卷积特征，但他们使用基于 HOG 的 DPM 作为其局部定位方法。性能的提高可能是由于更好的局部区域定位（见表4）。Oracle 方法在训练和测试时都使用真实的整体和局部标注。  \n",
    "\n",
    "第二组是在测试时未知鸟的标注框的结果。大多数的文章都没有展示在这个更真实、更复杂数据集上的效果。如表 1 所示，在此设置中，我们 part-based 的方法比基准 DPD 模型效果更好。我们在不进行微调的情况下获得 66.0％ 的分类精度，几乎与给出真实边界框时的精度一样好。这意味着在测试时不需要对任何目标进行标注就可以进行分类。通过微调 CNN 模型，我们的方法分类准确率达到 73.89％。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/1/4.jpg?raw=true)\n",
    "\n",
    "我们做的另一个有趣的实验是移除局部区域描述符仅使用预测框中的整体图像描述符。通过几何约束，我们的方法能够帮助局部区域定位。如表 2 所示，我们的方法优于使用 R-CNN 的单个对象检测器，这意味着几何约束有助于我们的方法更好地定位对象窗口。强 DPM 的检测不如我们的方法准确，这就解释了它性能下降的原因。oracle 方法使用真实边界框，并且达到了 57.94％ 的准确率，这仍然远低于表 1 中在对象和局部区域内使用两个图像描述符的方法。  \n",
    "\n",
    "### 4.2 Part localization  \n",
    "\n",
    "我们现在展示我们系统在定位局部区域上的效果。在表 4 中的结果是根据正确定位的区域分（PCP）度量的百分比给出的。对于第一组结果，给出了整个对象边界框，任务仅仅是定位边界框中局部区域的位置，与真实局部区域标注相交面积大于 0.5 的认为是正确的结果。  \n",
    "\n",
    "对于第二组结果，使用在第二部分中描述的目标函数在排名最高的局部区域预测上计算 PCP 度量。请注意，在这个更真实的设置中，我们不会假设在测试时就知道真实的目标标注框。  \n",
    "\n",
    "如表 4 所示，对于给定边界框和未知边界框的两种设置，我们的方法都优于强 DPM 方法。增加一个几何约束 $\\delta^{NP}$ 可以改善我们的结果。在全自动设置中，头部排名最高的检测和局部区域定位性能比基准方法好 65％。 $\\Delta_{null}= 1$是没有应用几何约束的仅外观情况。虽然细粒度的分类结果没有显示 Δgeometric 和 Δbox 之间的很大差距，但我们可以看到局部区域定位性能的差距。性能差距较小的原因可能在于深层卷积特征对于小的变化和旋转是不变的。  \n",
    "\n",
    "我们还评估了对边界框和局部区域 selective search region proposals 的 recall。表 3 列出了不同重叠阈值的召回结果。重叠要求为 0.5 时，鸟头部和身体部位的召回率很高，这为根据 region proposals 定位这些部件提供了基础。但是，我们还观察到，当重叠阈值为 0.7 时，头部召回率低于 40％，这表明自下而上的 region proposals 可能是精确局部定位的瓶颈。  \n",
    "\n",
    "其他可视化显示在图 4 中。我们显示了每个图像的三个检测和局部区域定位，第一列是强 DPM 的输出，第二列是我们的单个局部区域预测的方法，最后一列是我们使用局部优先的方法。我们使用预训练的模型来获得结果。我们还会在图 5 中显示我们方法的一些失败案例。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/1/5.jpg?raw=true)\n",
    "\n",
    "### 4.3 Component Analysis  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/1/6.jpg?raw=true)\n",
    "\n",
    "为了检验 $\\Delta_{geometric}$ 中使用的 $\\alpha$ 和 K 不同值的影响，我们进行交叉验证实验。结果显示在图 3 中，我们将图 3 中的 K = 20 固定，并将图 3 中 的 $\\alpha = 0.1$ 固定，所有关于以交叉验证方式训练数据的实验，我们将训练数据分成 5 次。如结果所示，端到端的细粒度分类结果对 $\\alpha$ 的选择非常敏感，并且 $\\alpha = 0.1$ 是没有任何几何约束的 $\\Delta_{box}$ 预测的情况。我们必须选择一个小 $\\alpha$ 的原因是高斯的 pdf 与我们的局部区域检测器输出的 logistic score 相比较大。另一方面，K 的选择不能太小，当 K 大于 10 时它的选择不是很敏感。  \n",
    "\n",
    "## 5. Conclusion  \n",
    "\n",
    "我们已经提出了一种能够进行最先进的细粒度目标识别的联合目标检测和区域定位系统。我们的方法学习探测器和区域模型，并强化区域之间和对象框架之间的几何约束。我们的实验结果表明，即使具有非常强大的特征表示和对象检测系统，通过区域来为具有高语义相似度的类别之间的细粒度区分的困难任务额外地建模对象的姿态也是非常有益的。在今后的工作中，我们将考虑在训练时联合建模对象类别及其各个部分和变形成本的方法。我们还计划探索弱监督设置，在该设置中，我们只会自动发现局部区域并将其模型化为仅来自对象边界框标注的潜在变量。最后，我们将考虑放宽对较小部分的 selective\n",
    "search，并采用密集窗口采样。\n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/1/7.jpg?raw=true)\n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/1/8.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained Image Recognition  \n",
    "\n",
    "\n",
    "## Abstract  \n",
    "\n",
    "细粒度的图像识别是一个具有挑战性的计算机视觉问题，因为由高度相似的从属类别引起的小类间变化，以及姿势，尺度和旋转引起的大的类内变化。本文中，我们对细粒度识别提出了一种新的没有全连接层的端到端 Mask-CNN 模型。基于细粒度图像的局部区域标注，所提出的模型由完全卷积网络组成，以定位可区分的部分（例如，头部和躯干），并且更重要的是生成用于选择有用和有意义的卷积描述符的目标/局部掩码。之后，建立一个 Mask-CNN 模型，用于同时聚合选定的对象和局部区域级描述符。与现有细粒度方法相比，所提出的 Mask-CNN 模型具有最少的参数，最低的特征维度和最高的识别准确度。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "细粒度的识别任务，例如识别鸟的种类，在计算机视觉中已经很流行。由于类别彼此相似，因此不同的类别只能通过轻微细微的差异来区分，这使得细粒度识别成为具有挑战性的问题。与一般的对象识别任务相比，细粒度识别更有益于学习对象的关键部分，这些部分这有助于区分不同的子类并对齐同一类的数据。  \n",
    "\n",
    "在深度学习时代，区域表示的直接方式是使用深度卷积特征/描述符。与全连接层的特征（即，整个图像）相比，卷积描述符包含更多的局部（即，局部区域）信息。另外，已知这些深度描述符对应于中级信息，例如目标局部区域。所有先前 part-based 的细粒度方法，都直接使用深卷积描述符并将它们编码成单一表示，而不评估获得的对象/局部区域深度描述符的有用性。通过使用强大的卷积神经网络，我们可能不需要在特征向量内选择有用的维度，就像我们对手工特征所做的那样。但是，由于大多数深度描述符对细粒度识别没有用处或意义，因此有必要选择有用的深度卷积描述符。最近，选择深度描述符对细粒度图像检索任务进行了阐述。而且，这对细粒度图像识别也是有益的。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/2/1.jpg?raw=true)\n",
    "\n",
    "在本文中，通过开发一种新颖的深度局部区域检测和描述符选择方案，我们提出了一种端到端的 Mask-CNN（M-CNN）模型，该模型丢弃全连接的层以进行细粒度识别。在训练期间，我们只需要局部区域标注和图像级标签。在 M-CNN 中，给定局部区域标注，我们首先将它们分成两个点集。一组对应于细粒鸟图像的头部部分，另一组对应于躯干。然后，将覆盖每个点集的最小凸多边形作为 ground-truth mask 返回，如图 1 所示。其他像素为背景。通过将局部定位视为三类分割任务，我们充分利用全卷积网络（FCN）在测试时生成 mask 并选择有效的深度描述符，在测试期间不使用任何标注。获得这两部分 mask 后，我们将它们组合起来形成对象。基于这些对象/局部区域 mask，构建了一个四流 four-stream Mask-CNN（图像，头部，躯干，对象），用于联合训练并同时聚合对象级和局部区域级特征。所提出的 four-stream M-CNN 的架构如图 2 所示。在每个 M-CNN 流中，我们丢弃全连接的 CNN 层。在最后的卷积层中，输入图像由多个深度描述符表示。为了选择有用的描述符以仅保留与对象相对应的描述符，使用 FCN 预先学习的对象/局部 mask。之后，每个流选定的描述符都被 averaged pooled 和 max pooled 成 512 维的向量，然后使用 $l_2$ 标准化。最后，将这四个流的特征向量连接起来，然后添加一个分类（fc + softmax）层进行端到端联合训练。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/2/2.jpg?raw=true)\n",
    "\n",
    "我们在 Caltech-UCSD Birds-200 2011 数据集上验证了提出的四流 M-CNN，我们实现了 85.5％ 的分类精度。也获得准确的局部区域定位（头部 84.62％和躯干 89.83％）。所提出的 M-CNN 模型的主要优点和贡献是：  \n",
    "\n",
    "- 据我们所知，Mask-CNN 是第一个选择深卷积描述符用于目标识别的端对端模型，特别是对于细粒度图像识别。  \n",
    "\n",
    "- 我们提出了一种用于细粒度识别的新颖且高效的 part-based four-stream 模型。丢弃全连接层，并且所提出的 M-CNN 在计算和存储方面是有效的。 与最先进的方法相比，M-CNN 的参数最小，特征维数最小（60.49M和8192-d）。同时，CUB200-2011 的分类准确率达到 85.4％，是现有方法中最高的。使用 SVD 白化方法，我们的特征表示可以压缩到 4,096-d，同时将精度提高到 85.5％。  \n",
    "\n",
    "- 该模型的局部区域定位性能优于其他需要额外标注框的 part-based 的细粒度方法。特别是，M-CNN 比用于头部定位的最新技术高出约 10％。  \n",
    "\n",
    "## 2. Related Work  \n",
    "\n",
    "细粒度识别是一个具有挑战性的问题，最近已成为一个热门话题。在过去的几年中，文献中已经发展了许多有效的细粒度识别方法。我们可以粗略地将这些方法分为三组。第一组试图通过开发用于分类细粒度图像的强大深度模型来学习更具辨别性的特征表示。第二组对细粒度图像汇总的目标进行矫正，以消除姿势变化和相机位置的影响。最后一组关注于 局部区域的表示，因为人们普遍认为细粒度图像之间的细微差别大多存在于对象局部区域的的独特属性中。  \n",
    "\n",
    "对于 part-based 的细粒度识别方法，有论文在训练期间使用鸟类的边界框和局部区域标注来学习准确的局部区域定位模型。然后，分别使用检测到的部分对不同的 CNN 进行微调。为了确保令人满意的定位结果，他们甚至在测试阶段使用了边界框。相比之下，我们的方法只需要在训练时使用局部区域标注，并且在测试过程中不需要任何标注。此外，我们的四流 M-CNN 是同时捕获对象和局部区域级信息的统一框架。其他一些 part-based 的方法被认为是一种弱监督的设置，在这种设置中，他们将细粒度图像仅分为图像级标签。正如我们的实验所显示的那样，M-CNN 的分类精确度明显高于这些弱监督方法。同时，M-CNN 的模型尺寸在所有最先进的方法中是最小的，这使得训练效率更高。  \n",
    "\n",
    "此外，还有基于分割的细粒度识别方法。它们与 M-CNN 最显著的区别在于：这些方法仅使用分割来定位整个对象或局部区域，而我们使用分割中的 mask 进一步选择有用的深度卷积描述符。其中，part-stacked 的 CNN 模型是与我们最相关的工作。part-stacked CNN 在训练中需要目标标注框和局部区域标注，甚至在测试期间也需要目标标注框。在边界框裁剪的图像块内，该方法将 15 个关键点中的每个关键点周围的图像作为 15 个分割前景类别，并使用FCN 解决 16 类分割任务。在获得训练的 FCN 之后，它将这些局部点位置定位在最后的卷积层中。然后，将对应于十五个区域的激活和整个对象堆叠在一起，全连接层被用于分类。与 part-stacked CNN 相比，M-CNN 只需要定位两个主要部分（头部和躯干），这使得分割问题更容易和更准确。如表 3 所示，M-CNN 实现了很高的定位精度。同时，part-stacked CNN 使用全部 15 个局部定位不会导致更好的分类准确率。此外，虽然我们在训练中使用较少的标注，并且在测试中不使用任何标注（参见4.2.2节），但 MUBN 的 CUB200-2011 的准确率比使用相同 baseline network 的 part-stacked CNN 高1.8％ 。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/2/3.jpg?raw=true)\n",
    "\n",
    "## 3. The Mask-CNN Model  \n",
    "\n",
    "在本节中，我们展示了提出的四流 Mask-CNN（M-CNN）模型。首先，我们采用全卷积网络（FCN）来生成用于定位对象/局部的对象/局部掩模，并且更重要的是选择深度描述符。然后，基于这些 mask，建立四流 M-CNN，用于联合训练并捕获对象和局部区域级别的信息。  \n",
    "\n",
    "### 3.1 Learning Object and Part Masks  \n",
    "\n",
    "全卷积网络（FCN）设计用于像素级别分类。FCN 可以以任何分辨率拍摄输入图像并产生相应尺寸的输出。在我们的方法中，我们使用 FCN 不仅将细粒度图像中的对象和局部区域进行定位，而且还将分割预测作为后面的描述符选择过程的 masks。  \n",
    "\n",
    "CUB200-2011 数据集中的每个细粒图像都对局部区域进行了标注，例如 15 个关键点。如图 1 所示，我们将这些关键点分成两组，包括头部关键点（即喙，额头，冠，左眼，右眼，颈背和喉咙）和躯干关键点（即背部，乳房，腹部，左腿，右腿，左翼，颈背，右翼，尾巴和喉咙）。基于关键点，生成了两个局部标注 mask。一个是头部 mask，它对应于覆盖所有头部关键点的最小凸多边形。另一个是躯干 mask，它是覆盖躯干关键点的最小凸多边形。在图 1中，红色多边形是头部 mask，蓝色是躯干 mask。图像的其余部分是背景。因此，我们将 part mask 学习过程建模为三类分割问题。为了进行有效的训练，所有训练和测试的细粒度图像均使用其原始分辨率。然后，我们在原始图像的中间裁剪一个 384×384 的图像块作为输入。mask 学习网络结构如图 3 所示。在我们的实验中，我们采用了 FCN-8s 来学习和预测 part masks。  \n",
    "\n",
    "在 FCN 推断期间，不使用任何标注，为每个图像返回三个类别热图（与原始输入图像大小相同）。我们随机选择一些预测的 part masks，并在图 4 中显示它们。在这些图中，学习的 mask 覆盖在原始图像上。头部以红色突出显示，躯干呈蓝色。预测的背景像素为黑色。从这些图中可以看出，即使真实标注的局部 mask 不是非常准确，学习的 FCN 模型也能够返回更准确的 part mask。同时，这些局部 mask 也可以定位局部位置。局部定位和对象分割的定量结果将在后面章节报告。  \n",
    "\n",
    "如果两局部 mask 都精确预测，将有利于后面的深度描述符选择过程和最终的细粒度分类。 因此，在训练和测试期间，我们将使用 M-CNN 预测的 masks 进行局部定位和描述符的选择。我们还将两个 mask 组合在一起，为整个对象形成一个 mask，称之为对象 mask。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/2/4.jpg?raw=true)\n",
    "\n",
    "### 3.2 Training Mask-CNN  \n",
    "\n",
    "在获得对象和局部 mask 模后，我们构建了四流 M-CNN 进行联合训练。该模型的整体结构如图 2 所示。我们以整个图像流为例来说明 M-CNN 中每个流的流水线。  \n",
    "\n",
    "整个图像流的输入是用 h×h 调整大小的原始图像。在我们的实验中，我们分别报告了 h = 224 和 h = 448 的结果。输入图像被馈送到传统的卷积神经网络中，但全连接层被丢弃。也就是说，我们提出的 M-CNN 中使用的 CNN 模型仅包含卷积，ReLU和汇聚层，这大大降低了 M-CNN 模型的大小。具体而言，我们使用 VGG-16 作为基准模型，并保留 pool5 之前的层（包括pool5）。如果输入图像为 224×224，我们在 pool5 中获得 7×7×512 激活张量。因此，我们有 49 个 512-d 的深度卷积描述符，它们也对应于输入图像中的 7×7 个空间位置。然后，学习对象 mask（参见3.1节）首先通过最近的插值重新调整为7×7，然后用于选择有用和有意义的深度描述符。如图 2（c）和（d）所示，描述符在位于目标区域时应保持不变。如果它位于背景区域中，则该描述符将被丢弃。在我们的实现中，mask 设置为二进制矩阵，其中 1 表示保留，0 表示丢弃。我们将选择过程作为卷积激活张量与 mask 矩阵之间的元素内积作来实现，这与 FCN 中的元素相加操作相似。因此，位于目标区域中的描述符将保留，而其他描述符将成为零向量。  \n",
    "\n",
    "对于这些选定的描述符，在端到端的 M-CNN 学习过程中，我们将它们分别平均和最大化为两个 512-d 特征向量。然后，对他们中的每一个都进行 $l_2$ 归一化。之后，我们将它们连接成 1024-d 特征作为整个图像流的最终表示。  \n",
    "\n",
    "用于头部和躯干的流与整个图像具有相似的处理步骤。然而，与整个图像流的输入不同，我们如下生成头部和躯干流的输入图像。在获得两部分 mask（即头部和躯干掩模）之后，我们使用 part masks 作为 part detectors 来定位输入图像中的头部部分和躯干部分。对于每个 part，我们返回包含 part masks 区域的最小矩形边界框。基于矩形边界框，我们裁剪作为 part 流输入的图像补丁。图 2 中间显示了 M-CNN 的头部和躯干流。最后一个流是对象流，它通过将两部分掩码组合到对象掩码中来裁剪图像补丁。因此，它的输入是我们的 FCN 分割网络检测到的主要对象（即，鸟）。在我们的实验中，这三个流的输入都调整为224×224。  \n",
    "\n",
    "在图 2（f）所示的分类步骤中，最终的 4096d 图像表示是整个图像，头部，躯干和对象特征的拼接。M-CNN 的最后一层是用于 CUB200-2011 数据集分类的 200 路分类（fc + softmax）层。 四个流 M-CNN 是端到端学习的，同时学习四个 CNN 的参数。在训练 M-CNN 期间，所学习的 FCN 分割网络的参数是固定的。  \n",
    "\n",
    "## 4 Experiments  \n",
    "\n",
    "在本节中，我们首先描述实验设置和实现细节。然后，我们报告分类的准确性，并对所提议的 M-CNN 模型进行讨论。最后，还将提供 part 定位和对象分割的性能。  \n",
    "\n",
    "### 4.1 Dataset and Implementation Details  \n",
    "\n",
    "是在广泛使用的细粒度基准鸟类数据集 Caltech-UCSD 2011 上进行的评估的。该数据集包含 200 种鸟类，每个类别包含大约 30 个训练图像。在训练阶段，采用 15 个局部标注生成 part masks，同时将图像级标签用于端到端的 M-CNN 联合训练。测试时，我们不需要监督信号（例如，局部标注或边界框）。  \n",
    "\n",
    "所提出的 Mask-CNN 模型和用于生成掩码的 FCN 使用开源库 MatConvNet 来实现。在我们的实验中，在获得学习到的 mask 后，我们首先生成鸟头，躯干和物体的图像块，如第二节所述。然后，为了促进四流 CNN 的收敛，对应于整个图像，头部，躯干和物体的每个单独的流在其输入图像上被分别微调。每个流中使用的 CNN 由在 ImageNet 上预先训练过的 VGG-16 模型初始化。另外，我们通过对所有四个流进行水平翻转来使训练数据翻倍。如图 2 所示，在对每个流进行微调之后，执行四流 M-CNN 的联合训练。在 M-CNN 中不使用 dropout。在测试时，我们对图像及其翻转副本的预测进行平均，并输出具有最高分数的类作为测试图像的预测。此外，与逻辑回归（LR）相比，直接使用 softmax 预测结果的准确性略有下降，这与之前中的观察结果一致。因此，在下文中，所报告的 M-CNN 结果全部是用提取的特征（4096-d）与默认超参数 $C_LR = 1$ 基于 one-vs-all logistic regression 实现的。  \n",
    "\n",
    "### 4.2 Classification Accuracy and Comparisons  \n",
    "\n",
    "我们报告了提出的四流 M-CNN 模型在 CUB200-2011 数据集上的分类准确率，并与文献中的基线方法和最新方法进行比较。  \n",
    "\n",
    "#### 4.2.1 Baseline Methods  \n",
    "\n",
    "为了验证 M-CNN 中描述符选择过程的有效性，我们执行了基于所提出的四流体系结构的两种基线方法。与我们的 M-CNN 不同，这两个基线方法不包含描述符选择部分，即图 2（d）所示的处理。  \n",
    "\n",
    "第一种基线方法使用传统的全连接层来对每个流进行分类，其被称为“4 流 FC”。 在“4 流 FC”中，我们用含有全连接层的 CNN（即，仅移除了 fc8 的VGG-16）代替图 2 中每个流的（b）到（e）部分。 因此，每个流的最后一层中生成的特征是一个 4096-d 单个矢量。 其余过程也是将 4 个 4,096d 特征连接到最后一个特征 16,384-d，并在 16,384d 图像表示上学习 200 路分类（fc + softmax）层。  \n",
    "\n",
    "第二个基线与提议的 M-CNN 类似。最显着的区别是它丢弃了描述符选择部分，即图 2（d）中的处理。因此，每个流中 pool5 的卷积深度描述符都是直接平均和最大值汇集，然后分别进行 $l_2$ 归一化。因此，我们称之为“4-stream Pooling”。其余程序与提出的 M-CNN 相同。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/2/5.jpg?raw=true)\n",
    "\n",
    "#### 4.2.2 Comparisons with state-of-the-art methods  \n",
    "\n",
    "表 2 列出了提议的四流 M-CNN 和 CUB200-2011 最先进方法的分类准确率。为了公平比较，我们只报告在测试中不使用局部标注的结果。  \n",
    "\n",
    "如前所述，当所有输入尺寸为 224×224 时，所提出的四流 M-CNN 模型的准确率为 83.1％。之后，我们将整个图像流的输入图像更改为 448×448 像素，这将分类性能提高了 2.1％。我们还将对象流的输入图像大小调整为 448×448。但精度略低于以前。  \n",
    "\n",
    "此外，由于多层的集合可以提高最终性能，所以在联合训练之后，我们从 pool5 前面三层的 relu5_2 层中提取深层描述符。然后，预测的 part mask 也用于选择四个流的相应描述符。完成与 pool5 相同拼接过程，我们可以获得 relu5_2 的另一个 4,096-d 图像表示形式。之后，我们将它与 pool5 结合成一个 8,192-d 特征向量（表2中称为“4-stream M-CNN +”），在 CUB200-2011 上实现了 85.4％ 的最佳分类精度。 另外，我们通过 SVD 白化将 8,192-d 特征向量压缩到 4,096。它可以降低维度，同时将精度提高到 85.5％。  \n",
    "\n",
    "特别的，由于 part-stacked CNN 使用了 Alex-Net 模型，我们还基于 Alex-Net 构建了另一个四流 M-CNN。我们的四流 M-CNN（Alex-Net）的准确率为78.0％。它比 part-stacked CNN 高1.8％。此外，在基于 Alex-Net 的四流 M-CNN 中，参数数量仅为 9.74M，最终的特征向量仅为 2,048 维。  \n",
    "\n",
    "### 4.3 Part Localization Results  \n",
    "\n",
    "除了在第二节中显示的定位结果外。在本节中，我们使用 Percentage of Correctly Localized Parts（PCP）度量来定量评估定位的准确性。如表 3 中所报告的，度量指标是定位的部分（即头部和躯干）与标准标注IOU超过 50% 的百分比。  \n",
    "\n",
    "通过比较躯干 PCP 的结果，我们的方法大大优于 part-based R-CNN 和强 DPM。但是，由于我们在测试中不使用任何监督，所以定位性能低于在测试期间使用边界框的 Deep LAC。另外，对于比躯干更具挑战性的头部定位任务，尽管我们的方法仅使用了局部注释来训练，但头部定位性能（84.62％）仍然明显高于其他方法。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/2/6.jpg?raw=true)\n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/2/7.jpg?raw=true)\n",
    "\n",
    "### 4.4 Object Segmentation Performance  \n",
    "\n",
    "由于 CUB200-2011 数据集也提供了对象分割标注，因此我们可以直接在分割上测试学习到的 mask。图 5 显示了定性分割结果。我们基于 FCN 的方法通常能够很好地对前景物体进行分割，但可以理解的是，分割更精细的鸟类部分，例如爪和喙是很麻烦的。由于我们的目标不是分割对象，因此我们不会将其作为预处理或后处理进行细化。此外，我们通过真实前景对象与预测对象 mask 的常见语义分割度量均值 IU（像素精度和区域相交联合）来定量评估分割性能。测试集合为 72.41％。实际上，更好的分割结果将导致更好的预测对象/局部 mask，并且也有利于最终的分类。为了进一步提高分类精度，一些预处理方法，例如 GrabCut，值得试着去获得比图 3（c）中的凸多边形更好的 mask。  \n",
    "\n",
    "## 5 Conclusion  \n",
    "\n",
    "在本文中，我们介绍了在目标识别中选择深度卷积描述符的好处，特别是细粒度图像识别。通过开发描述符选择方案，我们提出了一种不具有全连接层的新型端到端 Mask-CNN（M-CNN）模型，不仅能够精确地定位对象/局部，还能生成用于选择深读卷积的对象/局部 mask 描述。在聚合选定的描述符后，对象级和部分级特征由所提出的四流 M-CNN 模型编码。Mask-CNN 不仅在 CUB200-2011 上获得了 85.5％ 的分类精度，而且参数参数最少，维度特征表示最低。  \n",
    "\n",
    "今后，我们计划在弱监督环境中解决 M-CNN 的部分检测问题，其中我们只需要图像级标签。因此，要达到可比较的分类精确度，需要少得多的标注工作量。另外，另一个有趣的方向是探索用于一般对象分类的描述符选择的好处。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Application of Two-level Attention Models in Deep Convolutional Neural Network for Fine-grained Image Classification  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "细粒度分类是具有挑战性的，因为类别只能通过细微局部的差异来区分，位置，大小或者旋转都会使问题变得更加困难。大多数细粒度分类系统遵循寻找前景对象或对象局部区域来提取判别特征。在本文中，我们建议将视觉注意力应用于使用深度神经网络的细粒度分类任务。我们集成了三种注意力模型：提出候选 patches 的自下而上的注意力，选择特定目标相关 patches 的目标级别自顶向下的注意力，定位可区分局部区域的局部级别自顶向下的注意力。我们将这些注意力结合起来，对特定领域的深层网络进行训练。重要的是，我们避免使用昂贵的标注，如边界框或局部信息标注。弱监督约束使我们的工作更容易推广。我们已经验证了该方法对 ILSVRC2012 数据集和 CUB200 2011 数据集子集的有效性。我们的 pipeline 在最弱的监督条件下进行了重大改进并取得了最佳的准确性。该性能与其他依赖附加标注的方法相比具有竞争性。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/3/1.jpg?raw=true)\n",
    "\n",
    "细粒度分类是指在一些基本层次类别下的次分类问题，例如，对不同的鸟类，狗品种，花种，飞机模型等进行分类。这是一个具有广泛应用的重要问题。 即使在 ILSVRC2012 1K 类别中，犬类和鸟类下分别有 118 个和 59 个子类。直观地，如图 1 所示类内差异可能大于类间差异。因此，细粒度分类在技术上具有挑战性。  \n",
    "\n",
    "具体而言，细粒度分类的困难来源于可区分特征不仅仅是前景中的物体了，更应该是物体的局部信息，比如说鸟的头部。因此，大多数细粒度分类系统都遵循以下流程：找到前景对象或对象局部区域以提取可区分特征。  \n",
    "\n",
    "为了这个工作，自下而上的过程是不可避免的，我们需要提出很多图像区域作为物体候选者，或者这些区域包含那些具有判别力的部分。selective search 是一个无监督的过程，可以提出数千个这样的区域。这个方法在最近的研究中被广泛使用，我们也采用了这一点。  \n",
    "\n",
    "自下而上的过程具有较高的召回率，但精度很低。如果对象相对较小，则大多数 patch 都是背景，对对象分类没有帮助。这给 pipeline 中的定位部分带来了问题，导致需要自顶向下的注意力模型来过滤掉嘈杂的 patch 并选择相关的 patch。在细粒度分类的背景下，查找前景对象和对象局部区域可以被看作是一个两层的注意力过程，一个在对象层面，另一个在局部区域层面。  \n",
    "\n",
    "现有大多数的方法依靠强大的监督来处理注意力问题。他们严重依赖于人为的标签，使用边界框来表示对象级别和局部级别的区域标注。最强大的监督设置既可以在训练中也可以在测试阶段使用，而最弱的设置在训练和测试中都不使用这两种标注。大部分工作介于两者之间。  \n",
    "\n",
    "由于标注价格昂贵且不可扩展，因此本研究的重点是使用最弱的监督。认识到粒度差异，我们采用两条独立的 pipeline 来实现对象级和部分级的注意力，但共享组件。以下是我们方法的总结：  \n",
    "\n",
    "- 我们将在 ILSVRC2012 1K 预先训练的卷积神经网络转变成一个 FilterNet。FilterNet 可以选择跟基准类别很相近的 patches，因此可以处理 object-level attention。选定的 patches 用来训练另外一个 CNN，训练成一个 domain 分类器，称为 DomainNet。  \n",
    "\n",
    "- 经验上，我们观察到了在 DomainNet 中隐藏的聚类模式。多组神经元对可区分部分表现出高度敏感性。因此，我们选择相应的滤波器作为部分检测器来实现 part-level attention。  \n",
    "\n",
    "在这两个步骤中，我们只需要图像级标签。  \n",
    "\n",
    "接下来的关键步骤是从这两个注意力模型选择的区域中提取可区分性特征。最近，有令人信服的证据表明，由 CNN 产生的特征可以提供超过手工特征的优越性能。遵循上面概述的两个注意力 pipeline，我们采用相同的策略。在对象级别上，DomainNet 直接输出由图像的多个相关 patch 驱动的 multi-view 预测。在 part-level 上，由被检测部件驱动的 CNN 隐藏层的激活通过基于部件的分类器产生另一个预测。以利用两级注意力的优势，最终的分类合并了两条 pipeline 的结果。  \n",
    "\n",
    "我们的初步结果证明了这种设计的有效性。使用最弱监督，我们改进了 ILSVRC2012 数据集中犬类和鸟类的细粒度分类，错误率从 40.1% 和 21.1% 降到了 28.1% 和 11%。在 CUB200-2011 数据集中，我们达到了 69.7％ 的准确性，与其他使用更强监督的方法相比具有竞争力。我们的技术通过更好的网络会有改善，例如使用 VGGNet 的精度达到近 78％。  \n",
    "\n",
    "本文的其余部分安排如下。我们首先在第 2 节中描述利用对象级和部分级注意力进行细粒度分类的 pipeline。详细的性能研究和分析将在第 3 节中介绍。相关工作在第 4 节中介绍。最后，讨论我们学到的东西和未来的工作。  \n",
    "\n",
    "## 2. Methods  \n",
    "\n",
    "我们的设计基于一个非常简单的直觉：执行细粒度分类首先需要“看到”物体，然后才能看到物体中最具有区别性的部分。在图像中寻找奇瓦瓦狗需要首先看到一只狗，然后专注于它的重要特征，以区别于其他品种的狗。  \n",
    "\n",
    "为了实现这一点，我们的分类器不应该在原始图像上工作，而应该在指定的 patch 上工作。这些 patch 还应该保留与识别步骤相关的可区分行。在上面的例子中，第一步的目标是在狗类分类阶段，第二阶段的目标是在奇瓦瓦狗与其他品种（例如耳朵，头部，尾巴）之间进行区分的部分。至关重要的是，认识到详细标签价格昂贵且难以扩展的事实，我们选择使用最薄弱的标签。具体来说，我们的 pipeline 只使用图像级标签。  \n",
    "\n",
    "原始候选 patch 是在自底向上的过程中生成的，将像素分组为几个区域，突出显示某些对象局部区域。在这个过程中，我们采用 selective search 从输入图像中提取 patch。这一步将提供原始图像的多尺度和多角度图像。但是，自下而上的方法将提供高召回率但低精度的 patch。需要应用自顶向下的注意力来选择对分类有用的相关 patch。  \n",
    "\n",
    "### 2.1. Object-Level Attention Model  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/3/2.jpg?raw=true)\n",
    "\n",
    "**使用目标级别 attention 选择 patch** 此步骤通过自上而下的目标级 attention 过滤自下而上的原始 patches。目标是消除与对象无关的噪声patch，这对训练分类器十分重要。通过将在 ILSVR2012 数据集 1k 个类上训练的 CNN 转换为目标级 FilterNet 来实现这一点。我们将细粒度图像父类（例如奇瓦瓦父级是狗）softmax 的激活值作为选择 patch 的置信度，然后在分数上设置阈值以决定是否选择给定的 patch。这在图 2 中示出。通过这种方式，多尺度和多视图的优势已被保留，并且噪声也被滤除了。  \n",
    "\n",
    "**训练 DomainNet** 由 FilterNet 选择的 patch 用于训练适当变形后新的 CNN。我们将这个第二 CNN 称为 DomainNet，因为它提取了与属于特定类别（例如，狗，猫，鸟）相关的特征。  \n",
    "\n",
    "我们注意到，从单个图像中可以得到很多这样的 patch，并且网络的作用就像是数据增强的集合。与其他数据增强（如随机裁剪）不同，我们对 patch 与目标之间存在更高的相关性。数据量也推动了对更大网络的训练，使其能够构建更多特征。这有两个好处。首先，DomainNet 本身就是一个很好的细粒度分类器。其次，它的内部特征现在允许我们构建局部探测器，我们将在下面进行解释。  \n",
    "\n",
    "**目标级别 attention 分类** 通过目标级 attention 选择的 patches 可以自然地应用于测试阶段。为了获得图像的预测标签，我们向 DomainNet 输入由 FilterNet 选择的 patches。然后计算所有 patches 的 softmax 输出的平均分类分布。最后，我们可以得到平均 softmax 分布的预测。  \n",
    "\n",
    "该方法包含超参数置信度阈值，它会影响选定 patches 的质量和数量。实验中，我们将其设置为 0.9，因为此值提供了最佳的验证准确度和可容忍的训练时间。\n",
    "\n",
    "### 2.2. Part-Level Attention Model  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/3/3.jpg?raw=true)\n",
    "\n",
    "**构建局部检测器** DPD 和 Part-RCNN 的工作强烈表明，某些局部区域的区分性特征（例如头部和身体）对于细粒度分类至关重要。正如许多相关工作那样，我们不是在局部区域和关键点上使用强标签，而是从 DomainNet 的隐藏层具有聚类特征中受到启发。例如，有多组神经元对鸟头作出反应，有些则对鸟体作出反应，尽管它们可能具有不同的姿势。事后看来，这并不令人意外，因为这些特征确实可以代表一个类别。  \n",
    "\n",
    "图 3 从概念上展示了这一过程。本质上，我们对相似度矩阵 S 进行谱聚类，将中间层的滤波器分为 k 个组，其中 $S(i,j)$ 表示 DomainNet 中两个中间层滤波器 $F_i$ 和 $F_j$ 权重的余弦相似度。实验中，我们的网络与 AlexNet 基本相同，我们从第四个卷积层挑选神经元，其中 k 设为 3.每组都可以作为一个局部检测器。  \n",
    "\n",
    "使用聚类滤波器来检测 region proposals 的局部区域的步骤如下：1）resize patch proposal 输入的大小。 2）将 patch 前馈到 conv4，以便为每个filter 生成激活分数。3）将一个类别中 filter 的分数相加，得到类别分数。 4）对于每一聚类，选择生成分数最高的 patch，把这个 patch 当做 part patch。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/3/4.jpg?raw=true)\n",
    "\n",
    "图 4 显示了一些狗和鸟类的检测结果。很明显，DomainNet 中的一组 filter 特别注意鸟头，另一组注意身体。同样的，一组 filter 会关注狗头，另一组关注狗腿。  \n",
    "\n",
    "**构建 part-based 分类器** 将由 part 检测器选择的 patch 输入到 DomainNet 生成激活值。我们拼接不同部分和原始图像的激活值，然后训练 SVM 作为 part-based 分类器。  \n",
    "\n",
    "该方法包含几个超参数，例如检测滤波器层：conv4，聚类数量：3。实验中，取训练数据的 10％ 作为验证集用 grid search 方法来确认这些超参数。我们发现 conv4 比 conv3 或 conv5 更好，而设置 k > 3 并没有带来更好的准确率。  \n",
    "\n",
    "### 2.3. The Complete Pipeline  \n",
    "\n",
    "DomainNet 分类器和 part-based 分类器都是细粒度分类器。然而，他们的功能和优势是不同的，主要是因为他们接受的 patch 不同。使用 selective\n",
    "search 自下而上生成的是未加工的 patches。从它们中，FilterNet 选择多个视图希望将焦点集中在整个对象上，这些 patches 驱动 DomainNet。另一方面，part-based 分类器选择并专门处理包含可区分的和局部的特征。尽管有些 patches 被两个分类器都使用了，但它们的特征在每个分类器中都有不同的表示形式。最后，我们将 two level attention 模型的预测结果合并，利用以下公式综合两者的优势：  \n",
    "\n",
    "$final\\_score = object\\_score + \\alpha * part\\_score$\n",
    "\n",
    "其中对象得分是由 object attention 选择的多个 patch softmax 分数的平均值，part 得分是由 SVM 使用拼接特征产生的决策值，$\\alpha$ 是通过交叉验证得到的值。实验中，$\\alpha = 0.5$。选择具有最高评分的类作为预测结果。  \n",
    "\n",
    "图 5 显示了合并 two level attention 分类器的完整的 Pipeline。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/3/5.jpg?raw=true)\n",
    "\n",
    "## 3. Experiment  \n",
    "\n",
    "本节介绍性能评估并分析我们提出的方法在三个细粒度分类任务中的表现：  \n",
    "\n",
    "- ILSVRC2012 中的两个子集，狗数据集（ILSVRC2012 Dog）和鸟类数据集（ILSVRC2012 Bird）的分类。 第一个包含 118 类狗品种的 153,773 幅图像，第二个包含 59 种鸟类的 79,491 张图像。训练/测试分组遵循 ILSVRC2012 的标准协议。两个数据集都被弱注释，只有类标签可用。  \n",
    "\n",
    "- 广泛使用的细粒度分类基准 Caltech-UCSD Birds 数据集，包含 200 种鸟类的 11,788 幅图像。CUB200-2011 中的每个图像都有详细的标注，包括图像级标注，边界框和关键点标注。  \n",
    "\n",
    "### 3.1. Implementation Details  \n",
    "\n",
    "我们的 CNN 架构与流行的 AlexNet 基本相同。有 5 个卷积层和 3 个全连接层。它用于所有实验，除了输出层的神经元数量在需要时被设置为类别的数量。为了公平比较，我们尝试在同一网络体系结构上重现其他方法的结果。当使用 CNN 作为特征提取器时，第一个全连接层的激活作为特征输出。最后，为了证明我们的方法对网络结构是不可知的，并且可以通过网络结构改进分类效果，我们也尝试在特征提取阶段使用更新的 VGGNet。由于时间有限，我们没有复现使用 VGGNet 的所有结果  \n",
    "\n",
    "ILSVRC 1K 的 Bird 和 Dog 子集用于训练 DomainNet，CUB200 2011 用于微调 DomainNet_Bird。所有用于训练的图像都使用 object-level attention 方法进行增强。  \n",
    "\n",
    "### 3.2. Results on ILSVRC2012 Dog/Bird  \n",
    "\n",
    "在此任务中，只有图像级别的类标签可用。因此，需要详细标注的细粒度方法不适用。为简洁起见，我们只会报告狗类的结果，鸟类的结果在性质上是相似的。  \n",
    "\n",
    "基线是 CNN，但有两种不同的训练策略，包括：  \n",
    "\n",
    "- CNN_domain：只用来自狗类的图像训练网络。在训练阶段，从整幅图像中随机剪裁 227×227 图像块，以避免过拟合。在测试阶段，将 10 个固定视图（中心，四个角和它们的水平翻转）的 softmax 输出平均值做为最终预测。在这种方法中，没有使用特别的 attention 并且 patches 是 equally 选择的。  \n",
    "\n",
    "- CNN_1K：网络在 ILSVRC2012 1K 类别的所有图像上进行训练，然后删除不属于狗的 softmax 神经元。其他设置与上述相同。这是一个多任务学习方法，可以同时学习所有模型，包括狗和鸟。这种策略利用更多的数据来训练单个 CNN，并且更好地抵制过拟合，但是在不需要的类别上浪费了容量。  \n",
    "\n",
    "这些基准数字与我们方法的三种策略进行比较：仅使用对象级别和部分级别的 attention，以及两者的组合。Selective search 提出了数百个 patches，我们让 FilterNet 选择其中的大约 40 个，使用 0.9 的置信度得分。  \n",
    "\n",
    "表 1 总结了五种策略 top-1 的错误率。事实证明，两条基线的表现基本相同。但是，我们的注意力方法可以实现更低的错误率。与随机裁剪 patch 训练的CNN相比，仅使用对象级 attention 将错误率降低 9.3％。这清楚地表明了对象级 attention 的有效性：现在，DomainNet 专注于从前景对象中学习特定的特征。结合 part 级 attention，错误率下降 28.1％，明显好于基线。单独使用 part 级 attention 的结果不如对象级 attention，因为 part 级 attention 还存在更多模糊性。然而，它实现姿态规范化以抵抗大的姿态变化，这与对象级 attention 是互补的。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/3/6.jpg?raw=true)\n",
    "\n",
    "### 3.3. Results on CUB200-2011  \n",
    "\n",
    "对于这项任务，我们首先演示基于对象级 attention 学习深度特征的性能优势。然后，我们针对其他最先进的方法展示完整结果。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/3/7.jpg?raw=true)\n",
    "\n",
    "**学习深度特征的优势** 我们已经证明，受到对象级 attention 训练的鸟类 DomainNet 可以在 ILSVRC2012 Bird 上提供出色的分类性能。假设部分收益来自更好的学习特征是合理的。在这个实验中，我们使用 DomainNet 作为 CUB200- 2011 上的特征提取器来验证这些特征的优势。我们与两个基线特征提取器进行比较，一个是手工制作的核心描述符（KDES），它在使用 CNN 特征之前被广泛用于细粒度分类，另一个是 CNN 特征提取器从 ILSVRC2012 中所有数据进行预训练。我们比较了两种分类 pipelines 下的特征提取器。第一个使用边界框，第二个是在 Zhang 等人提出的（DPD）依靠基于可变形部分的检测器来查找对象及其局部区域。在这两条 pipeline 中，特征送入 SVM 分类器。在这个实验中，CUB200-2011 没有对 CNN 进行微调。如图 6 所示，基于 DomainNet 的特征提取器可以在两条管道 pipeline 上实现最佳结果。这进一步表明，使用对象级别的注意力过滤相关的 patch 是 CNN 学习好的特征的重要条件。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/3/8.jpg?raw=true)\n",
    "\n",
    "**分类 Pipeline 的优势** 本实验中，使用 CUB200-2011 对 DomainNet 进行了微调，并使用由对象级注意了产生的 patches。表 2 中给出了使用不同标注下的准确率。这些方法分为三组。第一组是我们基于注意力的方法，第二组使用与第一组相同的 DomainNet 特征提取器，但使用不同的 pipeline 和标注，第三组包括最近文献中的最新结果。由于训练数据数量有限，第二和第三组中的大多数比较方法使用 SVM 作为分类器，例如 BBox + DomainNet，DPD，Part RCNN。这些方法的区别在于提取特征的位置。  \n",
    "\n",
    "我们首先比较使用相同特征提取器的前两组结果，并将性能差异归因于不同的注意力模型。使用原始图像只能达到最低的准确率（58.8％）。这表明了细粒度图像分类中对象和部分注意了的重要性。相比之下，我们的注意力方法取得了显着的改善，两级注意力模型比使用使用人工标注框的方法更好（69.7％比68.4％），并且与 DPD（70.5％）相当。DPD 结果基于我们的特征提取器实现的，它使用可变形的 part-based 检测器，用对象边界框进行训练。标准 DPD pipeline 在测试时也需要边界框来产生相对较好的性能。就我们所知，69.7％ 是弱监管下的最佳结果。  \n",
    "\n",
    "第三个总结了目前最先进的一些方法。我们的结果比仅使用边界框进行训练和测试的结果要好得多，但与使用局部级标注的方法仍有差距。  \n",
    "\n",
    "我们的结果可以通过使用更强大的特征提取器来改进。如果我们使用 VGGNet 来提取特征，那么仅仅使用原始图像而没有注意力的基线方法可以提高到72.1％。增加对象级别的注意力，部分级别的注意力以及综合注意力分别将性能提升至 76.9％，76.4％ 和 77.9％。  \n",
    "\n",
    "## 4. Related Work  \n",
    "\n",
    "细粒度分类最近被广泛研究，以前的作品主要从三个方面提高识别精度：1. 对象和局部的定位，也可以将其视为对象/局部注意力; 2. 检测到的物体或局部的特征表示; 3. 人为介入。由于我们的目标是自动细粒度分类，因此我们重点关注前两项的相关工作。  \n",
    "\n",
    "### 4.1. Object/Part Level Attention  \n",
    "\n",
    "在细粒度分类任务中，可区分性特征主要局限于前景对象，甚至对象的局部，这使得对象和局部注意力成为第一个重要步骤。由于细粒度分类数据集通常使用边界框和关键点的详细标注，因此大多数方法都依赖其中一些标注来实现对象或局部级别的注意力。  \n",
    "\n",
    "最强有力的监督设置是在训练和测试阶段都使用边界框和关键点标注，这通常用于提升测试性能。为了在细粒度任务上验证 CNN 特征，在训练和测试阶段都假定提供了标注框。使用提供的标注框，提出了几种以无监督或潜在方式学习局部检测器的方法。为了进一步提高性能，局部级别标注还用于训练阶段，以学习强监督的可变形 part-based 模型或直接用于微调预先训练的 CNN。  \n",
    "\n",
    "我们的工作也与最近提出的基于 CNN 特征的目标检测方法（R-CNN）密切相关。R-CNN 首先通过一些自下而上的注意力模型为每幅图像提出数千个候选区域，然后选择分类得分高的区域作为检测结果。基于 R-CNN，Zhang 等人已经提出了 Part-based R-CNN 利用深度卷积网络进行局部检测。  \n",
    "\n",
    "### 4.2. Feature Representation  \n",
    "\n",
    "提高准确率的另一方面是直接引入更具辨别性的特征来表示图像区域。Ren 等人已经提出了 Kernel 描述符，并被广泛用于细粒度分类中。Berg 等人最近的一些作品尝试从数据中学习特征描述。从 ImageNet 数据中预先训练的 CNN 特征提取器在细粒度数据集上的性能也有显著的改善。Zhang 等人通过对细粒度数据集进行微调来进一步改善了 CNN 特征提取器的性能。  \n",
    "\n",
    "我们的方法采用相同的原则。我们也采用了自下而上的区域提议的策略，就像 R-CNN 和 Part R-CNN 一样。一个区别是我们使用提供多个视图和缩放的相关 patch 丰富了 object-level pipeline。更重要的是，我们整个模型选择最弱的监督，仅依靠 CNN 特征来实现注意力，检测丁武和提取特征。  \n",
    "\n",
    "## 5. Conclusions  \n",
    "\n",
    "在本文中，我们提出了一种结合了自下而上和自上而下注意力的细粒度分类 pipeline。对象级别的注意力为网络提供了与输入图像相关的具有不同角度和缩放比例的 patches。这用于细粒度分类的 CNN 特征更好。部分级别的注意力集中在局部可区分特征上，并且也实现了姿态规范化。这两种注意力都可以带来显著的收益，并且它们通过后期融合可以相互补充。我们方法的一个重要优点是，注意力来源于用分类任务训练的 CNN，因此它可以在只提供类别标签的最弱监督环境下进行。这与其他最先进的方法形成鲜明对比，这些方法需要对象边界框或局部标注进行训练或测试。据我们所知，我们在最弱的监督环境下获得了 CUB200-2011 数据集的最佳准确率。  \n",
    "\n",
    "这些结果是有希望的，同时，指出了一些经验教训和未来方向，总结如下：  \n",
    "\n",
    "- 处理 part level attention 中的模糊问题。我们目前的方法没有充分利用在 CNN 学到的东西。由于尺寸问题，part 特征可能出现在不同的层中，因此应将不同层的 filter 视为一个整体，以促进强大的 part 检测。  \n",
    "\n",
    "- 更紧密地整合 object-level 和 part level attention。object-level attention 的一个优点是它可以提供大量的相关 patches 以在一定程度上抵御图像变化。但是，目前的 part level attention 流程并未利用这一点。我们可以借用 multi-patch testing 的思想来实现 part-level attention 方法，从而产生更有效的姿态归一化。  \n",
    "\n",
    "我们正在研究追求上述方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bilinear CNNs for Fine-grained Visual Recognition  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "我们提出了一种简单有效的称为双线性卷积神经网络（B-CNN）的细粒度视觉识别体系结构。这些网络将图像表示为来自两个 CNN 的特征的 pooled outer product，并以平移不变的方式捕捉定位特征交互。B-CNN 属于无序纹理表示，但与以前的工作不同，它们可以以端对端的方式进行训练。我们最精确的模型分别对 Caltech-UCSD 鸟类，NABirds，FGVC 飞机和 Stanford cars 数据集分别获得 84.1％，79.4％，86.9％ 和 91.3％，在 NVIDIA Titan X GPU 上以每秒 30 帧的速度运行。然后，我们对这些网络进行了系统分析，结果表明（1）双线性特征是高度冗余的，可以减小一个数量级而不会显着降低准确性;（2）对其他图像分类任务也是有效的，例如作为纹理和场景识别，以及（3）可以在 ImageNet 数据集上从头开始训练，从而基线架构上提供一致的改进。最后，我们使用神经单元的最高激活和基于梯度的反演技术，在各种数据集上呈现这些模型的可视化。完整系统的源代码可在 http://vis-www.cs.umass.edu/bcnn上找到。  \n",
    "\n",
    "## 1. INTRODUCTION  \n",
    "\n",
    "细粒度识别涉及从属类别内的实例分类。例子包括识别鸟类物种，汽车模型或犬种。这些任务通常需要识别对象的高度本地化属性，同时不影响其在图像中的姿态和位置。例如，将“加利福尼亚海鸥”与“环状海鸥”区分开来需要识别其帐单上的图案或其羽毛的微妙颜色差异[1]。有两大类技术对这些任务有效。基于零件的模型通过对部件进行本地化并提取以检测到的位置为条件的特征来构建表示。这使得关于外观的后续推理更容易，因为由于位置，姿势和视点变化而引起的变化被排除。另一方面，整体模型直接构建整个图像的表示。这些包括经典的图像表示，如Bag-of-Visual-Words [12]及其变体在纹理分析中普及。大多数现代方法基于使用在ImageNet数据集上预先训练的卷积神经网络（CNN）提取的表示[54]。虽然基于CNN的基于零件的模型更加准确，但它们在培训期间需要部分注释。这使得它们在这些注释难于获取或昂贵的领域不适用，包括没有明确定义的部分（如纹理和场景）的类别。  \n",
    "\n",
    "在本文中，我们认为部分推理的有效性是由于它们对对象的位置和姿态的不变性。 纹理表示在设计上是平移不变的，因为它们是以无序方式聚集本地图像特征为基础的。 尽管基于SIFT [40]的经典纹理表示及其基于CNN [11]，[24]的近期扩展已被证明在细粒度识别中是有效的，但它们与基于部件的方法的性能不匹配。 这种差距的一个潜在原因是纹理表示中的基本特征不是以端到端的方式学习的，并且可能对于识别任务而言不是最理想的。  \n",
    "\n",
    "我们提出了双线性CNN（B-CNN），它解决了现有深度纹理表征的几个缺点。我们的关键洞察是几个广泛使用的纹理表示可以写成两个合适设计特征的汇集外部产品。当这些功能基于CNN时，生成的体系结构由用于特征提取的标准CNN单元组成，其后是专门设计的双线性层和池化层。输出是一个固定的高维表示，可以与完全连接的层结合来预测类别标签。最简单的双线性层是两个相同的特征与外部产品相结合的层。这与Carreira等人的二阶汇总方法密切相关。 [8]推广用于语义图像分割。我们还表明，一旦合适的非线性应用于基础特征，其他纹理表示可以写为B-CNN。这导致可以插入现有CNN的一系列图层用于大型数据集上的端到端培训，或用于转移学习的特定于域的微调。 B-CNN在各种细粒度识别数据集上表现优于现有模型，包括受过部分级监督培训的模型。而且，这些模型相当有效。我们在MatConvNet [66]中实现的最精确的模型在NVIDIA Titan X GPU上以每秒30帧的速度运行，并且在Caltech-UCSD鸟类中获得每图像准确度的84.1％，79.4％，86.9％和91.3％[67]， NABirds [64]，FGVC飞机[42]和斯坦福汽车[33]数据集。  \n",
    "\n",
    "\n",
    "1578/5000\n",
    "这份手稿结合了我们早期作品的分析[36,37]，并以多种方式扩展它们。我们介绍相关工作，包括随后发布的扩展（第2节）。我们描述了B-CNN体系结构（第3节），并提出了二阶汇总（O2P），费希尔矢量（FV），本地聚集描述符向量（Vectorof-Locally-Aggregated Descriptors）的精确和近似端对端可训练配方的统一分析VLAD），视觉词汇袋（BoVW）在各种细粒度识别数据集上的准确性（第3.2-4节）。我们表明这种方法是通用的，并且对其他图像分类任务（如材质，纹理和场景识别）有效（第4节）。我们提出了降维技术的详细分析，并提供了不同模型的精度和维数之间的权衡曲线，包括与最近提出的紧凑双线性汇集技术[19]（5.1节）的直接比较。此外，与基于在ImageNet数据集上预先训练的网络的先前纹理表示不同，B-CNN可以从头开始进行训练，并在基线架构上提供一致的改进，同时适度增加计算成本（第5.2节）。最后，我们可视化学习模型中几个单元的最高激活，并应用Mahendran和Vedaldi [41]的基于梯度的技术，在各种纹理和场景数据集上可视化反转图像（第5.3节）。我们已经在http://vis-www.cs.umass.edu/bcnn发布了系统的完整源代码。  \n",
    "\n",
    "## 2. RELATED WORK  \n",
    "\n",
    "细粒度识别技术。 在AlexNet [34]在ImageNet分类挑战方面令人印象深刻的性能之后，几位作者（例如[13]，[52]）已经证明从CNN层提取的特征在细粒度识别任务中是有效的。 基于零件技术的先前工作（例如，[5]，[15]，[71]），Zhang等人 [70]和布兰森等人。 [6]证明了将基于CNN的部分探测器[23]和基于CNN的特征结合起来进行细粒度识别任务的好处。 其他方法使用分割来以弱监督的方式指导零件发现并训练基于零件的模型[31]。 在非基于部分的技术中，纹理描述符如FV和VLAD传统上对于细粒度识别是有效的。 例如，FGCOMP'12挑战中性能最好的方法使用基于SIFT的FV表示[25]。  \n",
    "\n",
    "最近深层架构的改进也带来了对细粒度识别的改进。其中包括深度更深的体系结构，如牛津VGG集团的“深度”[9]和“非常深”[59]网络，初始网络[60]和“超深度”剩余网络[26]。空间变换网络[29]通过参数化图像变换来增强CNN，并且在细粒度识别任务中非常有效。其他技术通过“注意”机制来增强CNNs，从而可以对图像区域进行集中推理[4]，[43]。 B-CNNs可以被看作是隐式的空间关注模型，因为外部产品调制了基于另一个特征的一个特征，类似于注意机制中的乘性特征相互作用。虽然不能直接比较，Krause等人[32]表明，通过查询搜索引擎上的类别标签获得更多的训练数据，可以显着提高深度网络的准确性。最近，Moghimi等人[44]表明，提高B-CNNs对细粒度任务提供了一致的改进  \n",
    "\n",
    "纹理表示和二阶特征。 几十年来纹理表示已被广泛研究。 早期工作[35]通过计算线性滤波器组反应的统计量（例如，小波和可操纵金字塔）来表示纹理。 Portilla和Simoncelli [50]率先使用了滤波器组响应的二阶特征。 最近的变种如FV [46]和O2P [8]与SIFT被证明是非常有效的图像分类和语义分割[14]任务  \n",
    "\n",
    "在最近的一些作品中已经研究了无序纹理表示和深层特征相结合的优点。 Gong et al。进行了一个多尺度的无序CNN特征集合[24]进行场景分类。 Cimpoi等人[11]通过用来自CNN的非线性滤波器组代替线性滤波器组来对纹理表示进行系统分析，并显示出其对各种纹理，场景和细粒度识别任务的显着改进。他们发现CNN特征的无序聚合比这些任务中常用的完全连接层更有效。然而，这些方法的缺点是滤波器组没有以端对端的方式进行培训。我们的工作也与刘等人的跨层合并方法有关。 [38]谁表明，来自CNN的两个不同层的特征的二阶聚集在细粒度识别中是有效的。我们的工作表明，功能规范化和特定领域的微调提供了额外的好处，使用Caltech-UCSD Birds数据集上的相同网络将准确度从77.0％提高到84.1％[67]。另一个随后发表的感兴趣的工作是NetVLAD架构[3]，它提供了VLAD的端到端可训练近似。该方法应用于基于图像的地理定位问题。我们在第4节中将NetVLAD与其他纹理表示进行比较。  \n",
    "\n",
    "纹理合成和样式转换。 与我们的工作同时，Gatys等人 表明CNN特征的格拉姆矩阵是一种有效的纹理表示，并且通过匹配目标图像的格拉姆矩阵，可以创建具有相同纹理[20]和转移样式[21]的新颖图像。 当两个特征相同时，格拉姆矩阵与混合双线性表示法相同，但我们工作的重点是识别而不是一代。 这个区别是重要的，因为Ustyuzhaninov等。 [63]表明具有随机滤波器的浅CNN的格拉姆矩阵足以用于纹理合成，而区分性预训练和随后的微调对于实现高性能的识别是必不可少的。  \n",
    "\n",
    "多项式内核和和积网络。 将两个网络的特征结合起来的另一种策略是连接它们并通过顶层的一系列图层学习它们的成对相互作用。 然而，这样做太天真需要大量的参数，因为O（n）特征需要O（n 3）参数的层的O（n 2）交互。 我们使用外积的显式表示没有参数，并且类似于核心支持向量机中使用的二次核扩展[55]。 然而，人们可能能够使用替代架构来实现类似的近似，例如有效建模乘法相互作用的和积网络[22]。  \n",
    "\n",
    "双线性模型变体和扩展。 Tanenbaum和Freeman [62]使用双线性模型来模拟两种因素，例如图像的“风格”和“内容”。虽然我们还对部件的位置和外观的两个因素变化建模，但我们的目标是分类，而不是这些因素的显式建模。我们的工作涉及双线性分类器[49]，它将分类器表示为两个低秩矩阵的乘积。我们在5.1节中描述的基于低维表示的模型可以解释为双线性分类器。我们的模型与用于分析视频的“双流”体系结构有关，其中一个网络模拟时间方面，而另一个模拟空间方面[17]，[58]。使用外部产品组合两个特征的想法也被证明对其他任务有效，例如将文本和视觉特征组合在一起的视觉提示[18]，将光流和图像特征相结合的动作识别[16]。  \n",
    "\n",
    "低维双线性特征。 双线性特征的缺点是存储高维特征的内存开销。 例如，512维特征的外积导致512×512维表示。 我们之前的工作[37]表明，通过将其中一个特征投影到较低维空间，整体表示可以减少到512×64维。 或者，紧致双线性汇聚[19]应用张量素描[48]来聚合近似双线性特征的低维嵌入。 在第5.1节中，我们比较了两种方法，并发现投影方法更简单，更快，同样有效。 在大多数情况下，特征尺寸可以减少8-32倍而不会显着降低准确性。  \n",
    "\n",
    "可扩展性和速度。 就速度而言，B-CNN比传统的CNN架构有优势，因为它们用双线性池层和线性层代替了几个完全连接的层。 基于MatConvNet的[66]实现在具有cudnn-v5的NVIDIA Titan X GPU上以每秒30到100帧的速度运行，具体取决于型号架构。 即使使用更快的物体检测模块，例如更快的R-CNN [53]或单次检测器（SSD）[39]，用于细粒度识别的基于零件的模型速度也会降低2-10倍。 B-CNN的主要优点是仅需要图像标签，并且可以轻松应用于不同的细粒度数据集。  \n",
    "\n",
    "## 3. B-CNNS FOR IMAGE CLASSIFICATION  \n",
    "\n",
    "在本节中，我们介绍用于图像分类的B-CNN体系结构，然后展示各种广泛使用的纹理表示可以写成B-CNN。  \n",
    "\n",
    "### 3.1 The B-CNN architecture  \n",
    "\n",
    "用于图像分类的B-CNN由四元组B =（fA，fB，P，C）组成。 这里fA和fB是基于CNN的特征函数，P是汇集函数，C是分类函数。 特征函数是映射f：L×I→RK×D，其将图像I∈I和位置l∈L并且输出尺寸为K×D的特征。我们通常指代位置，其可以包括位置和 规模。 使用矩阵外积在每个位置组合特征输出，即，在位置l的fA和fB的双线性组合由下式给出：  \n",
    "\n",
    "fA和fB必须具有相同的特征尺寸K才能兼容。 K的值取决于特定的模型。 例如，BoVW模型的K = 1，等于FV模型中的聚类数量（详见第2节）。 合并函数P聚合图像中所有位置上的特征的双线性组合以获得全局图像表示Φ（I）。 我们在我们所有的实验中使用总和汇总，即，  \n",
    "\n",
    "由于在合并期间特征的位置被忽略，所以双线性特征Φ（I）是无序表示。 如果fA和fB分别提取大小为K×M和K×N的特征，则Φ（I）的大小为M×N。 双线性特征是可以与分类器C一起使用的通用图像表示（图1）。 直观地说，外部产品通过考虑它们的成对相互作用来相互调节特征fA和fB的输出，类似于二次核函数中的特征扩展。  \n",
    "\n",
    "#### 3.1.1 Feature functions  \n",
    "\n",
    "特征函数f的自然候选者是由卷积和分层层次组成的CNN。 在我们的实验中，我们使用在中间层截断的ImageNet数据集预先训练的CNN作为特征函数。 通过预培训，我们可以在领域特定数据有限的情况下受益。 这已经被证明对从物体检测，纹理识别到细粒度分类等许多任务都是有效的[10-13] [23] [52]。 使用CNN的另一个优势是所得到的网络可以处理任意大小的图像并产生由图像位置和特征通道索引的输出。  \n",
    "\n",
    "我们之前的工作[37]试验了模型，其中特征函数fA和fB独立或完全共享。 在这里，我们还试验了共享前馈计算一部分的特征函数，如图3所示。特征函数用于逼近3.2节中介绍的经典纹理表示以及我们呈现的低维B-CNN 在5.1节中。  \n",
    "\n",
    "#### 3.1.2 Normalization and classification  \n",
    "\n",
    "我们执行额外的归一化步骤，其中双线性特征x =Φ（I）通过符号平方根（y←sign（x）p | x |），然后进行2归一化（z←y / || y || 2 ）受到[47]的启发。 这提高了实践中的表现（参见我们之前的工作[37]，以评估标准化的影响）。 对于分类我们使用逻辑回归或线性SVM [55]。 虽然这可以用任意的多层网络来替代，但是我们发现线性模型在双线性特征之上是有效的。  \n",
    "\n",
    "#### 3.1.3 End-to-end training  \n",
    "\n",
    "由于整体架构是有向无环图，所以可以通过反向传播分类损失的梯度（例如，交叉熵）来训练参数。 双线性形式简化了梯度计算。 如果两个网络的输出分别是尺寸为L×M和L×N的矩阵A和B，则双线性特征是尺寸为M×N的x = AT B。设d'/ dx为损失函数的梯度关于x，然后通过梯度链条我们有：  \n",
    "\n",
    "只要能够有效计算特征A和B的梯度，就可以以端对端的方式训练整个模型。 该方案如图2所示。  \n",
    "\n",
    "### 3.2 Relation to classical texture representations  \n",
    "\n",
    "在本节中，我们展示可以用双线性形式写出各种无序纹理描述符，并推导端到端可训练的变体。 由于纹理的属性通常是平移不变的，所以大多数纹理表示基于局部图像特征的无序聚集，例如和或最大操作。 通常在聚集局部特征之前应用非线性编码以改善其表示能力。 此外，聚合特征的规格化（例如，功率和'2）被完成以增加不变性。 因此，可以通过选择局部特征，编码函数，池化函数和归一化函数来定义纹理表示。 为了简化进一步的分析，我们将特征函数f分解为f（l，I）= g（h（l，I））= g（x）以表示对图像的明确依赖关系以及h和附加非 - 线性g。  \n",
    "\n",
    "用于纹理的最早表示之一是视觉单词袋（BoVW）[12]。 它被证明在纹理以外的几个识别任务中是有效的。 尽管变体在如何学习视觉词汇方面存在差异，但流行的方法是通过对图像特征进行聚类（例如，使用k均值）来获得一组k个中心。 然后将每个特征x分配到最近的聚类中心（也称为“硬分配”），并且图像被表示为表示每个视觉词的频率的直方图。 如果我们将η（x）表示为在x最接近的中心的指数处为1并且在其他地方为零的单热编码，则BoVW可以写为具有gA（x）= 1和gB（x）的双线性模型， =η（x）。  \n",
    "\n",
    "VLAD表示[30]将描述符x编码为（x-μk）⊗η（x），其中⊗是克罗内克积，μk是x的最接近中心，η（x）是x的单热编码 像之前一样。 这些编码通过汇总汇总在图像中汇总。 因此VLAD可以写成双线性模型，其中gA（x）= [x - μ1; x - μ2;。。。; x - μk]。 这里，gA具有各自对应于中心的k行。 gB（x）= diag（η（x）），一个矩阵，其中对角线上的η（x）和别处的0。 请注意，VLAD的特征函数在每个位置输出k> 1行的矩阵。  \n",
    "\n",
    "FV表达式[47]计算一阶统计量αi=Σ-1 2 i（x - μi）和二阶统计量βi=Σ-1 i（x - μi）（x - μi） 由高斯混合模型（GMM）后验θ（x）加权。 这里μi和Σi分别是第i个GMM分量的均值和协方差，并且表示单元乘法。 因此，FV可以写成双线性模型，其中gA = [α1β1; α2β2;。。。; αkβk]和gB = diag（θ（x））。  \n",
    "\n",
    "O2P表示[8]计算一个区域内SIFT特征的协方差统计，然后是logEuclidean映射和功率归一化。 他们的方法被证明对于语义分割是有效的。 O2P可以写成具有对称特征的双线性模型，即fA = fB = fsift，然后是汇集和非线性。  \n",
    "\n",
    "在BoVW，VLAD和FV表示中，由编码器学习的基于外观的聚类中心η（x）或θ（x）可以被认为是部分探测器。 实际上已经观察到，聚类中心倾向于在面部训练时定位面部标志[45]。 因此，通过对编码器η（x）或θ（x）和外观x的联合统计进行建模，模型可以有效地描述部件的外观，而不管它们在图像中出现的位置。 这对于对象未在图像中定位的细粒度识别特别有用。  \n",
    "\n",
    "#### 3.2.1 End-to-end trainable formulations  \n",
    "\n",
    "之前关于使用纹理编码器进行细粒度识别的工作[11]，[25]并没有以端到端的方式学习这些特征。 下面我们描述一个最近提出的VLAD的端到端可训练近似，并且为前面部分描述的所有纹理表示提供类似的表达式。 直接微调这些模型的能力可以显着提高各种细粒度数据集的准确度。  \n",
    "\n",
    "最近，Arandjelovic'等人提出了称为NetVLAD的端到端可训练VLAD制剂。[3]。 第一个简化是用可微“软赋值”η（x）代替gB中的“硬分配”η（x）。 给定第k个聚类中心μk，输入x的软分配向量的第k个分量由下式给出，  \n",
    "\n",
    "其中wk =2γμk，bk =-γ||μk|| 2，γ是模型的参数。 这只是在具有偏置项的卷积层之后应用的softmax操作，并且可以使用标准的CNN构建块来实现。 函数gA保持不变[x - μ1; x - μ2;。。。; x - μk]。 第二种简化是在训练期间解耦gA和gB两者对μ的依赖性，这使得梯度计算更容易。 因此，在训练期间的NetVLAD中，权重wk，bk和μk是独立参数。  \n",
    "\n",
    "通过将二阶统计量附加到特征gA，我们将NetVLAD扩展到NetFV，即gA = [x - μ1，（x - μ1）2; x - μ2，（x - μ2）2;。。。; x - μk，（x - μk）2]。 这里，平方以元素方式进行，即（x-μi）2 =（x-μi）（x-μi）。 功能gB与NetVLAD保持一致。 这种简化放弃了在FV模型中使用的真实GMM后验中存在的协方差和先验。 类似地，BoVW的NetBoVW近似值以类似于NetVLAD的方式计算的软分配η（x）替代硬分配。  \n",
    "\n",
    "当特征函数fA和fB相同时，O2P表示与B-CNN相同。 然而，O2P表示将一个对数 - 欧几里得（矩阵对数）映射应用到汇集的表示上，因为它涉及特征值分解，并且当前在GPU上没有有效的实现，所以这对于计算而言相当昂贵。 这显着减慢了整个网络的正向和梯度计算。 跳过这一步使我们能够高效地微调模型。 我们还注意到，与我们的出版物[37]同时，Ionescu等人 [28]提出了一种DeepO2P方法，并指出了类似的困难。  \n",
    "\n",
    "## 4. IMAGE CLASSIFICATION EXPERIMENTS  \n",
    "\n",
    "我们概述4.1节中的实验中使用的模型。 然后，我们将比较各种B-CNN与4.2节中的细粒度识别以及4.3节中的纹理和场景识别的先前工作。  \n",
    "\n",
    "### 4.1 Models  \n",
    "\n",
    "下面我们介绍我们实验中使用的各种模型：  \n",
    "\n",
    "FV与SIFT。 我们使用使用VLFEAT [65]提取的密集SIFT特征[47]实现了一个FV。 首先将图像调整为448×448，并使用4像素的步幅在图像上密集地计算8像素的像素大小的SIFT特征。 在学习具有256个组件的GMM之前，这些特征是将PCA投影到80个维度。  \n",
    "\n",
    "有全连接（FC）层的CNN。 这是从最后一个FC层提取特征的标准基线，即在CNN的softmax层之前。 将输入图像调整为224×224（CNN的输入大小）并在通过CNN传播之前进行平均相减。 我们考虑两种不同的表示形式：VGG-M网络[9]和16层VGG-D网络[59]的4096维度Relu7层输出。  \n",
    "\n",
    "FV / NetFV与CNNs。 这表示[11]的方法，它使用64个GMM分量的CNN滤波器组响应的FV池建立描述符。 对[11]的一种修改是，我们首先将图像重新调整为448×448像素，即CNNs训练的分辨率的两倍，并从单一比例的池特征。 与多尺度方法相比，这会导致性能略有下降。 但是我们选择单尺度设置，因为这可以使我们所有方法的特征提取保持一致，从而使比较更容易。 我们考虑基于VGG-M和VGG-D网络的两种表示。 与早期的FC模型不同，这些特征分别从VGG-M和VGG-D网络的relu5和relu5 3层中提取。  \n",
    "\n",
    "带有CNN的VLAD / NetVLAD。 与FV类似，该方法在CNN滤波器组响应上构建VLAD描述符。 我们在将图像传递到网络之前将图像大小调整为448×448像素，并使用VLAD / NetVLAD池与64个群集中心汇总从CNN获得的功能。 与FV模型相同，我们认为VGG-M和VGG-D网络的VLAD模型分别在relu5和relu5 3层截断  \n",
    "\n",
    "BoVW / NetBoVW与CNNs。 对于BoVW，我们使用CNN特征顶部的k-means构建了4096个词汇的词汇表。 对于NetBoVW，我们使用4096路softmax图层作为硬分配的近似值。 我们使用与FV和VLAD相同的设置来进行特征提取。  \n",
    "\n",
    "B-细胞神经网络。这些是第3部分介绍的模型，其中来自两个CNN的特征使用外部产品进行汇集。当两个CNN相同时，该模型是使用没有对数欧几里德标准化的深度特征的O2P的扩展。我们考虑几个B-CNN--（i）在relu5层有截断的两个相同的VGG-M网络，（ii）分别在relu5层和relu5层截断的VGG-D和VGG-M网络的一个， iii）用在relu5 3层截断的两个相同的VGG-D网络进行初始化。与FV和VLAD中的设置相同，输入图像的大小调整为448×448，使用两个CNN提取功能。与VGG-M网络的27×27相比，VGG-D网络产生28×28输出，因此我们在与VGG-M输出组合时忽略行和列来对VGG-D输出进行降采样。所有这些型号的双线性特征尺寸为512×512。请注意，对称模型，即选项（i）和（iii），这两个网络共享所有参数（当由于对称性而对它们进行微调时也是如此），因此实际上这些模型具有相同的内存开销并加速作为单一网络评估。  \n",
    "\n",
    "#### 4.1.1 Fine-tuning  \n",
    "\n",
    "为了进行微调，我们添加k路线性+ softmax图层，其中k是细粒度数据集中类的数量。 线性层的参数是随机初始化的。 我们采用两步训练[6]，首先训练线性层使用逻辑回归，凸优化问题，然后微调整个模型使用反向传播的几个纪元（约45 - 100取决于 数据集和模型）以相对较小的学习速率（η= 0.001）。 在整个数据集中，我们发现用于微调的超参数是相当一致的。 尽管无法直接精确调整VLAD，FV，BoVW模型，但我们使用间接微调来报告结果，其中网络与FC层进行了微调。 我们发现这提高了准确性。 例如，具有CNN特征的间接微调的FV模型胜过了多尺度但未在[11]中报告的微调结果。 但是，使用NetFV进行直接微调显然更好。  \n",
    "\n",
    "#### 4.1.2 SVM training and evaluation  \n",
    "\n",
    "在微调前后的所有实验中，将训练和验证集合在一起，并通过设置学习超参数Csvm = 1来训练所提取特征上的一对所有线性SVM。由于我们的特征被归一化，所以最优的Csvm可能独立于数据集。经训练的分类器通过缩放权重向量来校准，使得正负训练样本的中值分别分别为+1和-1。对于每个数据集，我们通过翻转图像来加倍训练数据，并在测试时平均预测图像及其翻转副本。支持向量机训练比使用VGGM网络进行逻辑回归改善1-3％，但VGG-D网络提供的改进可以忽略不计。测试时间翻转可将VGG-M网络的性能平均提高0.5％，而对VGG-D网络的精度影响可以忽略不计。性能以所有数据集的正确分类图像的百分比来衡量。  \n",
    "\n",
    "### 4.2 Fine-grained recognition  \n",
    "\n",
    "我们评估以下细粒度数据集的方法并报告表2中的每个图像精度。  \n",
    "\n",
    "CUB-200-2011 [67]数据集包含200个鸟类的11,788幅图像，这些图像被分割成大致相同的列车和测试集，并具有零件和边界框的详细注释。 由于鸟类以不同的姿势和观点出现，并在杂乱的背景中占据小部分图像，因此对鸟类进行分类具有挑战性。 请注意，在我们所有的实验中，我们仅在训练期间使用图像标签，而没有任何部分或边界框注释。 在下面的章节中，“鸟”是指这个数据集上的结果。  \n",
    "\n",
    "FGVC飞机数据集[42]由100个飞机变体的10,000幅图像组成，并作为FGComp 2013挑战的一部分引入。 这项任务涉及从波音737-400中区分诸如波音737-300等变体。 差别是微妙的，例如，可以通过计算模型中窗口的数量来区分它们。 与鸟类不同，飞机倾向于占据图像的显着较大部分，并出现在相对清晰的背景中。 与鸟类相比，飞机在ImageNet数据集中的代表性也较小。  \n",
    "\n",
    "斯坦福汽车数据集[33]包含16,185个196类图像。 类别通常处于品牌，型号，年份的级别，例如“2012年特斯拉Model S”或“2012年宝马M3 coupe”。与飞机相比，汽车更小，出现在更混乱的背景中。 因此，物体和零件定位可能在这里扮演更重要的角色。 该数据集也是FGComp 2013挑战的一部分。  \n",
    "\n",
    "NABirds [64]比CUB数据集更大，包括555种鸟类香料的48,562幅图像，其中包括大部分北美的鸟类。 这项工作使公民科学家以经济有效的方式制作出高质量的注释。 该数据集还提供零件和边界框注释，但我们只使用类别标签来训练我们的模型。  \n",
    "\n",
    "#### 4.2.1 Bird species classification  \n",
    "\n",
    "与基线比较。 表2“鸟”栏显示了CUB-200-2011数据集的结果。 微调之后，纹理表示（NetBoWV，NetVLAD，NetFV）的端到端近似值显着提高。 具有间接微调（即FC层进行微调）的精确模型也有所改进，但改进较小（表2中以灰色斜体显示）。 通过对单尺度FV模型进行微调，使用VGG-M和VGG-D网络的性能优于[11] - 49.9％报告的多尺度结果和66.7％。 BCNN在所有型号中提供最佳准确性，其最佳表现模型获得84.1％的准确性（VGGM + VGG-D）。 次佳方法是具有81.9％准确度的NetVLAD。 我们发现增加集群中心不会提高NetVLAD的性能（参见第5.1节）。  \n",
    "\n",
    "我们还在更大的NABirds数据集上训练了B-CNN（VGG-M + VGG-D）模型。 对于该模型，我们跳过了SVM训练步骤，并使用softmax层预测报告了精度。 该模型的准确率达到79.4％，优于精确调整的VGG-D网络，准确度达到63.7％。 Van Horn等人 [64]在测试时使用AlexNet和部分注释获得75％的准确性，而使用GoogLeNet架构[60]的“神经激活星座”[57]获得76.3％的准确性。  \n",
    "\n",
    "与其他技术比较。表2显示了此数据集上其他性能最佳的方法。数据集还提供了边界框和部分注释和技术，它们根据训练和测试时使用的注释而不同（表中也显示）。两种在测试时无法提供边界框时表现良好的早期方法是“基于部分的R-CNN”的73.9％[70]和“姿态归一化的CNN”的75.7％[6]。这些方法基于AlexNet [34]，可以通过更深入，更精确的网络（如VGG-D）进行改进。例如，SPDA-CNN [69]使用VGG-D网络训练更好的部分检测器和特征表示，并报告84.6％的准确性。 Krause等人[31]使用弱监督方法学习部分检测器报告82.0％的准确性，然后使用VGG-D网络对[70]进行基于部分的分析。但是，我们的方法更简单快捷，因为它不依赖于培训和评估部件检测器。采用从CNN的两个不同层提取的成对特征的“跨层合并”技术[38]报告使用AlexNet的准确率为73.5％，使用VGG-D准确率为77.0％。该方法在训练和测试期间使用边界框。  \n",
    "\n",
    "空间变压器网络[29]是一种不依赖额外注释的高性能方法。 它使用batchnormalized Inception网络获得了84.1％的准确性[27]。 PD + SWFV-CNN方法将无监督部分检测与CNN特征的FV汇集相结合，以获得83.6％的准确性，并且在FC和FV汇集时具有84.5％的准确性。  \n",
    "\n",
    "自发布以来，B-CNN模型在其他方面已有所改进。 首先，提出了紧凑双线性池方法[19]来减小双线性特征的大小（我们在5.1节中对此进行了评估）。 Zhang等人 [72]将B-CNN与部分注释相结合，并将结果提高至85.9％。 Moghimi等人 [44]提高BCNN对不同分辨率的图像进行训练以获得86.2％的准确性。 虽然不能直接比较，Krause等人 [32]表明，通过对搜索引擎上的类别标签查询获得的更多标记数据进行两个数量级的训练，深层架构的性能可以提高到92.1％。  \n",
    "\n",
    "常见的错误。 图4显示了我们的微调B-CNN（VGGM + VGG-D）模型混淆的前六对类。 最混乱的一对类是“美国乌鸦”和“普通乌鸦”。 不同之处在于翅膀，栖息地和声音，这些都不容易从图像中测量出来。 其他混淆的类也是相似的 - 各种各样的Shrikes，燕鸥，捕蝇器，鸬鹚和海鸥。 我们注意到数据集估计有4.4％的标签噪声，因此其中一些错误可能来自不正确的标签[64]。  \n",
    "\n",
    "#### 4.2.2 Aircraft variant classification  \n",
    "\n",
    "与基线比较。 基线的趋势与鸟类中的趋势相似，只有少数例外。 SIFT的FV非常好（61.0％），与一些CNN基线相当。 与鸟类相比，基于VGG-D网络的模型的微调效果显着较大，这表明来自ImageNet数据集的更大的域偏移。 由于飞机主要出现在图像中心，裁剪中央图像提高了我们早期工作的准确性[37]。 我们将图像大小调整为512×512，然后裁剪中央448×448作为输入。 这通过B-CNN（VDD-D）模型实现了86.9％的最佳性能。 NetVLAD获得81.4％的准确性。  \n",
    "\n",
    "与其他技术比较。 该数据集不包含部分注释，因此这里不适用于鸟类数据集的几种最佳执行方法。 我们还将在FGComp 2013挑战赛上与“第2轨”的结果进行比较，即无界包围盒[2]。 表现最好的方法[25]是精心设计的FV-SIFT，其准确度达到80.7％。 我们的基线FV-SIFT与它们之间的显着差异是（i）较大的字典（256→1024），（ii）空间金字塔池（1×1→1×1 + 3×1），（iii）多个SIFT变体， （iv）多尺度SIFT。 Wang等人 [68]通过挖掘有区别的补丁三元组来报告88.4％的准确性，但在训练和测试期间需要边界框。 提高B-CNNs [44]的准确性达到了88.5％的现有技术水平。  \n",
    "\n",
    "#### 4.2.3 Car model classification  \n",
    "\n",
    "与基线比较。 使用SIFT的FV在这个数据集上取得了59.2％的准确度。 与鸟类和飞机相比，微调对汽车的影响更大。 使用B-CNN（VGG-D + VGG-M）模型，B-CNN再次胜过所有其他基线，准确度达到91.3％。 NetVLAD获得88.6％的准确性。  \n",
    "\n",
    "与其他技术比较。 这个数据集的最佳精度是由Krause等人。 [31]获得92.6％的准确性。 这与识别性贴片三联体[92]的准确性为92.5％，Boosted BCNNs的准确性为92.1％[44]密切相关。 与其他方法不同，Boosted B-CNN在训练时不依赖于包围盒。  \n",
    "\n",
    "### 4.3 Texture and scene recognition  \n",
    "\n",
    "我们在三个纹理数据集 - 可描述纹理数据集（DTD）[10]，Flickr材料数据集（FMD）[56]和KTH-TISP2-b（KTH-T2b）[7]上进行实验。 DTD由5640个图像组成，标有47个可描述的纹理属性。 FMD由10个材料类别组成，每个材料类别包含100个图像。与从互联网收集图像的DTD和FMD不同，KTH-T2b包含4752张在受控尺度，姿势和照明下拍摄的材料图像。 KTH-T2b数据集将图像分成每个类别的四个样本。我们按照标准方案对一个样本进行培训并对其余三个进行测试。在DTD和FMD上，我们随机将数据集分成10个分组，并报告分组的平均准确度。除此之外，我们还评估了我们的MIT室内场景数据集模型[51]。室内场景结构微弱，无序纹理表现已被证明是有效的。该数据集由67个室内类别和定义的训练和测试分割组成。  \n",
    "\n",
    "我们将B-CNN与先前使用VGGD网络的CNN特征[11]的FV汇集方法相比较。这些结果没有进行微调。在麻省理工学院室内数据集中，微调B-CNNs在s = 1时使用relu5 3导致72.8％→73.8％的小幅提高，而在其他数据集上，改进可以忽略不计，可能是由于这些数据集的尺寸相对较小。表3示出了使用B-CNN和FV表示，通过单个尺度的特征和来自多个尺度2 s，s∈{1.5：-0.5： - 3}的特征相对于224×224图像获得的结果。我们放弃图像比滤镜的接受区域的尺寸更小的尺度，或者效率大于10242像素的尺度。在输入图像的所有尺度上，两种方法的性能是相同的。多个量表一致地提高了准确度。这里报道的多尺度FV结果与Cimpoi等报道的结果相当（±1％）。对于除KTH-T2b（-4％）以外的所有数据集[11]。这些差异是由于CNN的选择（它们使用19层VGG网络的conv5 4层）和范围的范围。这些结果表明，B-CNN与用于纹理识别的FV池相当。一个缺点是具有64个GMM组件的FV特征具有比双线性特征（512×512）更小的尺寸（64×2×512）。然而，双线性特征是非常多余的，它们的维数可以减少一个数量级而不损失性能（见5.1节）。  \n",
    "\n",
    "## 5. ANALYSIS OF BILINEAR CNNS  \n",
    "\n",
    "### 5.1 Dimensionality reduction  \n",
    "\n",
    "CNN特征的外积产生非常高维的图像描述符，例如表2中B-CNN模型的262K。我们早期的工作[36]表明，特征是高度冗余的，它们的维数可以减少一个数量级 没有分类表现的损失。 先前的工作[30]也表明，在基于SIFT的FV和VLAD的情况下，可以获得高度紧凑的表示。  \n",
    "\n",
    "在本节中，我们将研究第3节中为细粒度识别提出的各种纹理模型的精度和特征维数之间的权衡。 对于NetVLAD和NetFV，可以通过更改群集中心的数量来改变要素维度。 对于B-CNN，考虑在特征x和y之间计算外积的情况。 有几种减少特征维度的策略：  \n",
    "\n",
    "（1）将外积投影到较低维空间中，即Φ（x，y）= vec（x T y）P，其中P是投影矩阵并且vec算子将矩阵重塑为向量。  \n",
    "\n",
    "（2）将两个特征投影到较低维空间并计算外积，Φ（x，y）=（xA）T（yB），其中A，B是投影矩阵。  \n",
    "\n",
    "（3）将特征之一投影到较低维空间中并计算外积，即通过在先前方法中将B设置为单位矩阵。  \n",
    "\n",
    "在每种情况下，投影矩阵都可以使用主成分分析（PCA）进行初始化。尽管第一种方法很简单，但由于特征的高维度（外部产品的协方差矩阵对于d维特征具有d 4个条目），计算PCA在计算上是昂贵的。第二种方法在计算上很有吸引力，但是两个PCA投影特征的外积导致了我们早期工作[37]和最近[19]中所显示的准确性的显着降低。我们相信这是因为在PCA轮换之后，各个维度之间的特征不再相互关联。值得注意的是，使用PCA（第三选项）减少仅一个特征的维度在实践中效果很好。虽然可以使用PCA初始化投影，但可以与分类层一起训练它们。这项技术曾用于我们早期的工作[37]以减少特征尺寸。它在使用相同网络时打破了特征的对称性，并且是部分共享特征流水线的一个例子（图3b）。它也类似于VLA和FV表示法的计算，其中fA和fB都基于相同的基础特征。  \n",
    "\n",
    "NetFV和NetVLAD的精度与图5所示的功能维度相关。 这些结果是通过改变聚类中心的数量获得的。 结果表明，当集群中心数量超过32个时，NetVLAD和NetFV的性能不会提高。  \n",
    "\n",
    "图6显示了相同的B-CNN功能。 我们还比较了我们的PCA方法和最近提出的紧凑双线性池（CBP）技术[19]。 CBP使用稀疏线性投影与张量素描[48]的特征逼近外层产品。 具有512×512尺寸的整个模型的性能（具有和不具有微调）显示为直线。 在鸟类和飞机上，维度可以减少16倍（即，达到32×512），精度损失小于1％。 相比之下，具有相同特征尺寸（即32个组件）的NetVLAD的精确度降低约3-4％。 总体而言，对于给定的维度预算，预计的B-CNN优于NetVLAD和NetFV表示。 PCA方法比CBP稍差。 然而，一个优点是PCA可以实现为密集矩阵乘法，其经验性地比计算傅立叶变换及其逆矩阵的CBP快1.5倍。  \n",
    "\n",
    "## 5.2 Training B-CNNs on ImageNet LSVRC  \n",
    "\n",
    "我们评估在ImageNet LSRVC 2012数据集上从头开始训练的B-CNN [54]。 特别是，我们训练带有VGG-M网络的Relu5层输出的B-CNN，并将其与标准VGG-M网络进行比较。 这允许将双线性池与FC池进行直接比较，因为其余架构在两个网络中都是相同的。 此外，我们通过空间“抖动”数据来比较CNNs中获得的隐式平移不变性的效果，以及由于无序池的显着的B-CNN的平移不变性。  \n",
    "\n",
    "我们训练这两个网络对224×224图像进行不同空间抖动量的分类 - “f1”翻转，“f5”翻转+ 5翻译，“f25”翻转+ 25翻译。 随机抽样进行训练，其中每个示例随机选择一个抖动副本。 随机初始化参数并使用具有多个时期动量的随机梯度下降进行训练。 我们从高学习率开始，并在验证错误停止下降时将其降低10倍，并且持续到没有观察到进一步的改进。  \n",
    "\n",
    "表4显示了B-CNN和VGG-M的“top1”和“top5”验证错误。 验证错误报告在单个中心裁剪图像上。 请注意，我们培训的所有网络既没有PCA颜色抖动也没有批量标准化，我们的基线结果在[9]中报告的top1错误的2％以内。 VGG-M模型在训练过程中通过翻转增强获得46.4％的top1错误。 随着f25增强，性能显着提高至39.6％。 B-CNN使用f1增强获得38.7％top1误差，优于使用f25增强训练的VGG-M。 结果表明，B-CNN具有歧视性，对翻译具有鲁棒性，而且显式的翻译不变性更有效。 这种趋势也反映在最新的深层架构中，例如Residual Networks [26]，它用全局池层来替换完全连接的层。  \n",
    "\n",
    "### 5.3 Visualizing learned models  \n",
    "\n",
    "B-CNN单位的最高激活。 图7显示了微调B-CNN（VGG-D + VGG-M）模型的VGG-D的relu5 3层和VGG-M网络的relu5层的几个单元的最高激活。 这两个网络都包含强烈激活高度本地化功能的设备。 例如，VGG-D的最后一行检测到“簇绒头部”，而同一列中的第四行检测到鸟类的“红黄色条纹”。 对于飞机也是如此，这些单元将不同类型的窗户，鼻子，垂直稳定器定位在一起，其中一些专门检测特定的客机徽标。 对于汽车，单位可以在不同类型的头灯/尾灯，车轮等上激活。  \n",
    "\n",
    "反转类别。 为了理解B-CNN学到的特性，我们通过“反演”可视化了一个类别的预先图像。 我们使用Mahendran和Vedaldi [41]的框架来生成一个图像，该图像基于B-CNN分类器为目标类别C生成高分。 具体而言，对于图像x和图层ri，i = 1，...。。。 ，n，我们使用B-CNN计算双线性特征Bri。 令Cri为使用Bri训练的线性分类器以监督方式获得的类别预测概率。 我们通过求解以下优化来获得最大化目标标签C的图像：  \n",
    "\n",
    "这里，L是损失函数，例如标签C的负对数似然性，并且γ是折衷参数。 之前的图像Γ（x）鼓励输出图像的平滑度。 我们使用β= 2的TVβ范数：  \n",
    "\n",
    "指数β= 2通过实验发现可以减少优化过程中出现的“尖峰”伪影[41]。 我们使用基于VGG-D网络的B-CNN。 在我们的实验中，输入图像在计算目标双线性特征之前调整为224×224像素。 我们求解x∈R 224×224×3的优化问题。 较低的分辨率主要用于速度，因为双线性特征的维度与图像的大小无关。 我们使用在双线性特征上训练的分类器来优化类概率的对数似然性。我们使用L-BFGS进行优化，并使用back-down函数计算目标相对于x的梯度。 传播。 根据经验发现超参数γ= 10-8可以产生良好的反转图像。 我们还向读者介绍我们早期的工作[36]和更近期的工作[61]，其中此框架应用于纹理合成和使用属性进行样式转移。  \n",
    "\n",
    "图8显示了DTD，FMD，MIT室内和CUB-200-2011数据集的各种类别的反转图像。 这些图像揭示了B-CNN如何将各种类别表现为纹理。 例如，DTD的点分类包含各种颜色和点尺寸的图像，而反转图像由多尺度多色点组成。 来自FMD的水和木材的相反图像是这些类别的高度代表。 麻省理工学院室内数据集的逆向图像揭示了一个类别的关键属性 - 一家书店有一些书架，一家洗衣店在各种规模和地点都有洗衣机。 各种鸟类的相反图像捕捉到身体上独特的颜色和图案。 图9通过递增地添加双线性表示中的图层来可视化重构。 尽管relu5 3图层提供了最佳的识别准确性，但由于缺少颜色信息，使用该图层不会产生良好的反转图像。  \n",
    "\n",
    "## 6. CONCLUSION  \n",
    "\n",
    "我们提出了B-CNN架构，它汇总了CNN激活的二阶统计量，从而导致图像的无序表示。这些网络可以以端对端的方式进行训练，允许在大型数据集上进行从头开始的训练，以及用于传输学习的领域专用微调。此外，这些型号相当高效，可在NVIDIA Titan X GPU上以30〜100 FPS的速度处理448×448分辨率图像。我们还将BCNN与深度纹理表示的精确和近似变体进行了比较，并研究了它们提供的精度和内存折衷。主要结论是，外部产品表示的变体在各种精细训练，纹理和场景识别任务中非常有效。而且，这些表示是多余的，并且在大多数情况下它们的尺寸可以减小一个数量级而不会显着降低准确性。 B-CNN的可视化表明，这些模型有效地将对象表示为纹理，并且它们的单位与用于细粒度识别的局部属性相关联。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1407.3867.pdf  \n",
    "\n",
    "https://arxiv.org/pdf/1605.06878.pdf\n",
    "\n",
    "https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Xiao_The_Application_of_2015_CVPR_paper.pdf\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/24738319?utm_campaign=rss&utm_content=title&utm_medium=rss&utm_source=rss"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
