{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前最先进的目标检测网络需要先用区域建议算法推测目标位置，像SPPnet和Fast R-CNN这些网络已经减少了检测网络的运行时间，这时计算区域建议就成了瓶颈问题。本文中，我们介绍一种区域建议网络（Region Proposal Network, RPN），它和检测网络共享全图的卷积特征，使得区域建议几乎不花时间。RPN是一个全卷积网络，在每个位置同时预测目标边界和objectness得分。RPN是端到端训练的，生成高质量区域建议框，用于Fast R-CNN检测。通过一种简单的交替运行优化方法，RPN和Fast R-CNN可以在训练时共享卷积特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 摘要  \n",
    "\n",
    "Faster R-CNN可以看做“区域生成网络+Fast R-CNN“的系统，其中区域生成网络代替了Fast RCNN中的Selective Search方法。  \n",
    "\n",
    "Faster R-CNN解决了三个问题：  \n",
    "1、如何设计区域生成网络；2、如何训练区域生成网络；3、如何让区域生成网络和Fast R-CNN网络共享特征提取网络\n",
    "\n",
    "该论文开发了一种允许两个网络间共享卷积层的技术，而不是分别学习两个网络。不仅仅定义一个包含了RPN和Fast R-CNN的单独网络，然后用反向传播联合优化它那么简单。原因是Fast R-CNN训练依赖于固定的目标建议框，而且并不清楚当同时改变建议机制时，学习Fast R-CNN会不会收敛。所以训练的时候采用了一种实用的4步训练算法，通过交替优化来学习共享的特征。 \n",
    "第一步，训练RPN，该网络用ImageNet预训练的模型初始化，并端到端微调用于区域建议任务。第二步，利用第一步的RPN生成的建议框，由Fast R-CNN训练一个单独的检测网络，这个检测网络同样是由ImageNet预训练的模型初始化的，这时候两个网络还没有共享卷积层。第三步，我们用检测网络初始化RPN训练，但我们固定共享的卷积层，并且只微调RPN独有的层，现在两个网络共享卷积层了。第四步，保持共享的卷积层固定，微调Fast R-CNN的fc层。这样，两个网络共享相同的卷积层，构成一个统一的网络。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPN网络介绍  \n",
    "\n",
    "RPN是一种全卷积网络（fully-convolutional network, FRN），可以针对生成检测建议框的任务端到端地训练。为了统一RPN和Fast R-CNN目标检测网络，提出一种简单的训练方案，即保持建议框固定，微调区域建议和微调目标检测之间交替进行。这个方案收敛很快，最后形成可让两个任务共享卷积特征的标准网络。  \n",
    "\n",
    "先使用VGG等现有模型提取特征，然后使用一个小的网格在卷积最后得到的特征图上进行滑动扫描，这个滑动的网格每次与特征图上n\\*n 的窗口全连接，然后映射到一个低维向量，最后将这个低维向量送入到两个全连接层，对特征图上所有可能的候选框进行分类和回归。  \n",
    "\n",
    "RPN目的是对给定生成图片，生成前景框并给出对应的位置，并没有给出每个前景框属于哪一类目标，所以训练据只是二分类的：前景和背景。\n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/train_rpn.jpg?raw=true )\n",
    "\n",
    "### 构造数据  \n",
    "\n",
    "训练RPN网络的输入数据有两部分组成：  \n",
    "1 data: 原图或者是某一层的feature map\n",
    "2 label: ground truth bounding box  \n",
    "\n",
    "### anchor 生成  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/RPN.jpg?raw=true)  \n",
    "\n",
    "由VGG或ZF等训练好的模型提取输入图片特征，在大小为51\\*39\\*256的特征图上，滑动窗口，提取region proposals。滑动窗的中心就是当前特征图上的每个像素的位置，按一定的步长可以对应回原图的相应位置，在每个中心点位置上以3种尺度{128,256,512}，3种长宽比{1:1,1:2,2:1}提取9个候选窗口。  \n",
    "\n",
    "分类层（cls_score）输出每一个位置上，9个anchor属于前景和背景的概率； \n",
    "窗口回归层（bbox_pred）输出每一个位置上，9个anchor对应窗口应该平移缩放的参数。  \n",
    "\n",
    "这样共生成9\\*39\\*51个候选窗口，这里并没有显式的生成这些图片，而是根据feature map 推导出来的。  \n",
    "\n",
    "### 计算loss的训练样本  \n",
    "1 data: 原图\n",
    "2 label: ground truth bounding box  \n",
    "3 与label重叠面积最大的anchor作为前景样本  \n",
    "4 与label重叠面积比例大于0.7的的anchor作为前景样本  \n",
    "5 与label重叠面积比例小于0.3的的anchor作为背景样本\n",
    "6 剩余的anchor不进行loss 的计算，即不参与训练\n",
    "7 训练样本中每个mini-batch由包含了许多正负样本的单个图像组成。我们可以优化所有anchor的损失函数，但是这会偏向于负样本，因为它们是主要的。因此，我们随机地在一个图像中采样256个anchor，计算mini-batch的损失函数，其中采样的正负anchor的比例是1:1。如果一个图像中的正样本数小于128，我们就用负样本填补这个mini-batch。  \n",
    "\n",
    "### loss 计算  \n",
    "\n",
    "对一个图像的损失函数定义为：  \n",
    "\n",
    "$L({p_i},{t_i}) = \\frac{1}{N_{cls}}\\sum_iL_{cls}(p_i,p_i^*) + \\lambda \\frac{1}{N_{reg}}\\sum_i p_i^* L_{reg}(t_i,t_i^*)$  \n",
    "\n",
    "这里 i 是mini-batch中anchor的索引，$p_i$是anchor i 的预测概率，$p_i^*$是acchor i 的label，$t_i$是预测的bbox的四个参数化坐标，$t_i^*$是与anchor i 对应的标注好的四个参数化坐标。  \n",
    "\n",
    "$L_{cls}(p_i,p_i^*)$就是前景背景的分类损失。  \n",
    "这里主要介绍$L_{reg}(t_i,t_i^*)$即边框回归  \n",
    "\n",
    "以下参数定义的字母与上述不同，只是编辑问题  \n",
    "\n",
    "一个anchor由一个四维向量表示: P=(Px,Py,W,H), 其中, (Px,Py)为中心点的位置(RCNN)或左上角的位置(Fast RCNN), (W,H)为它的宽和高. 它对应的bbox ground truth 由G=(Gx,Gy,Gw,Gh)表示, 各参数的涵义与P类似.  \n",
    "\n",
    "#### L2 loss  \n",
    "\n",
    "用f(P)表示regressor的输出, 最简单的loss可以表示为:   \n",
    "\n",
    "$L_* = (f_*(P) - G_*)^2$  \n",
    "\n",
    "其中，\\* 表示x,y,w,h, 整个loss：L=Lx+Ly+Lw+Lh  \n",
    "\n",
    "这样预测的是bbox的绝对坐标以及长宽比，这样计算的问题是loss的大小会受到图片大小的影响，例如, 当ground truth 分别为(100,100,200,200), (10,10,20,20)时, 假如分别得到(90,90,200,200), (8,8,20,20)的bbox预测值. 那么前者对应的loss会远大于后者, 但是从实际情况上来看, $\\frac{100-90}{100} = 0.1$,$\\frac{10-8}{10} = 0.2$, 前者的相对误差要小于后者. 所以需要一个规范化(normalization)处理. 若在loss上规范化:  \n",
    "\n",
    "$L_* = (\\frac {f_*(P) - G_*}{**})^2$ \\* 是x,y,w,h,\\*\\*是W或H  \n",
    "\n",
    "这样loss是不受绝对大小的影响了, 但是还有一个问题: f(P)直接输出了绝对距离, 这种输出值是没有上下限的. 有可能会使训练的收敛变得很困难，超参数的选择也会很困难。所以, 原文中的规范化操作直接在region的bbox ground truth上进行了处理。  \n",
    "\n",
    "$t_x = (x - x_a) / w_a, t_y = (y - y_a) / h_a,t_w = log(w/w_a), t_h = log(h/h_a)$  \n",
    "\n",
    "$t_x^* = (x^* - x_a) / w_a, t_y^* = (y^* - y_a) / h_a,t_w^* = log(w^*/w_a), t_h^* = log(h^*/h_a)$ \n",
    "\n",
    "x,y,w,h 分别表示bbox的坐标和宽高。$x,x_a,x_x$分别是predicted box, anchor box, 和 groundtruth box 的坐标，其他参数含义类似。这里可看做将bbox向距离近的ground-truth box进行回归。  \n",
    "\n",
    "这样得到的是bbox在图片上的相对位置，w和h在log空间，这样做是为了降低w和h产生的loss的数量级，让这部分loss的比重更小一点，不至于因为w,h太大，让x,y 产生的loss无用，因为若x,y不准确，w,h再准确也没有用。  \n",
    "\n",
    "#### Smooth L1 loss  \n",
    "\n",
    "当预测值与目标值相差较大的时候，容易造成梯度爆炸，因梯度包含了$t - t_*$。所以边框回归用了SmoothL1Loss。文章认为这种损失函数对outliers更加鲁棒。  \n",
    "\n",
    "$L_{reg}(t_i,t_i^*) = smooth_{L_1}(t_i - t_i^*)$  \n",
    "\n",
    "$smooth_{L_1}(x) = \\begin{cases} 0.5x^2,  if |x| < 1 \\\\ |x|-0.5,  otherwise \\end{cases}$ \n",
    "\n",
    "这样可以避免梯度爆炸  \n",
    "\n",
    "Fast-RCNN为了使用SmoothL1Loss定义了一个新的layer, 它的实现更general:   \n",
    "\n",
    "$x_i = p_i^*(t_i - t_i^*)$ \n",
    "$smooth_{L_1}(x) = \\begin{cases} 0.5x^2 \\sigma^2,  if |x \\sigma| < 1 \\\\ |x|-0.5/\\sigma^2,  otherwise \\end{cases}$ \n",
    "$p_i^* = 1 $的时候才计算loss，即只有前景框才参与边框回归。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### generate proposals  \n",
    "\n",
    "训练好RPN网络之后，就需要生成proposals，待后面rcnn网络训练使用。  \n",
    "\n",
    "先导入RPN前向的model  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/test_rpn.jpg?raw=true) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rpn网络输入data：im = cv2.imread(imdb.image_path_at(i))\n",
    "\n",
    "经过前向运算之后，得到rpn_bbox_pred 和 rpn_clc_score，分别是边框回归后每个anchor的相对位置和前景的概率  \n",
    "\n",
    "anchor 的相对位置经过还原之后，变为绝对的位置，去掉面积小于阈值的目标  \n",
    "\n",
    "对剩余目标的前景概率进行排序，然后进行非极大值抑制，每张图片最多保留2000个前景区域，然后输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def py_cpu_nms(dets, thresh):\n",
    "    \"\"\"Pure Python NMS baseline.\"\"\"\n",
    "    x1 = dets[:, 0]\n",
    "    y1 = dets[:, 1]\n",
    "    x2 = dets[:, 2]\n",
    "    y2 = dets[:, 3]\n",
    "    scores = dets[:, 4]\n",
    "\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        # 计算相交面积\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= thresh)[0]\n",
    "        order = order[inds + 1]\n",
    "\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 非极大值抑制  \n",
    "\n",
    "抑制的过程是一个迭代-遍历-消除的过程  \n",
    "\n",
    "1 将所有框的得分排序，选中最高分及其对应的框  \n",
    "2 遍历其余的框，如果和当前最高分框的重叠面积(IOU)大于一定阈值，我们就将框删除  \n",
    "3 从未处理的框中继续选一个得分最高的，重复上述过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_fast_rcnn  \n",
    "\n",
    "将上面rpn网络生成的proposals进行处理之后，送入fast rcnn 网络，对目标检测网络进行训练  \n",
    "\n",
    "### 1 训练数据：\n",
    "\n",
    "1 data: 原图  \n",
    "2 label: ground truth bounding box  \n",
    "3 rpn_proposals：rpn网络生成的前景区域\n",
    "\n",
    "### 2 rpn_proposals 处理  \n",
    "\n",
    "1 fast_rcnn 分类函数样本构造  \n",
    "\n",
    "计算 rpn_proposals 与 ground truth bounding box 的重叠面积，重叠面积比例大于0.5的proposal作为fast_rcnn分类函数的正样本，重叠面积比例大于0.1且小于0.5的作为fast_rcnn分类函数的负样本  \n",
    "\n",
    "计算上述作为正样本的proposal 与 ground truth bounding box的重叠面积，为每个 proposal 分配21个分类标签，这样fast_rcnn分类函数的样本就构造成功了  \n",
    "\n",
    "2 fast_rcnn 边框回归样本构造  \n",
    "\n",
    "计算 rpn_proposals 与 ground truth bounding box 的重叠面积，计算重叠面积比例大于0.5的proposal的与对应ground truth bounding box的回归距离，用smoothL1loss 优化这些回归距离，进行边框回归  \n",
    "\n",
    "### 3 fast_rcnn网络结构\n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/fast_rcnn_train.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
