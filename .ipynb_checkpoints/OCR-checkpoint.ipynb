{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "基于图像的序列识别一直是计算机视觉中长期存在的研究课题。在本文中，我们研究了场景文本识别的问题，这是基于图像的序列识别中最重要和最具挑战性的任务之一。提出了一种将特征提取，序列建模和 transcription 整合到统一框架中的新型神经网络架构。与之前的场景文本识别系统相比，所提出的架构具有四个不同的特性：（1）与大多数现有的组件需要单独训练和协调的算法相比，它是端对端训练的。（2）它自然地处理任意长度的序列，不涉及字符分割或水平尺度标准化。（3）它不仅限于任何预定义的词汇，并且在无词典和基于词典的场景文本识别任务中都取得了显著的表现。（4）它产生了一个有效而小得多的模型，这对于现实世界的应用场景更为实用。在包括 IIIT-5K，Street View Text 和 ICDAR 数据集在内的标准基准数据集上的实验证明了提出的算法比现有的技术更有优势。此外，提出的算法在基于图像的音乐乐谱识别任务中表现良好，这显然证实了模型的泛化能力。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "最近，社区已经看到神经网络的强大复兴，这主要受到深度神经网络模型，特别是深度卷积神经网络（DCNN）在各种视觉任务中的巨大成功的推动。然而，最近大多数与深度神经网络相关的工作主要致力于检测或对象分类。在本文中，我们关注计算机视觉中的一个经典问题：基于图像的序列识别。在现实世界中，稳定的视觉对象，如场景文字，手写字符和乐谱，往往以序列的形式出现，而不是孤立地出现。与一般的对象识别不同，识别这样的序列化对象通常需要系统预测一系列对象标签，而不是单个标签。因此，可以自然地将这样的对象的识别作为序列识别问题。序列化对象的另一个独特之处在于它们的长度可能会有很大变化。例如，英文单词可以由 2 个字符组成，如“OK”，或由 15 个字符组成，如“congratulations”。因此，最流行的深度模型像 DCNN 不能直接应用于序列预测，因为 DCNN 模型通常对具有固定维度的输入和输出进行操作，因此不能产生可变长度的标签序列。  \n",
    "\n",
    "已经针对特定的序列化对象（例如场景文本）进行了一些尝试来解决该问题。例如，[35,8]中的算法首先检测单个字符，然后用 DCNN 模型识别这些检测到的字符，并使用标注的字符图像进行训练。这些方法通常需要训练强字符检测器，以便从原始单词图像中准确地检测和裁剪每个字符。一些其他方法（如[22]）将场景文本识别视为图像分类问题，并为每个英文单词（总共 9 万个词）分配一个标签，结果是一个大的训练模型中有很多类，这很难泛化到其它类型的序列化对象，如中文文本，音乐配乐等，因为这种序列的基本组合数目可能大于 100 万。总之，目前基于 DCNN 的系统不能直接用于基于图像的序列识别。  \n",
    "\n",
    "循环神经网络（RNN）模型是深度神经网络家族中的另一个重要分支，主要设计来处理序列数据。RNN 的优点之一是在训练和测试中不需要序列目标图像中每个元素的位置。然而，将输入目标图像转换成图像特征序列的预处理步骤通常是必需的。例如，Graves 等人从手写文本中提取一系列几何或图像特征，而 Su 和 Lu 将字符图像转换为序列 HOG 特征。预处理步骤独立于流程中的后续组件，因此基于 RNN 的现有系统不能以端到端的方式进行训练和优化。 \n",
    "\n",
    "一些不是基于神经网络的传统场景文本识别方法也为这一领域带来了 insightful 的想法和新颖的效果。例如，Almazan 等人和 Rodriguez-Serrano 等人提出将单词图像和文本字符串嵌入到公共向量子空间中，并将词识别转换为检索问题。Yao 等人和 Gordo 等人使用中层特征进行场景文本识别。虽然在标准基准数据集上取得了有效的性能，但是前面的基于神经网络的算法以及本文提出的方法通常都优于这些方法。  \n",
    "\n",
    "本文的主要贡献了一种新颖的神经网络模型，其网络架构专门用于识别图像中的序列化对象。所提出的神经网络模型被称为卷积循环神经网络（CRNN），因为它是 DCNN 和 RNN 的组合。对于序列化对象，CRNN 与传统神经网络模型相比具有一些独特的优点：1）可以直接从序列标签（例如单词）学习，不需要详细的标注（例如字符）；2）直接从图像数据学习信息表示时具有与 DCNN 相同的性质，既不需要手工特征也不需要预处理步骤，包括二值化/分割，组件定位等；3）具有与 RNN 相同的性质，能够产生一系列标签；4）对类序列对象的长度无约束，只需要在训练阶段和测试阶段对高度进行归一化；5）与现有技术相比，它在场景文本（字识别）上获得更好或更具竞争力的表现。6）它比标准 DCNN 模型包含的参数要少得多，占用更少的存储空间。  \n",
    "\n",
    "## 2. The Proposed Network Architecture  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/1.jpg?raw=true)  \n",
    "\n",
    "如图 1 所示，CRNN 的网络架构由三部分组成，包括卷积层，循环层和 transcription 层，从底向上。  \n",
    "\n",
    "在 CRNN 的底部，卷积层自动从每个输入图像中提取特征序列。在卷积网络之上，构建了一个循环网络，用于对卷积层输出的特征序列的每一帧进行预测。CRNN 顶部的 transcription 层将循环层的每帧预测转化为标签序列。虽然 CRNN 由不同类型的网络架构（如 CNN 和 RNN）组成，但可以通过一个损失函数进行联合训练。   \n",
    "\n",
    "### 2.1. Feature Sequence Extraction  \n",
    "\n",
    "在 CRNN 模型中，通过采用标准 CNN 模型（去除全连接层）中的卷积层和最大池化层来构造卷积层的组件。这样的组件用于从输入图像中提取序列特征表示。在进入网络之前，所有的图像需要缩放到相同的高度。然后从卷积层组件产生的特征图中提取特征向量序列，这些特征向量序列作为循环层的输入。具体地，特征序列的每一个特征向量在特征图上按列从左到右生成。这意味着第 i 个特征向量是所有特征图第 i 列的拼接。在我们的设置中每列的宽度固定为单个像素。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/2.jpg?raw=true)  \n",
    "\n",
    "由于卷积层，最大池化层和元素激活函数在局部区域上执行，因此它们是平移不变的。因此，特征图的每列对应于原始图像的一个矩形区域，并且这些矩形区域与特征图上从左到右的相应列具有相同的顺序。如图 2 所示，特征序列中的每个向量关联一个感受野，并且可以被认为是该区域的图像描述符。  \n",
    "\n",
    "鲁棒的，丰富的和可训练的深度卷积特征已被广泛应用于各种视觉识别任务。一些以前的方法已经使用 CNN 来学习诸如场景文本之类的序列化对象的鲁棒表示。然而，这些方法通常通过 CNN 提取整个图像的整体表示，然后收集局部深度特征来识别序列化对象的每个分量。由于 CNN 要求将输入图像缩放到固定尺寸，以满足其固定的输入尺寸，因为它们的长度变化很大，因此不适合序列化对象。在 CRNN 中，我们将深度特征传递到序列表示中，以便对类序列对象的长度变化保持不变。  \n",
    "\n",
    "### 2.2. Sequence Labeling  \n",
    "\n",
    "一个深度双向循环神经网络是建立在卷积层的顶部，作为循环层。循环层预测特征序列 $x=x_1,...,x_T$ 中每一帧 $x_t$ 的标签分布 $y_t$。循环层的优点有三个。首先，RNN 具有很强的捕获序列内上下文信息的能力。对于基于图像的序列识别使用上下文提示比独立处理每个符号更稳定且更有帮助。以场景文本识别为例，宽字符可能需要一些连续的帧来完全描述（参见图 2）。此外，一些模糊的字符在观察其上下文时更容易区分，例如，通过对比字符高度更容易识别“il”而不是分别识别它们中的每一个。其次，RNN 可以将误差差值反向传播到其输入，即卷积层，从而允许我们在统一的网络中共同训练循环层和卷积层。第三，RNN 能够从头到尾对任意长度的序列进行操作。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/3.jpg?raw=true)  \n",
    "\n",
    "传统的 RNN 单元在其输入和输出层之间具有自连接的隐藏层。每次接收到序列中的帧 $x_t$ 时，它将使用非线性函数来更新其内部状态 $h_t$，该非线性函数同时接收当前输入 $x_t$ 和过去状态 $h_{t−1}$ 作为其输入：$h_t = g(x_t,h_{t-1})$。那么预测 $y_t$ 是基于 $h_t$ 的。以这种方式，过去的上下文 $\\{x_{t'}\\}_{t'< t}$ 被捕获并用于预测。然而，传统的 RNN 单元有梯度消失的问题，这限制了其可以存储的上下文范围，并给训练过程增加了负担。长短时记忆（LSTM）是一种专门设计用于解决这个问题的 RNN 单元。LSTM（图 3 所示）由一个存储单元和三个多重门组成，即输入，输出和遗忘门。在概念上，存储单元存储过去的上下文，并且输入和输出门允许单元长时间地存储上下文。同时，单元中的存储可以被遗忘门清除。LSTM 的特殊设计允许它捕获长距离依赖，这经常发生在基于图像的序列中。  \n",
    "\n",
    "LSTM 是定向的，它只使用过去的上下文。然而，在基于图像的序列中，两个方向的上下文是相互有用且互补的。因此，我们遵循[17]，将两个 LSTM，一个向前和一个向后组合到一个双向 LSTM 中。此外，可以堆叠多个双向 LSTM，得到如图 3.b 所示的深双向 LSTM。深层结构允许比浅层抽象更高层次的抽象，并且在语音识别任务中取得了显著的性能改进。  \n",
    "\n",
    "在循环层中，误差在图 3.b 所示箭头的相反方向传播，即反向传播时间（BPTT）。在循环层的底部，传播差异的序列被连接成映射，将特征映射转换为特征序列的操作进行反转并反馈到卷积层。实际上，我们创建一个称为“Map-to-Sequence”的自定义网络层，作为卷积层和循环层之间的桥梁。  \n",
    "\n",
    "### 2.3. Transcription\n",
    "\n",
    "转录是将 RNN 所做的每帧预测转换成标签序列的过程。数学上，转录是根据每帧预测找到具有最高概率的标签序列。在实践中，存在两种转录模式，即无词典转录和基于词典的转录。词典是一组标签序列，预测受拼写检查字典约束。在无词典模式中，预测时没有任何词典。在基于词典的模式中，通过选择具有最高概率的标签序列进行预测。  \n",
    "\n",
    "#### 2.3.1 Probability of label sequence  \n",
    "\n",
    "我们采用 Graves 等人提出的 Connectionist Temporal Classification（CTC）层中定义的条件概率。按照每帧预测结果 $y = y_1,...,y_T$ 对计算标签序列 $l$ 的概率，并忽略 $l$ 中每个标签所在的位置。因此，当我们使用这种概率的负对数似然作为训练网络的目标函数时，我们只需要图像及其相应的标签序列，避免了标注单个字符位置的劳动。  \n",
    "\n",
    "条件概率的公式简要描述如下：输入是序列 $y = y_1,...,y_T$，其中 T 是序列长度。这里，每个 $y_t \\in R^{\\mid L' \\mid}$ 是在集合 $L' = L \\bigcup$ 上的概率分布，其中 L 包含了任务中的所有标签（例如，所有英文字符），以及由 - 表示的“空白”标签。序列到序列的映射函数 BB 定义在序列 $\\pi \\in L'^T$ 上，其中 T 是长度。BB 通过首先删除重复的标签，然后删除 blank，将 $\\pi$ 映射到 $l$ 上。例如，BB 将“–hh-e-l-ll-oo–”（- 表示blank）映射到“hello”。然后，条件概率被定义为由 BB 将所有 $\\pi$ 映射到 $l$ 上的概率之和：  \n",
    "\n",
    "$p(l|y) = \\sum_{\\pi:B(\\pi)=1} p(\\pi|y) \\tag{1}$\n",
    "\n",
    "其中，$\\pi$ 的概率定义为 $p(\\pi|y) = \\quad_{t=1}{T} y_{\\pi_t}^t, \\ y_{\\pi_t}^t$ 是时刻 t 时有标签 $\\pi_t$ 的概率。由于存在指数级数量的求和项，直接计算上述方程在计算上是不可行的。然而，使用[15]中描述的前向算法可以有效的进行计算。  \n",
    "\n",
    "#### 2.3.2 Lexicon-free transcription  \n",
    "\n",
    "在这种模式下，将具有方程 1 中定义的最高概率的序列 $l^*$ 作为预测。由于不存在用于精确找到解的可行方法，我们采用[15]中的策略。序列 $l^*$ 通过 $l^* \\approx B(arg max_{\\pi} p(\\pi|y))$ 近似发现，e.g 在每个时间戳 t 采用最大概率的标签 $\\pi_t$，并将结果序列映射到 $l^*$。  \n",
    "\n",
    "#### 2.3.3 Lexicon-based transcription  \n",
    "\n",
    "在基于字典的模式中，每个测试样本与词典 D 相关联。基本上，通过选择词典中具有方程 1 中定义的最高条件概率的序列来识别标签序列。然而，对于大型词典，例如 5 万个词的 Hunspell 拼写检查词典，对词典进行详尽的搜索是非常耗时的，即对词典中的所有序列计算方程 1，并选择概率最高的一个。为了解决这个问题，我们观察到，2.3.2 中描述的通过无词典转录预测的标签序列通常在编辑距离度量下接近于实际结果。这表示我们可以将搜索限制在最近邻候选目标 $N_{\\delta}(l')$，其中 $\\delta$ 是最大编辑距离，$l'$ 是在无词典模式下从 y 转录的序列：  \n",
    "\n",
    "$l^* = arg max_{l \\in N_{\\delta}(l')} p(l|y) \\tag{2}$\n",
    "\n",
    "可以使用 BK 树数据结构有效地找到候选目标 $N_{\\delta}(l')$，这是一种专门适用于离散度量空间的度量树。BK 树的搜索时间复杂度为 $O(log|D|)$，其中 |D| 是词典大小。因此，这个方案很容易扩展到非常大的词典。在我们的方法中，一个词典离线构造一个 BK 树。然后，我们使用树执行快速在线搜索，通过查找具有小于或等于 $\\delta$ 编辑距离来查询序列。\n",
    "\n",
    "### 2.4. Network Training  \n",
    "\n",
    "用 $X = \\{I_i,l_j\\}_i$ 表示训练数据集，其中 $I_i$ 是训练图像，$l_i$ 是真实标签序列。目标是最小化真实条件概率的负对数似然：  \n",
    "\n",
    "$O = - \\sum_{I_i,l_i \\in X} \\log p(l_i|y_i) \\tag{3}$\n",
    "\n",
    "$y_i$ 是循环层和卷积层从 $I_i$ 生成的序列。目标函数直接从图像和它的真实标签序列计算代价值。因此，网络可以在成对的图像和序列上进行端对端训练，去除了在训练图像中手动标记所有单独组件的过程。  \n",
    "\n",
    "用随机梯度下降（SGD）训练网络。梯度由反向传播算法计算。特别是，在转录层中，错误差异使用前向后向算法反向传播，如[15]中所述。在循环层中，应用反向传播时间（BPTT）来计算误差差异。  \n",
    "\n",
    "为了优化，我们使用 ADADELTA 自动计算每维度学习率。与传统动量方法相比，ADADELTA 不需要手动设置学习速率。更重要的是，我们发现使用 ADADELTA的优化比动量方法更快收敛。  \n",
    "\n",
    "## 3. Experiments  \n",
    "\n",
    "为了评估提出的 CRNN 模型的有效性，我们在场景文本识别和乐谱识别的标准基准数据集上进行了实验，这些都是具有挑战性的视觉任务。数据集和训练测试的设置见 3.1 小节，场景文本图像中 CRNN 的详细设置见 3.2 小节，综合比较的结果在 3.3 小节报告。为了进一步证明 CRNN 的泛化性，在 3.4 小节我们在乐谱识别任务上验证了提出的算法。  \n",
    "\n",
    "### 3.1. Datasets  \n",
    "\n",
    "对于场景文本识别的所有实验，我们使用 Jaderberg 等人发布的合成数据集（Synth）作为训练数据。数据集包含 8 百万训练图像及其对应的实际单词。这样的图像由合成文本引擎生成并且是非常现实的。我们的网络在合成数据上进行了一次训练，并在所有其它现实世界的测试数据集上进行了测试，而没有在其训练数据上进行任何微调。即使 CRNN 模型是在纯合成文本数据上训练，但它在标准文本识别基准数据集的真实图像上工作良好。  \n",
    "\n",
    "有四个流行的基准数据集用于场景文本识别的性能评估，即 ICDAR 2003（IC03），ICDAR 2013（IC13），IIIT 5k-word（IIIT5k）和 Street View Text (SVT)。  \n",
    "\n",
    "IC03 测试数据集包含 251 个具有标记文本边界框的场景图像。我们忽略包含非字母数字字符或少于三个字符的图像，并获得具有 860 个裁剪的文本图像的测试集。每张测试图像与由 Wang 等人定义的 50 词的词典相关联。通过组合所有的每张图像词汇构建完整的词典。此外，我们使用由 Hunspell 拼写检查字典中的单词组成的 5 万个词的词典。  \n",
    "\n",
    "IC13 测试数据集继承了 IC03 中的大部分数据。它包含 1015 个实际的裁剪单词图像。  \n",
    "\n",
    "IIIT5k 包含从互联网收集的 3000 张裁剪的词测试图像。每张图像关联一个 50 词的词典和一个 1000 词的词典。  \n",
    "\n",
    "SVT 测试数据集由从 Google 街景视图收集的 249 张街景图像组成。从它们中裁剪出了 647 张词图像。每张单词图像都有一个由 Wang 等人定义的 50 个词的词典。  \n",
    "\n",
    "### 3.2. Implementation Details  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/4.jpg?raw=true)  \n",
    "\n",
    "在实验中我们使用的网络配置总结在表 1 中。卷积层的架构是基于 VGG-VeryDeep 的架构。为了使其适用于识别英文文本，对其进行了调整。在第 3 和第4 个最大池化层中，我们采用 1×2 大小的矩形池化窗口而不是传统的平方形。这种调整产生宽度较大的特征图，因此具有更长的特征序列。例如，包含 10 个字符的图像通常为大小为 100×32，可以从其生成 25 帧的特征序列。这个长度超过了大多数英文单词的长度。最重要的是，矩形池窗口产生矩形感受野（如图 2 所示），这有助于识别一些具有窄形状的字符，例如 i 和 l。  \n",
    "\n",
    "不仅有卷积层，而且还有循环层。众所周知两者都难以训练。我们发现批归一化技术对于训练这种深度网络非常有用。分别在第 5 和第 6 卷积层之后插入两个批归一化层。使用批归一化层训练过程大大加快。  \n",
    "\n",
    "我们在 Torch7 框架内实现了网络，使用定制实现的 LSTM 单元（Torch7/CUDA），转录层（C++）和 BK 树数据结构（C++）。实验在具有 2.50 GHz Intel（R）Xeon E5-2609 CPU，64GB RAM 和 NVIDIA（R）Tesla(TM) K40 GPU 的工作站上进行。网络用 ADADELTA 训练，将参数ρ设置为 0.9。在训练期间，所有图像都被缩放为 100×32，以加快训练过程。训练过程大约需要 50 个小时才能达到收敛。测试图像缩放的高度为 32。宽度与高度成比例地缩放，但至少为 100 像素。平均测试时间为 0.16s/样本，在 IC03 上测得的，没有词典。近似词典搜索应用于 IC03 的 50k 词典，参数 $delta$ 设置为3。测试每个样本平均花费 0.53s。    \n",
    "\n",
    "### 3.3. Comparative Evaluation  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/6.jpg?raw=true)  \n",
    "\n",
    "提出的 CRNN 模型在上述四个公共数据集上获得的所有识别精度以及最近的包括基于深度模型的最新技术的识别精度，如表 2 所示。  \n",
    "\n",
    "在有约束词典的情况中，我们的方法始终优于大多数最新的方法，并且平均打败了[22]中提出的最佳文本阅读器。具体来说，与[22]相比，我们在 IIIT5k 和 SVT 上获得了卓越的性能，仅在 IC03 上通过“Full”词典实现了较低性能。请注意，[22]中的模型是在特定字典上训练的，即每个单词都与一个类标签相关联。与[22]不同，CRNN 不限于识别已知字典中的单词，并且能够处理随机字符串（例如电话号码），句子或其他诸如中文单词的脚本。因此，CRNN 的结果在所有测试数据集上都具有竞争力。  \n",
    "\n",
    "在无约束词典的情况下，我们的方法在 SVT 上仍取得了最佳性能，但在 IC03 和 IC13 上仍然落后于一些方法。注意，表 2 的“none”列中的空白表示这种方法不能应用于没有词典的识别，或者在无约束的情况下不能报告识别精度。我们的方法只使用具有单词级标签的合成文本作为训练数据，与PhotoOCR 非常不同，后者使用 790 万个具有字符级标注的真实单词图像进行训练。[22]中报告的最佳性能是在无约束词典的情况下，受益于它的大字典，然而，它不是前面提到的严格的无约束词典模型。在这个意义上，我们在无限制词典表中的结果仍然是有前途的。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/5.jpg?raw=true)  \n",
    "\n",
    "为了进一步了解与其它文本识别方法相比，所提出算法的优点，我们提供了在一些特性上的综合比较，这些特性名称为 E2E Train，Conv Ftrs，CharGT-Free，Unconstrained 和 Model Size，如表 3 所示。  \n",
    "\n",
    "E2E Train：这一列是为了显示某种文字阅读模型是否可以进行端到端的训练，无需任何预处理或经过几个分离的步骤，这表明这种方法对于训练是优雅且干净的。从表 3 可以看出，只有基于深度神经网络的模型，包括[22,21]以及CRNN具有这种性质。  \n",
    "\n",
    "Conv Ftrs：这一列用来表明一个方法是否使用从训练图像直接学习到的卷积特征或手动特征作为基本的表示。  \n",
    "\n",
    "CharGT-Free：这一列用来表明字符级标注对于训练模型是否是必要的。由于 CRNN 的输入和输出标签是序列，因此字符级标注是不必要的。  \n",
    "\n",
    "Unconstrained：这一列用来表明训练模型是否受限于一个特定的字典，是否不能处理字典之外的单词或随机序列。注意尽管最近通过标签嵌入[5, 14]和增强学习学习到的模型取得了非常有竞争力的性能，但它们受限于一个特定的字典。  \n",
    "\n",
    "Model Size：这一列报告了学习模型的存储空间。在 CRNN 中，所有的层有权重共享连接，不需要全连接层。因此，CRNN 的参数数量远小于 CNN 变体所得到的模型，与[22,21]相比，模型要小得多。我们的模型有 830 万个参数，只有 33MB RAM（每个参数使用 4 字节单精度浮点数），因此可以轻松地移植到移动设备上。  \n",
    "  \n",
    "表 3 详细列出了不同方法之间的差异，充分展示了 CRNN 与其它竞争方法的优势。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/7.jpg?raw=true)  \n",
    "\n",
    "另外，为了测试参数 $\\delta$ 的影响，我们在方程 2 中实验了 $\\delta$ 的不同值。在图 4 中，我们将识别精度绘制为 $\\delta$ 的函数。更大的 $\\delta$ 导致更多的候选目标，从而基于词典的转录更准确。另一方面，由于更长的 BK 树搜索时间，以及更大数量的候选序列用于测试，计算成本随着 $\\delta$ 的增大而增加。实际上，我们选择 $\\delta = 3$ 作为精度和速度之间的折衷。  \n",
    "\n",
    "### 3.4. Musical Score Recognition  \n",
    "\n",
    "乐谱通常由排列在五线谱的音符序列组成。识别图像中的乐谱被称为光学音乐识别（OMR）问题。以前的方法通常需要图像预处理（主要是二值化），五线谱检测和单个音符识别。我们将 OMR 作为序列识别问题，直接用 CRNN 从图像中预测音符的序列。为了简单起见，我们仅识别音调，忽略所有和弦，并假定所有乐谱具有相同的大调音阶（C 大调）。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/8.jpg?raw=true)  \n",
    "\n",
    "据我们所知，没有用于评估音调识别算法的公共数据集。为了准备 CRNN 所需的训练数据，我们从[2]中收集了 2650 张图像。每个图像中有一个包含 3 到20 个音符的乐谱片段。我们手动标记所有图像的真实标签序列（不是的音调序列）。收集到的图像通过旋转，缩放和用噪声损坏增强到了 265k 个训练样本，并用自然图像替换它们的背景。对于测试，我们创建了三个数据集：1）“纯净的”，其中包含从[2]收集的 260 张图像。实例如图 5.a 所示；2）“合成的”，使用“纯净的”创建的，使用了上述的增强策略。它包含 200 个样本，其中一些如图 5.b 所示；3）“现实世界”，其中包含用手机相机拍摄的音乐书籍中的 200 张图像。例子如图 5.c 所示。 \n",
    "\n",
    "由于我们的训练数据有限，因此我们使用简化的 CRNN 配置来减少模型容量。与表1 中指定的配置不同，我们移除了第 4 和第 6 卷积层，将 2 层双向 LSTM 替换为 2 层单向 LSTM。网络对图像对和对应的标签序列进行训练。使用两种方法来评估识别性能：1）片段准确度，即正确识别的乐谱片段的百分比；2）平均编辑距离，即预测音调序列与真实值之间的平均编辑距离。为了比较，我们评估了两种商用 OMR 引擎，即 Capella Scan 和 PhotoScore。    \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/9.jpg?raw=true)  \n",
    "\n",
    "表 4 总结了结果。CRNN 大大优于两个商业系统。Capella Scan 和 PhotoScore 系统在干净的数据集上表现相当不错，但是它们的性能在合成和现实世界数据方面显著下降。主要原因是它们依赖于强大的二值化来检五线谱和音符，但是由于光线不良，噪音破坏和杂乱的背景，二值化步骤经常会在合成数据和现实数据上失败。另一方面，CRNN 使用对噪声和扭曲具有鲁棒性的卷积特征。此外，CRNN 中的循环层可以利用乐谱中的上下文信息。每个音符不仅自身被识别，而且被附近的音符识别。因此，通过将一些音符与附近的音符进行比较可以识别它们，例如对比他们的垂直位置。  \n",
    "\n",
    "结果显示了 CRNN 的泛化性，因为它可以很容易地应用于其它的基于图像的序列识别问题，需要极少的领域知识。与 Capella Scan 和 PhotoScore 相比，我们的基于 CRNN 的系统仍然是初步的，并且缺少许多功能。但它为 OMR 提供了一个新的方案，并且在音高识别方面表现出有前途的能力。  \n",
    "\n",
    "## 4. Conclusion  \n",
    "\n",
    "在本文中，我们提出了一种新颖的神经网络架构，称为卷积循环神经网络（CRNN），其集成了卷积神经网络（CNN）和循环神经网络（RNN）的优点。CRNN 能够获取不同尺寸的输入图像，并产生不同长度的预测。它直接在粗粒度的标签（例如单词）上运行，在训练阶段不需要详细标注每一个单独的元素（例如字符）。此外，由于 CRNN 放弃了传统神经网络中使用的全连接层，因此得到了更加紧凑和高效的模型。所有这些属性使得 CRNN 成为一种基于图像序列识别的极好方法。  \n",
    "\n",
    "在场景文本识别基准数据集上的实验表明，与传统方法以及其它基于 CNN 和 RNN 的算法相比，CRNN 实现了优异或极具竞争力的性能。这证实了所提出的算法的优点。此外，CRNN 在光学音乐识别（OMR）的基准数据集上显著优于其它的竞争者，这验证了 CRNN 的泛化性。  \n",
    "\n",
    "实际上，CRNN 是一个通用框架，因此可以应用于其它的涉及图像序列预测的领域和问题（如汉字识别）。进一步加快 CRNN，使其在现实应用中更加实用，是未来值得探索的另一个方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STN-OCR: A single Neural Network for Text Detection and Text Recognition  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "在自然场景图像中检测和识别文本是一项具有挑战性但尚未完全解决的任务。近年来，已经提出了几个试图解决两个子任务（文本检测和文本识别）中的至少一个的新系统。在本文中，我们介绍 STN-OCR，这是半监督神经网络用于场景文本识别的一个步骤，可以进行端到端优化。与大多数由多个深度神经网络和多个预处理步骤组成的现有作品相比，我们建议使用一个深度神经网络，通过半监督方式学习检测和识别自然图像中的文本。STN-OCR 是一个集成并共同学习空间变换网络的网络，可以学习检测图像中的文本区域，还可以使用文本识别网络来识别文本区域并识别它们的文本内容。我们研究我们的模型如何在一系列不同的任务（检测和识别字符以及文本行）上发挥作用。公共基准数据集上的实验结果表明，我们的模型能够处理各种不同的任务，而整个网络结构没有实质性变化。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "文字在我们的日常生活中无处不在。文字可以在文件，道路标志，广告牌和其他物体（如汽车或电话）上找到。从自然场景图像自动检测和阅读文本是系统的重要组成部分，可用于几项具有挑战性的任务，如基于图像的机器翻译，自动驾驶汽车或图像/视频索引。近年来，在自然场景中检测文本和识别文本的任务已经引起了计算机视觉和文档分析界的极大兴趣。此外，近期在计算机视觉领域取得的突破能够创造出比以前更好的场景文本检测和识别系统。虽然光学字符识别（OCR）的问题可以看作是解决了印刷文档文本，但检测和识别自然场景图像中的文本仍然具有挑战性。包含自然场景的图像表现出明显的光照变化，视角扭曲，图像质量，文本字体，不同背景等。  \n",
    "\n",
    "现有的大部分研究工作都开发了端到端的场景文本识别系统，这些系统由复杂的两步流程组成，第一步是检测图像中的文本区域，第二步是识别该区域的文本内容。大部分现有作品只专注于这两个步骤中的一个。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/2/1.jpg?raw=true)  \n",
    "\n",
    "在本文中，我们提出一个由单个深度神经网络（DNN）组成的解决方案，该网络可以以半监督的方式学习检测和识别文本。这与现有的作品是相反的，在这些作品中，文本检测和文本识别系统是以完全监督的方式单独训练的。最近的工作表明，卷积神经网络（CNN）能够学习如何解决复杂的多任务问题，同时接受端到端的训练。我们的动机是使用 CNN 的这些功能，并创建一个端到端的场景文本识别系统，通过将手头的任务分成较小的子任务并相互独立地解决这些子任务，从而表现得更像人。为了实现这种行为，我们学习了一个能够将输入图像划分为子任务（单个字符，单词或者甚至是文本行）的单个 DNN，并相互独立地解决这些子任务。这是通过联合学习使用循环空间转换器作为注意机制的定位网络和文本识别网络实现的（参见图 1 以获得系统的示意图）。在此设置中，网络仅接收图像和包含在该图像中的文本的标签作为输入。文本的定位由网络本身学习，使得这种方法是半监督的。  \n",
    "\n",
    "我们的贡献如下：（1）我们提出的系统是通过集成空间变换器网络来解决端到端场景文本识别的一个步骤。（2）我们以半监督的方式端对端地训练我们提出的系统。（3）我们证明我们的方法能够在一系列标准场景文本检测和识别基准上达到最先进/最具竞争力的性能。（4）我们向研究界提供我们的 code 和训练好的模型。  \n",
    "\n",
    "本文的结构如下：在第 2 节中，我们概述了与我们有关的其他研究人员的工作。第 3 节详细介绍了我们提出的系统，并提供了如何训练这种系统的最佳实践。我们在第 4 节展示并讨论标准基准数据集的结果，并在第 5 节总结我们的发现。  \n",
    "\n",
    "## 2. Related Work  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/2/2.jpg?raw=true)  \n",
    "\n",
    "多年来，已经开发和发布了不同场景文本检测和识别的方法。几乎所有的系统都使用两步过程来执行场景文本的端到端识别。第一步是检测文本区域并从输入图像中提取这些区域。第二步是识别文本内容并返回这些提取的文本区域的文本字符串。  \n",
    "\n",
    "还可以将这些方法分为三大类：（1）依靠手工特征和人类知识进行文本检测和文本识别的系统。（2）使用深度学习方法的系统与手工制作的特征一起使用，或者对于两个步骤中的每一个使用两个不同的深度网络。（3）不是由两步法组成的系统，而是使用单个深度神经网络进行文本检测和识别。我们将在下面的每个类别中讨论一些这些系统。  \n",
    "\n",
    "**Hand Crafted Features** 在开始的基础上，手工制作的特征和人类的知识已被用于执行文本检测。这些系统使用 MSER，Stroke Width Transforms  或 HOGFeatures 等特征来识别文本区域并将其提供给系统的文本识别阶段。在文本识别阶段使用滑动窗口分类器和 SVMs 或使用 HOG 特征的 k-最近邻分类器的集合。所有这些方法都使用手工制作的特征，这些特征具有各种各样的超参数，需要专业知识才能正确调整以获得最佳结果。  \n",
    "\n",
    "**深度学习方法** 更新的方法在端到端的识别系统中使用 DNNs 来进行特征提取。G'omez 和 Karatzas 提出了一种特定文本的选择性搜索算法，与 DNN 一起可以用来检测自然场景图像中的（失真）文本区域。Gupta 等人提出了基于 YOLO 架构的文本检测模型，该模型使用全卷积深度神经网络来识别文本区域。这些方法识别的文本区域可以用作基于执行文本识别的 DNN 的其他系统的输入。  \n",
    "\n",
    "Bissacco等人提出了一个完整的端到端体系结构，使用手工制作的特征执行文本检测。识别的文本区域被二值化，然后用作深度全连接的神经网络的输入，该神经网络将每个找到的字符独立分类。Jaderberg 等人提出了几个使用深度神经网络进行文本检测和文本识别的系统。Jaderberg 等人提出了一种滑动窗口文本检测方法，以多种分辨率在图像上滑动卷积文本检测模型。文本识别阶段使用单个字符 CNN，该字符滑过已识别的文本区域。此 CNN 与用于文本检测的 CNN 共享权重。Jaderberg 等人建议使用带有用于文本检测的额外边界框回归 CNN 的区域提议网络和以整个文本区域作为输入并且跨越预先定义的单词词典执行分类的 CNN，使得该方法仅适用于给定的语言。  \n",
    "\n",
    "Goodfellow 等人提出了一个房屋号码的文本识别系统，Jaderberg 等人进行了优化，用于无约束的文本识别。该系统使用一个 CNN，它将完整提取的文本区域作为输入，并提供该文本区域中包含的文本。这是通过给定单词中每个可能的字符都有一个独立的分类器来实现的。基于这个想法，He等人和Shi 等人提出了文本识别系统，其将提取的文本区域中的字符识别视为序列识别问题。He 等人使用一种朴素的滑动窗口方法创建文本区域的切片，用作文本识别 CNN 的输入。由文本识别 CNN 产生的特征被用作预测字符序列的递归神经网络（RNN）的输入。在我们关于纯场景文本识别的实验中（参见第 4.3 节以获取更多信息），我们使用了类似的方法，但是我们的系统使用了更复杂的滑动窗口方法，其中滑动窗口的选择由网络自动学习。Shi 等人利用一个 CNN，它使用完整的文本区域作为输入，并产生一个特征向量序列，这些特征向量被馈送到预测提取的文本区域中的字符序列的 RNN。该方法基于文本区域的宽度生成固定数量的特征向量。这意味着对于仅包含几个字符但与具有足够多字符的文本区域相同的宽度的文本区域，该方法将产生用作 RNN 输入的相同数量的特征向量。在我们的纯文本识别实验中，我们利用我们的方法的优势来学习提取文本区域中最重要的信息，因此只需要生成尽可能多的特征向量。 Shi 等人通过首先增加额外的步骤来改善他们的方法，该步骤利用空间转换网络的整流能力来纠正提取的文本行。其次，他们在他们的网络中增加了一种软注意力机制，有助于在输入图像中产生字符序列。在他们的工作中 Shi 等人利用空间转换器作为额外的预处理步骤，使识别网络更容易识别图像中的文本。在我们的系统中，我们使用空间转换器作为以半监督方式检测文本的核心构建块。  \n",
    "\n",
    "**端到端的可训练方法** 所提出的系统总是使用两步法来检测和识别来自场景文本图像的文本。尽管最近的方法利用深度神经网络，但他们仍然在任一步骤中或在两步骤的结果融合在一起的地方使用大量手工制作的知识。Smith 等人提出了一种端到端的可训练系统，该系统能够使用单个 DNN 检测和识别法国街道名称标志上的文本。与我们的系统不同，系统不提供图像中文本的位置，只能提取文本内容。此外，我们的方法中使用的注意力机制显示出更像人类的行为，因为它是依次定位并识别来自给定图像的文本。  \n",
    "\n",
    "## 3. Proposed System  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/2/3.jpg?raw=true)  \n",
    "\n",
    "人类会按顺序找到并阅读文本。第一个操作是将注意力放在一行文本上，顺序读取每个字符，然后再处理下一行文本。目前大多数用于场景文本识别的端到端系统都没有这种行为。这些系统试图通过一次从图像中提取所有信息来解决问题。我们的系统首先尝试顺序关注图像中的不同文本区域，然后识别每个文本区域的文本内容。为此，我们创建了一个简单的 DNN，包含两个阶段：（1）文本检测（2）文本识别。在本节中，我们将介绍文本检测阶段所使用的注意概念和所提议系统的总体结构。我们还报告了成功训练这种系统的最佳实践。  \n",
    "\n",
    "### 3.1. Detecting Text with Spatial Transformers  \n",
    "\n",
    "由 Jaderberg 等人提出的空间转换器是用于 DNN 的可微模块，其采用输入特征图 I 并将空间变换应用于该特征图，从而产生输出特征图 O。这样的空间变换模块由三部分组成。第一部分是一个定位网络，它计算一个函数 $f_{loc}$，它预测要应用的空间变换的参数 $\\theta$。在第二部分中使用这些预测参数来创建采样网格，该采样网格定义输入特征映射的哪些特征应映射到输出特征映射。第三部分是一种可微插值方法，它将生成的采样网格生成为空间变换的输出特征映射 O。我们将在下面的段落中简要描述每个组件。  \n",
    "\n",
    "**定位网络** 定位网络采用输入特征映射 $I \\in R^{C \\times H \\times W}$，C 通道，高度 H和宽度 W，并输出应用变换的参数 $\\theta$。在我们的系统中，我们使用定位网络（$f_{loc}$）来预测 N 个二维仿射变换矩阵 $A_{\\theta}^n$，其中 $n \\in \\{0,...,N-1\\}$：  \n",
    "\n",
    "$f_{loc}(I) = A_{\\theta}^n = \\begin{bmatrix} \\theta_1^n & \\theta_2^n & \\theta_3^n \\\\ \\theta_4^n & \\theta_5^n & \\theta_6^n \\\\ \\end{bmatrix} \\tag{1}$\n",
    "\n",
    "因此 N 是定位网络定位的字符，词语或文本线的数量。以这种方式预测的仿射变换矩阵允许网络对输入图像应用平移，旋转，缩放和倾斜，因此网络学习产生变换参数，该变换参数可以放大将要从输入图像中提取的字符，单词或文本行图片。  \n",
    "\n",
    "在我们的系统中，通过使用前馈 CNN 和 RNN 一起产生 N 个变换矩阵 $A_{\\theta}^n$。对于 RNN 的每个时间步骤，使用隐藏状态 $h_n$ 来计算 N 个变换矩阵中的每一个：  \n",
    "\n",
    "$c = f_{loc}^{conv}(I) \\tag{2}$\n",
    "$h_n = f_{loc}^{rnn}(c,h_{n-1}) \\tag{3}$\n",
    "$A_{\\theta}^n = g_{loc}(h_n) \\tag{4}$\n",
    "\n",
    "其中 $g_{loc}$ 是另一个前馈网络，并且每个变换矩阵 $A_{\\theta}^n$ 是以全局提取的卷积特征（$f_{loc}^{conv}$）和以先前执行的时间步骤的隐藏状态为条件计算的。  \n",
    "\n",
    "我们使用的定位网络中的 CNN 是 He等人著名的 ResNet 的变体。我们使用 ResNet 的一个变种，因为我们发现，与使用 VGGNet 等其他网络结构的实验相比，通过这种网络结构，我们的系统学得更快，更成功。我们认为这是由于 ResNet 的残差连接有助于保持强梯度下降到最初的卷积层的事实。除了结构之外，我们还对所有实验使用了批量标准化。定位网络中使用的 RNN 是双向长短时记忆（BLSTM）单元。这个 BLSTM 用于生成隐藏状态 $h_n$，而隐藏状态 $h_n$ 又用于预测仿射变换矩阵。我们在第 4 节中报告的所有实验中使用了相同的网络结构。图 3 提供了该网络的结构概述。  \n",
    "\n",
    "**网格生成器** 网格生成器使用具有高度 $H_o$ 和宽度 $W_o$ 的坐标 $y_{h_o}, x_{w_o}$和仿射变换矩阵 $A_{\\theta}^n$ 的规则间隔网格 $G_o$ 来生成输入 N 个规则网格 $G_i^n$,将特征图 I 的坐标映射为 $u_i^n, v_j^n$，其中 $i \\in H_o, j \\in W_o$：  \n",
    "\n",
    "$ \\begin{pmatrix} u_i^n \\\\ v_j^n \\\\ \\end{pmatrix} = A_{\\theta}^n \\begin{pmatrix} x_{w_o} \\\\ y_{h_o}\\\\ 1 \\end{pmatrix} = \\begin{bmatrix} \\theta_1^n & \\theta_2^n & \\theta_3^n \\\\ \\theta_4^n & \\theta_5^n & \\theta_6^n \\\\ \\end{bmatrix} \\begin{pmatrix} x_{w_o} \\\\ y_{h_o}\\\\ 1 \\end{pmatrix} \\tag{5}$\n",
    "\n",
    "在推理过程中，我们可以提取 N 个结果网格 $G_i^n$，其中包含定位网络找到的文本区域的边界框。高度 Ho 和宽度 Wo 可以自由选择，如果它们低于输入特征图 I 的高度 H 或宽度 W，则网格生成器生成网格，该网格在下一步中执行下采样操作。  \n",
    "\n",
    "**图像采样** 网格生成器生成的 N 个采样网格 $G_i^n$ 现在用于对每个 $n \\in N$ 在对应的坐标 $u_i^n, v_j^n$ 上采样特征图 I。当然，这些点在输入特征图中并不总是与离散网格完全对齐的。因此，我们使用双线性采样，通过对最近邻居的值进行双线性内插来提取给定坐标处的值。由此，我们定义 给定位置 i,j 上 N 个输出特征图 $O^n$ 的值，其中 $i \\in H_o, j \\in W_o$：  \n",
    "\n",
    "$O_{i,j}^n = \\sum_h^{H_o} \\sum_w^{W_o} I_{hw} max (0, 1-|u_i^n -h|) max (0,1 - |v_j^n - w|)$\n",
    "\n",
    "这种双线性采样是（次）可微分的，因此可以通过使用标准反向传播将误差梯度传播到定位网络。  \n",
    "\n",
    "定位网络，网格生成器和图像采样器的组合形成空间变换器，并且通常可以用于 DNN 的每个部分。在我们的系统中，我们使用空间变换器作为我们网络的第一步。定位网络接收输入图像作为输入特征映射，并产生一组仿射变换矩阵，由网格生成器用来计算应由双线性采样操作采样的像素的位置。  \n",
    "\n",
    "### 3.2. Text Recognition Stage  \n",
    "\n",
    "文本检测阶段的图像采样器产生从原始输入图像提取的一组 N 个区域。文本识别阶段（这个阶段的结构概述可以在图 3 中找到）使用这 N 个不同区域中的每一个并且彼此独立地处理它们。N 个不同区域由 CNN 处理。这个 CNN 也基于 ResNet 架构，因为我们发现如果我们使用 ResNet 架构的变体来实现我们的识别网络，我们能取得好的结果。我们认为在识别阶段使用 ResNet 比在检测阶段更重要，因为检测阶段需要从识别阶段接收强梯度以成功更新定位网络的权重。识别阶段的 CNN 预测标签空间 $L_{\\epsilon}$ 上的概率分布 $\\hat y$，其中 $L_{\\epsilon} = L \\bigcup \\{\\epsilon\\}$，其中 L = {0-9a-z}，$\\epsilon$ 代表空白标签。根据任务的不同，这种概率分布或者由固定数量的 T softmax 分类器生成，其中每个 softmax 分类器用于预测给定单词的一个字符：  \n",
    "\n",
    "$ \\begin{align}\n",
    "x^n = O^n \\tag{7} \\\\\n",
    "\\hat y_t^n = softmax(f_{rec}(x^n)) \\tag{8} \\\\\n",
    "\\hat y^n = \\sum_{t=1}^T \\hat y_t^n \\tag{9} \\\\\n",
    "\\end{align} $\n",
    "\n",
    "其中 $f_{rec}(x)$ 是在采样输入 x 上应用卷积特征提取器的结果。  \n",
    "\n",
    "另一种可能性是使用 Connectionist Temporal Classification（CTC）对网络进行训练，并通过将 $\\hat y$ 设置为最可能的标记路径 $\\pi$ 来检索最可能的标记，其由下式给出：  \n",
    "\n",
    "$ \\begin{align}\n",
    "p(\\pi|x^n) = \\quad_{t=1}^{T} \\hat y_{\\pi t}^n, \\ \\pi \\in L_{\\epsilon}^T \\tag{10} \\\\\n",
    "\\hat y_t^n = argmax p(\\pi|x^n) \\tag{11} \\\\\n",
    "\\hat y^n = B (\\sum_{t=1}^T \\hat y_t^n) \\tag{12} \\\\\n",
    "\\end{align} $\n",
    "\n",
    "其中 $L_{\\epsilon}^T$ 是所有长度为 T 的标签的集合，$p(\\pi|x^n)$ 是 DNN 预测路径 $\\pi \\in L_{\\epsilon}^T$ 的概率。B 是除去所有预测空白标签和所有重复标签（例如 B(-IC-CC-V)= B(II-CCC-C-V-) = ICCV）的函数。  \n",
    "\n",
    "### 3.3. Model Training  \n",
    "\n",
    "用于训练模型的训练集 X 包括一组输入图像 I 和一组用于每个输入图像的文本标签 $L_I$。我们不使用任何标签来训练文本检测阶段。这个阶段仅通过计算预测标签和文本标签的交叉熵损失或 CTC 损失获得的误差梯度来学习检测文本区域。实验中，我们发现，从头开始训练时，检测并识别两条以上文本行的网络不会收敛。解决这个问题的方法是在难度逐渐增加的情况下执行一系列预训练步骤。此外，我们发现选择用于训练网络的优化算法对网络的收敛性有很大影响。我们发现使用随机梯度下降（SGD）对网络进行简单任务训练是有益的，Adam 利用更多的文本线对图像上已经预先训练好的网络进行微调。我们认为 SGD 在预训练期间表现更好，因为学习率 η 在较长的时间内保持不变，这使得文本检测阶段能够探索输入图像并更好地查找文本区域。随着学习速度的降低，检测阶段的更新变得更小，并且文本检测阶段（理想地）稳定在已经找到的文本区域上。同时，文本识别网络可以开始使用提取的文本区域并学习识别该区域中的文本。在使用 SGD 对网络进行训练时，需要注意的是，选择过高的学习率会导致模型早期出现分歧。我们发现，使用 $1^{-5}$ 和 $1^{-7}$ 之间的初始学习率几乎适用于所有情况，除非网络应该进行微调。在这里，我们发现使用 Adam 是更可靠的选择，因为 Adam 以自适应的方式选择每个参数的学习速率。  \n",
    "\n",
    "## 4. Experiments  \n",
    "\n",
    "在本节中，我们将在几个标准场景文本检测/识别数据集上评估我们提出的网络体系结构。我们给出三个不同数据集的实验结果，其中每个数据集的任务难度增加。我们的实验首先从 SVHN 数据集开始，用来证明我们的概念是可行的。我们进行实验的第二种数据集是用于聚焦的场景文本识别的数据集，在这里探索了我们的模型的性能，当涉及到查找和识别单个字符时。我们欣赏的第三个数据集是法国街道名称符号（FSNS）数据集，这是我们使用的最具挑战性的数据集，因为此数据集包含大量不规则的低分辨率文本行。通过介绍我们的实验设置开始本节。然后，将介绍上述每个数据集的实验结果和特征。  \n",
    "\n",
    "### 4.1. Experimental Setup  \n",
    "\n",
    "**定位网络** 每个实验中使用的定位网络都基于 ResNet 架构。网络的输入是文本应该被定位并且被识别的图像。在第一个残差块之前，网络执行 3×3 卷积，然后执行 2×2 平均池化，步长为 2。在这些层之后，使用了三个具有两个 3×3 卷积的残差块，每个卷积都采用批量归一化。卷积滤波器的数量分别为 32，48 和 48，并且 ReLU 被用作每个卷积层的激活函数。在第二个残差块之后跟随一个带有步幅 2 的 2×2 最大池化。最后一个残差块后面跟着一个 5×5 的平均池化层，这个层跟着一个 256 个隐藏单元的 BLSTM。对于 BLSTM 的每个时间步骤，都有一个带有 6 个隐藏单元的全连接层。该层预测仿射变换矩阵，用于生成双线性插值的采样网格。由于场景文本的修正超出了本文的范围，因此我们通过将相关参数设置为0来禁用仿射变换矩阵中的偏斜和旋转。我们将在以后的工作中讨论空间变换器对场景文本检测的纠正能力。  \n",
    "\n",
    "**识别网络** 识别网络的输入是来自原始输入图像的 N 个 crops，代表定位网络找到的文本区域。识别网络具有与定位网络相同的结构，但卷积滤波器的数量更多。卷积滤波器的数量分别是 32，64 和 128。取决于实验，我们使用[6]和[17]中使用的 T 个独立的 softmax 分类器的集合，其中 T 是单词可能具有的最大长度，或者我们使用[11]中所用的具有最佳路径解码的 CTC。  \n",
    "\n",
    "**实现** 我们使用 MXNet 实现了所有的实验。我们在一台拥有英特尔®酷睿™i7-6900K CPU，64 GB RAM 和 4颗TITAN X（Pascal）GPU 的工作站上进行了所有实验。  \n",
    "\n",
    "### 4.2. Experiments on the SVHN dataset  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/2/8.jpg?raw=true)  \n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/2/4.jpg?raw=true) \n",
    "\n",
    "通过我们在 SVHN 数据集上的第一次实验，我们想证明我们的概念是有效的，并且可以用于现实世界的数据。因此，我们首先在 SVHN 图像上进行了与[16]中的实验相似的实验，每个图像中都有一个门牌号码，以数字为中心并且还包含背景噪音。表 1 显示我们能够达到有竞争力的识别精度。  \n",
    "\n",
    "基于这个实验，我们想要确定我们的模型是否能够检测排列成规则网格或放置在图像中随机位置的不同文本行。在图 4 中，我们展示了来自为两个目的构建的数据集中的样本，我们将其用于基于 SVHN 数据的其他实验。我们发现，我们的网络在寻找和识别以规则网格排列的房屋号码的任务方面表现良好。我们在对这些数据进行训练期间所做的一个有趣的观察是，当我们进行了两个训练步骤时，我们能够取得最佳成果。第一步是从头开始训练整个模型（所有权重都是随机初始化的），然后再次用相同的数据训练模型，但是这次定位网络使用从上一次训练获得的权重进行初始化，识别网络用随机权重初始化。该策略导致定位网络更好的定位结果，并因此改善识别结果。  \n",
    "\n",
    "在我们对由我们创建的第二个数据集进行的实验中，我们发现从零开始训练模型是不可能的，它可以找到并识别散布在整个图像上的两个以上的文本行。 通过首先在较容易的任务上训练模型（几条文本线，接近图像中心的文本线），然后逐渐增加任务的难度，可以训练这样的网络。在补充材料中，我们提供了简短的视频剪辑，显示了网络如何探索图像，同时学习检测各种不同实验的文本。  \n",
    "\n",
    "### 4.3. Experiments on Robust Reading Datasets  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/2/5.jpg?raw=true) \n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/2/6.jpg?raw=true) \n",
    "\n",
    "在我们的下一个实验中，我们使用了从输入图像中裁剪文本区域的数据集。我们想看看我们的文本定位网络是否可以用作智能滑动窗口生成器，该生成器采用裁剪文本区域中文本的不规则性。因此，我们使用 CTC 在我们自己的数据生成器生成的合成裁剪字图像数据集上训练了我们的识别模型，该数据生成器与 Jaderberg 等人引入的数据生成器类似。在表 2 中，我们报告了我们的模型在 ICDAR 2013 robust reading，街景文本（SVT）和 IIIT5K 基准数据集上的识别结果。为了对 ICDAR 2013 和 SVT 数据集进行评估，我们过滤了所有包含非字母数字字符的图像，并丢弃了[28,32]中所做的小于 3 个字符的所有图像。我们通过使用标准 hunspell english（en-US）字典后处理预测来获得最终结果。总体而言，我们发现我们的模型在 ICDAR 2013 和 IIIT5K 数据集上获得了无约束识别模型的最先进性能，并在 SVT 数据集上实现了竞争性能。在图 5 中，我们显示我们的模型学习遵循单个文本区域的斜率，证明我们的模型以智能方式生成滑动窗口。  \n",
    "\n",
    "### 4.4. Preliminary Experiments on the FSNS dataset  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/2/7.jpg?raw=true) \n",
    "\n",
    "根据我们提高网络解决任务难度的方案，我们选择了 Smith 等人的法国街道名称标志（FSNS）数据集，成为我们的第三个数据集来进行实验。我们在这里报告的结果是初步的，仅表明我们的网络架构也适用于这类数据，尽管它尚未达到最新的结果。FSNS 数据集包含从 Google Streetview 中提取的法国街道名称标志的图像。这个数据集对于我们的方法来说是最具挑战性的数据集，因为它（1）包含嵌入自然场景中具有不同长度的多行文本，具有分散的背景和（2）包含大量不包含街道全名的图像。  \n",
    "\n",
    "在我们对该数据集进行的第一次实验中，我们发现我们的模型在提供的 groundtruth 上训练时不能收敛。我们认为这是因为原始数据集的标签不包含任何关于哪些文字可以在哪个文本行中找到的提示。因此，我们改变了方法，并开始尝试寻找单个单词而不是多个单词的文本行。我们相应地修改了 groundtruth，并将包含最多三个词的所有图像用于我们的实验，这为我们留下了大约 80% 的来自数据集的原始数据。图 6 显示了 FSNS 数据集中的一些示例，我们的模型正确定位了单个单词并正确识别了这些单词。使用这种方法，我们能够在测试集上获得 97% 的合理良好的字符识别准确性，但只有71.8% 的字词准确率。字符识别率和字识别率的差异是由于我们为这个任务训练的模型使用独立的 softmax 分类器来处理单词中的每个字符。具有 97% 的字符识别准确度意味着至少一个分类器出错并且因此增加序列错误的可能性很高。  \n",
    "\n",
    "## 5. Conclusion  \n",
    "\n",
    "在本文中，我们提出了一个系统，该系统可以看作是仅使用单个多任务深度神经网络来解决端到端场景文本识别的一个步骤。我们以半监督的方式训练了我们模型的文本检测组件，并且能够提取文本检测组件的定位结果。我们的系统的网络架构很简单，但要训练这个系统并不容易，因为成功的训练需要在模型能够收敛到真正的任务之前对更简单的子任务进行大量的预训练。我们还表明，可以使用相同的网络架构在一系列用于场景文本检测/识别的公共基准数据集上获得竞争性或最新的结果。  \n",
    "\n",
    "在当前状态下，我们注意到我们的模型不能完全检测图像中任意位置的文本，正如我们在 FSNS 数据集实验中看到的那样。目前，我们的模型还受限于可以一次检测到的固定数量的最大文本行/字符，在我们未来的工作中，我们希望以一种方式重新设计网络，使网络能够确定文本行的数量 一个图像本身。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data for Text Localisation in Natural Images  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "在本文中，我们介绍一种用于自然图像中文本检测的新方法。该方法包括两个贡献：首先，一个快速和可扩展的引擎，用于生成杂乱的文本合成图像。该引擎以自然的方式将合成文本叠加到现有背景图像上。其次，我们使用合成图像来训练全卷积回归网络（FCRN），该网络可高效地在图像中的所有位置和多个比例上执行文本检测和边界框回归。我们讨论了 FCRN 与最近推出的 YOLO 检测器以及其他基于深度学习的端到端物体检测系统的关系。由此产生的检测网络大大超出了自然图像中文本检测的当前方法，在标准 ICDAR 2013 基准测试中实现了 84.2% 的 F 值。此外，它可以在 GPU 上每秒处理 15 个图像。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "文本识别，即在自然场景中阅读文本的能力，在计算机视觉的以人类为中心的应用中是非常令人满意的特征。最先进的系统通过结合两个简单而强大的见解，实现了高文本识别性能。首先是复杂的识别流程，通过明确地结合识别和检测单个字符来识别文本，可以被非常强大的分类器所替代，这些分类器可以直接将图像 patches 映射到单词。第二，这些强大的分类器可以通过综合生成所需的训练数据来学习。  \n",
    "\n",
    "虽然文献[20]成功地解决了在给定包含单词的图像 patches 的情况下识别文本的问题，但是获取这些 patches 的过程仍然不理想。该流程结合了 HoG ，EdgeBoxes 和 Aggregate Channel Features 等通用功能，并且仅在后期阶段才引入文本特定（CNN）特征，其中最终将 patches 识别为特定词。由于两个原因，这种情况非常不可取。首先，检测流程的性能成为文本识别的新瓶颈：在[20]中正确裁剪的词语的识别准确率为98%，而端到端的文本识别 F 分数仅为 69%，主要是由于不正确的或错过了字区域建议。其次，流程缓慢而不雅。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/3/1.jpg?raw=true)  \n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/3/2.jpg?raw=true)  \n",
    "\n",
    "在本文中，我们提出类似于[20]的文本检测问题的补充改进。我们做出两项重要贡献。首先，我们提出了一种用于生成合成图像的新方法，该方法自然地融合了现有自然场景中的文本，使用现成的深度学习和分割技术将文本与背景图像的几何图形对齐，并尊重场景边界。我们使用这种方法**在杂乱的条件下自动生成一个新的综合文本数据集**（图1（上）和第 2 节）。该数据集称为 Wild 中的 SynthText（图2），适用于训练高性能场景文本检测器。与现有合成文本数据集（如[20]之一）的主要区别在于，它们仅包含字级图像区域，不适合训练检测器。  \n",
    "\n",
    "第二个贡献是**深层文本检测架构**，既精确又高效（图1（下）和第 3 节）。我们称之为全卷积回归网络。与用于图像分割的全卷积网络（FCN）模型类似，它在每个图像位置密集地执行预测。然而，与 FCN 不同的是，预测不仅仅是一个分类标签（文本/不是文本），而是包围以该位置为中心的单词的边界框的参数。后者的想法是借鉴了 Redmon 等人的 You Only Only Once（YOLO）技术，但卷积回归机显着提升了性能。  \n",
    "\n",
    "新的数据和检测器在标准基准数据集（第4部分）上实现了最先进的文本检测性能，同时在测试时间比传统文本检测器快一个数量级（GPU 上每秒高达 15 幅图像）。我们还证明了在数据集中逼真性的重要性，通过显示如果检测器在图像上用综合插入的词汇进行训练而不考虑场景布局，则检测性能明显较差。 最后，由于更精确的检测步骤，一旦新检测器换入现有流程中，端到端的单词识别也得到改进。我们的研究结果在第 5 部分中总结。  \n",
    "\n",
    "### 1.1. Related Work  \n",
    "\n",
    "**使用 CNN 进行对象检测** 我们的文本检测网络主要基于 Long 等人的全卷积网络和 Redmon 等人的基于 YOLO 图像网格的边界框回归网络。YOLO 是使用 CNN 特征进行对象类别检测的一个广泛工作的一部分，可追溯到 Girshick 等人的 Region-CNN（R-CNN）框架区域提案和 CNN 特征的组合。R-CNN 框架有三个广泛的阶段 - （1）生成对象提议，（2）提取每个提议的 CNN 特征映射，以及（3）通过特定分类的 SVM 过滤提议。Jaderberg 等人的文本识别方法也使用类似的流程进行检测。Girshick 等人将独立的提取每个区域的特征图确定为瓶颈。在 Fast R-CNN中，他们通过计算一次 CNN 特征并为每个提案汇集它们，从而在 R-CNN 上获得 100 倍的加速；他们还将 R-CNN 的最后两个阶段简化为单个多任务学习问题。这项工作揭示了区域建议阶段是新的瓶颈。Lenc 等人完全放弃区域建议阶段，并使用 PASCAL VOC 数据上通过 K 均值聚类学习的恒定区域集合。Ren 等人也从一套固定的提案开始，但在检测之前通过使用区域提案网络进行改进，该提案网络与后来的检测网络分享权重并简化多阶段 R-CNN 框架。  \n",
    "\n",
    "**合成数据** 合成数据集提供详细的真实注释，并且是手动标注图像的便宜且可扩展的替代方案。他们已被广泛用于学习大型 CNN 模型 - Wang 等人和Jaderberg 等人使用合成文本图像来训练词语图像识别网络；Dosovitskiy 等人使用浮椅渲染来训练密集的光流回归网络。详细的合成数据也被用于学习生成模型 - Dosovitskiy 等人训练 inverted CNN 模型来呈现 chairs 的图像，而 Yildirim 等人使用在合成脸部渲染上训练的深 CNN 特征来从脸部图像中回归姿态参数。  \n",
    "\n",
    "**Augmenting 单个图像** 有一大部分工作是以真实的方式插入物体，并从单幅图像中推断 3D 结构 - Karsch 等人开发了一种令人印象深刻的半自动方法来渲染具有正确照明和视角的物体；他们根据 Criminisi 等人的技术推断物体的实际尺寸。Hoiem 等人将图像区域从单个图像分为地平面，垂直平面或天空，并通过将图像分解为平面来生成“pop-ups”。同样，我们也将单个图像分解成局部平面区域，但使用 Liu 等人的密集深度预测。 \n",
    "\n",
    "## 2. Synthetic Text in the Wild  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/3/3.jpg?raw=true)  \n",
    "\n",
    "包含数百万个参数的深度 CNN 等大型模型的监督训练需要非常大量的标记训练数据，这对手动获取来说是昂贵的。此外，如表 1 所总结的，公开可用的文本发现或检测数据集非常小。这些数据集不仅不足以训练大型 CNN 模型，还不足以表示自然场景中字体，颜色，大小，位置等可能的文本变化空间。因此，在本节中，我们开发了一个合成文本场景图像生成引擎，用于构建用于文本定位的大型标注数据集。  \n",
    "\n",
    "我们的合成引擎（1）生成逼真的场景文本图像，以便训练过的模型可以推广到真实（非合成）图像，（2）是全自动的，（3）快速，能够在无监督下生成大量的数据。文本生成流程可以概括如下（另见图3）。在获取合适的文本和图像样本（2.1节）后，图像被分割成基于局部颜色和纹理提示的连续区域，并且使用[30]的 CNN 获得密集的像素深度图（2.2节）。然后，对于每个连续区域估计局部表面法线。  \n",
    "\n",
    "接下来，根据区域的颜色（第2.3节）选择文本的颜色以及可选的轮廓颜色。最后，使用随机选择的字体呈现文本样本并根据局部表面方向进行变换；使用泊松图像编辑将文本混合到场景中。我们的引擎需要大约半秒来生成新的场景文本图像。  \n",
    "\n",
    "该方法用于生成 800,000 个场景文本图像，每个图像具有以不同样式呈现的多个单词实例，如图2 所示。数据集可在以下网址获得：http://www.robots.ox.ac.uk/~vgg/data/scenetext/  \n",
    "\n",
    "### 2.1. Text and Image Sources  \n",
    "\n",
    "合成文本生成过程首先对一些文本和背景图像进行采样。该文本以三种方式从 Newsgroup20 数据集中提 -文字，行（最多 3 行）和段落（最多 7 行）。 单词被定义为由空格字符分隔的标记，线由换行符分隔。这是一个丰富的数据集，英文文本的自然分布中散布着符号，标点符号，名词和数字。  \n",
    "\n",
    "为了支持多样化，通过与不同对象/场景以及室内/室外和自然/人工场所相关的查询从 Google 图像搜索中提取 8,000 个背景图像。为了保证所有的文本事件都被充分注释，这些图像不能包含它们自己的文本（街景文本的局限性在于注释并非详尽无遗）。因此，避免了会召回图像中大量文本的关键词（例如“街道标志”，“菜单”等） 包含文字的图像通过人工检查被丢弃。  \n",
    "\n",
    "### 2.2. Segmentation and Geometry Estimation  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/3/4.jpg?raw=true)  \n",
    "\n",
    "在实际图像中，文本倾向于包含在明确定义的区域（例如符号）中。我们通过要求文本被包含在具有统一颜色和纹理特征的区域中来逼近这个约束。这也可以防止文字跨越强烈的图像不连续性，这在实践中不太可能发生。使用[3]的有效图形切割实现，通过在 0.11 处对 gPb-UCM 轮廓分层结构[2]进行阈值化来获得区域。图 4 显示了一个关于局部区域提示的文本示例。  \n",
    "\n",
    "在自然图像中，文字往往被涂在表面的顶部（例如标志或杯子）。为了近似我们的合成数据中的类似效应，文本根据局部表面法线进行透视变换。首先使用[30]的 CNN 预测密集的深度图，对上面分割的区域自动估计法线，然后使用 RANSAC 拟合一个平面的面。  \n",
    "\n",
    "文本与估计的区域方向对齐如下：首先，使用估计的平面法线将图像区域轮廓翘曲为正视平面视图；然后，一个矩形适合于平行的区域；最后，文本与此矩形的较大边（“宽度”）对齐。 在同一区域放置多个文本实例时，将检查文本遮罩是否相互碰撞，以避免将它们放置在彼此之上。  \n",
    "\n",
    "并非所有的分割区域都适合文本放置 - 区域不应该太小，具有极端的纵横比，或者表面法线与观察方向正交；所有这些区域都在这个阶段被过滤。此外，纹理过多的区域也被过滤，纹理的程度由 RGB 图像中三阶导数的强度来度量。  \n",
    "\n",
    "**讨论** 使用 CNN 来估计深度（这是容易出错的过程）的替代方案是使用 RGBD 图像的数据集。我们宁愿估算不完美的深度图，因为：（1）它基本上允许使用任何场景类型的背景图像，而不仅仅是那些 RGBD 数据可用的背景图像；（2）因为公开可用的 RGBD 数据集，例如 NYUDv2，B3DO，Sintel 和 Make3D 在我们的上下文中有几个限制：小尺寸（NYUDv21 中 1500张图片，Make3D 中 400 张图片，B3DO 和 Sintel 中少量视频） ，低分辨率和运动模糊，对室内图像（在 NYUDv2 和 B3DO中）的限制，以及基于视频数据集（B3DO 和 Sintel）的图像的有限可变性。  \n",
    "\n",
    "### 2.3. Text Rendering and Image Composition  \n",
    "\n",
    "一旦文字的位置和方向被确定，文本就被分配一种颜色。文本的调色板是从 IIIT5K 单词数据集中的裁剪后的单词图像中学习的。使用 K 均值将每个裁剪后的单词图像中的像素分为两组，其中一个颜色接近前景（文本）颜色，另一个颜色背景。在渲染新文本时，选择其背景色与目标图像区域最匹配的颜色对（使用 Lab 颜色空间中的 L2 范数），并使用相应的前景色来渲染文本。  \n",
    "\n",
    "大约 20% 的文本实例被随机选择为有边框。边框颜色选择为与前景色相同，其值通道增加或减少，或选择为前景色和背景色的平均值。  \n",
    "\n",
    "为了保持合成文本图像中的亮度梯度，我们使用泊松图像编辑将文本混合到基本图像上，引导字段在其方程式（12）中定义。我们使用 Raskar1 提供的实现高效地解决此问题。  \n",
    "\n",
    "## 3. A Fast Text Detection Network  \n",
    "\n",
    "在本节中，我们将介绍用于自然场景中文本检测的 CNN 体系结构。虽然现有的文本检测流程结合了几个 ad-hoc 步骤并且速度很慢，但我们提出了一种高度准确，快速并可端到端训练的检测器。  \n",
    "\n",
    "设 x 表示图像。基于 CNN 的检测最常见的方法是提出一些可能包含目标对象（我们的案例中的文本）的图像区域 R，裁剪图像，并使用 CNN 来评分它们是否正确。这种已被 R-CNN 广泛使用的方法运行良好，但速度很慢，因为它需要用 CNN 对每张图像评估数千次。  \n",
    "\n",
    "另一种快速的对象检测策略是构造一个预测器的固定 field $(c,p) = \\phi_{uv}(x)$，其中每一个都专门用于预测围绕特定图像位置(u,v)的对象是否 $c \\in R$ 并且构成 $p =(x-u,y- v,w,h)$ 。这里姿势参数 (x,y) 和 (w,h) 分别表示紧密包围对象的边界框的位置和大小。  \n",
    "\n",
    "虽然这种结构听起来很抽象，但它实际上是一种常见的结构，例如由隐式形状模型（ISM）和 Hough voting 实现。有一个预测器 $\\phi_{uv}$ 查看以 (u,v) 为中心的局部图像片，并试图预测 (u,v) 周围是否有对象，以及对象相对于它的位置。  \n",
    "\n",
    "在本文中，我们提出 Hough voting 的一个极端变体，受 Long 等人的全卷积网络（FCN）和 Redmon 等人的 You Only Only Once（YOLO）技术的启发。 。在 ISM 和 Hough 投票中，个人预测通过投票方案在图像中汇总。YOLO 类似，但避免投票并直接使用个人预测;因为这个想法可以加速检测，所以我们在这里采用它。  \n",
    "\n",
    "YOLO 和 Hough 投票的另一个关键概念区别在于，霍夫投票预测变量 $\\phi_{uv}$ 是局部和平移不变的，而在 YOLO 中它们不是：首先，在 YOLO 中，每个预测变量允许从整个图像中搜集证据，而不是只在一个以 (u,v) 为中心的图像 patch 中。其次，在不同位置 $(u,v \\neq (u',v'))$ 的 YOLO 预测因子中是独立学习的不同函数 $\\phi_{uv} \\neq \\phi_{u'v'}$。  \n",
    "\n",
    "尽管 YOLO 的方法允许该方法获取用于检测 PASCAL 或 ImageNet 对象的上下文信息，但我们发现这不适用于更小和更可变的文本。相反，我们在这里提出一种在 YOLO 和 Hough 投票之间的方法。与在 YOLO 中一样，每个检测器 $\\phi_{uv}(x)$ 仍然直接预测对象发生，而不经历昂贵的投票累积过程；然而，如在霍夫投票中，检测器 $\\phi_{uv}(x)$ 是局部和平移不变的，共享参数。我们将这个平移不变和局部预测器的领域作为深度 CNN 的最后一层的输出来实现，从而获得全卷积回归网络（FCRN）。  \n",
    "\n",
    "### 3.1. Architecture  \n",
    "\n",
    "本节介绍 FCRN 的结构。首先，我们描述该体系结构的前几层，它计算文本特定的图像特征。然后，我们描述了建立在这些特征之上的密集回归网络，最后描述了它在多个尺度上的应用。  \n",
    "\n",
    "**单尺度特征** 我们的架构受到 VGG-16 的启发，使用几层小密集滤波器；然而，我们发现一个更小的模型对文本的工作效果也更好。该体系结构包含 9 个卷积层，每个卷积层后面跟着 Relu，偶尔还有一个 Maxpooling 层。所有线性滤波器都有 1 个样本的步幅，并通过零填充来保留特征映射的分辨率。 Max-pooling 在 2×2 窗口上执行，步长为 2 个采样，因此特征图分辨率减半。  \n",
    "\n",
    "**类和边界框预测** 单尺度特征终止于密集特征 field。假设有四个下采样 Maxpooling 图层，这些特征的步幅是 Δ= 16个像素，每个包含 512 个特征通道 $\\phi_{uv}(x)$（为了方便起见，我们用 uv 表示像素）。  \n",
    "\n",
    "给定特征 $\\phi_{uv}(x)$，我们现在可以讨论构造密集文本预测器 $\\phi_{uv}(x) = \\phi_{uv}^r o \\phi^f(x)$。这些预测因子被实现为另外七个 5×5 线性滤波器（C-7-5×5）$\\phi_{uv}^r$，每个回归七个数字中的一个：对象存在置信度 c 和多达六个对象姿态参数 $p = (x-u, y-v, w, h, \\cos \\theta, \\sin \\theta)$ 其中 x，y，w，h 已经在前面讨论过，$\\theta$ 是边界框旋转。  \n",
    "\n",
    "因此，对于尺寸为 H×W 的输入图像，我们获得 $\\frac {H}{\\Delta} \\times \\frac {W}{\\Delta}$ 个预测结果，每个结果对应于尺寸为 Δ×Δ 像素的图像单元。如果单词中心落入相应的单元格内，每个预测器负责检测一个单词。 YOLO 类似，但以大约一半的分辨率运行；更密集的预测器采样对于减少冲突（多个词落入同一单元格）并因此增加召回（因为每个单元最多可检测一个单词）是重要的。在实践中，对于 224×224 图像，我们获得 14×14 个cells/预测。  \n",
    "\n",
    "**多尺度检测** 我们的卷积滤波器的有限接收域禁止检测大型文本实例。因此，我们得到输入图像的多个缩小版本的检测结果，并通过非极大值抑制来合并它们。更详细地说，输入图像通过因子 {1，1 / 2,1 / 4,1 / 8} 进行缩小（由于基线特征已经非常密集地计算，所以放大是过度的）。然后，通过抑制分数低于重叠检测的分数来合并所得到的检测结果。  \n",
    "\n",
    "**训练损失** 我们对 CNN 的 $\\frac {H}{\\Delta} \\times \\frac {W}{\\Delta} \\times 7$ 中的每个输出使用一个平方损失项，与 YOLO 相似。如果单元格不包含真实词，则损失除了 c(text/no-text) 之外，忽略所有参数。  \n",
    "\n",
    "**与 YOLO 比较** 我们的全卷积回归网络（FCRN）比 YOLO 网络（最后两个全连接层具有约 90% 的参数）少 30 倍的参数。由于其全局性，对于每个图像尺寸（包括多个尺度），标准 YOLO 必须进行再训练，进一步增加模型尺寸（而我们的模型需要 44MB，YOLO 需要 2GB）。这使 YOLO 不仅更难训练，而且效率也更低（比 FCRN 慢 2 倍）。  \n",
    "\n",
    "## 4. Evaluation  \n",
    "\n",
    "首先，在 4.1 节中，我们描述了我们评估模型的文本数据集。接下来，我们在 4.2 节的文本定位任务中评估我们的模型。在第 4.3 节中，为了研究合成数据生成流程中的哪些组件是重要的，我们执行详细的消融实验。在第 4.4 节中，我们使用定位模型的结果进行端到端的文本识别。我们在文本定位和端到端文本识别方面展现了实质性的改进。最后，在第 4.5 节中，我们将讨论使用我们的文本定位模型获得的加速。  \n",
    "\n",
    "### 4.1. Datasets  \n",
    "\n",
    "我们使用标准基准评估我们的文本检测网络：ICDAR 2011,2013 数据集和街景视图文本数据集。这些数据集将在下面进行回顾，他们的统计数据见表 1。  \n",
    "\n",
    "**SynthText in the Wild** 这是使用我们的第 2 部分的合成引擎生成的 800,000 个训练图像的数据集。每个图像都有大约 10 个用字符和字级边界框注释的单词实例。  \n",
    "\n",
    "**ICDAR 数据集** ICDAR 数据集（IC011，IC013）分别来自于 2011 年和 2013 年举办的 Robust 阅读挑战赛。它们包含标志牌，书籍，海报和其他具有world-level 轴对齐边界框注释的对象上的真实世界的文本图像。数据集大部分包含相同的图像，但是 shuffle 测试和训练集。我们不评估最近的 ICDAR 2015 数据集，因为它与 2013 数据集几乎相同。  \n",
    "\n",
    "**街景视图文本** 此数据集缩写为 SVT，由谷歌街景收获的图像组成，该图像使用 word-level axis-aligned 边界框注释。SVT 比 ICDAR 数据更具挑战性，因为它包含更小和更低的分辨率文本。此外，并非所有文本实例都被注释。实际上，这意味着在评估过程中精确度被严重低估。为每幅图像提供包含 50 个干扰词以及真实词的词典；我们在 SVT 测试时将这些词典作为参考 。  \n",
    "\n",
    "## 4.2. Text Localisation Experiments  \n",
    "\n",
    "我们评估我们的检测网络：（1）比较应用于图像的单尺度和多尺度缩小版本时的性能；（2）使用高质量的区域提议，改进文本检测中的最新结果。  \n",
    "\n",
    "**训练** FCRN 在 Wild 数据集的 SynthText 中对 800,000 个图像进行训练。每个图像的大小调整为 512×512 像素。在每个卷积层（除最后一个卷积层之外）之后，我们使用 SGD 对动量和批量归一化进行优化。我们分别使用 16 个图像的 mini-batches，将动量设置为 0.9，并使用 $5^{-4}$ 的权重衰减。学习率最初设定为 $10^{-4}$，当训练损失到极值时，学习率降至 $10^{-5}$。  \n",
    "\n",
    "由于只有一小部分（1-2%）的网格单元包含文本，因此我们首先将非文本概率误差项与 0.01 相乘；随着训练的进行，这个权重逐渐增加到 1。由于类别不平衡，如果不使用这种加权方案，所有概率分数都会降至零。  \n",
    "\n",
    "**推理** 我们从我们的 FCRN 模型中得到分类概率和边界框预测。通过对类别概率设置阈值对预测进行过滤。最后，使用非极大值抑制抑制来自邻近小区的多个检测，由此在两个重叠检测中抑制具有较低概率的检测。在下面，我们首先给出 t = 0.3 的保守阈值的结果，以获得更高的精度，然后将其放松到 t = 0.0（即接受所有提议）以获得更高的召回率。  \n",
    "\n",
    "**评估协议** 我们使用文献中常用的两种协议报告文本检测性能 - （1）在 ICDAR 竞赛中广泛使用的评估定位方法的 DetEval，以及（2）PASCAL VOC 风格 intersection-over-union 重叠方法（≥0.5 IoU positive 检测）。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/3/5.jpg?raw=true)  \n",
    "\n",
    "**单一和多尺度检测** 表 2 中的“FCRN 单尺度”条目显示了我们的 FCRN 模型在测试数据集上的表现。单尺度 FCRN 的最大 F 测量精度与 Neuman 等人的方法相当，而召回则明显恶化了12%。  \n",
    "\n",
    "表2 中的“FCRN 多尺度”条目显示了我们网络多尺度应用的性能。这种方法比单尺度方法的最大召回提高了 12% 以上，并且胜过了 Neumann 等人的方法。    \n",
    "\n",
    "**proposals 后处理** 当前的端到端文本识别（检测和识别）方法通过将检测与文本识别相结合来提高性能。为了进一步改进 FCRN 检测，我们使用FCRN 的多尺度检测作为提议并通过使用 Jaderberg 等人的后处理阶段对其进行改进。有三个阶段：首先使用二元 text/notext 随机森林分类器进行过滤；其次，使用 CNN 回归改进边界框；第三基于识别的 NMS，其中使用基于大型固定词典的 CNN 来识别单词图像，并且通过基于单词身份的非最大抑制来合并检测。详情见[20]。我们使用作者提供的代码进行公平比较。  \n",
    "\n",
    "我们在两种模式下进行测试 - （1）low-recall：只使用高分（概率> 0.3）的多尺度 FCRN 检测（之前在单尺度和多尺度推理中使用的阈值）。这通常会产生少于 30 个提案。（2）high-recall：所有多尺度 FCRN 检测（通常约一千个）被使用。 这些方法在文本检测中的性能分别由表 2 中的名为“FCRN + multi-filt”和“FCRNall + multi-filt”的条目示出。请注意，low-recall 方法比现有技术更好，而 high-recall 方法则显着改善了所有数据集的 F-measure，改进 6% 的现状。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/3/6.jpg?raw=true)  \n",
    "\n",
    "图 5 显示了 IC13 数据集上文本检测的 Precision-Recall 曲线。请注意，在使用多重过滤后处理进行修正之前，FCRN 多尺度检测输出的 high-recall（85.9%）。此外，与 Jaderberg 等人相比，注意“FCRNall + multi-filt”的最大召回率（+ 10.3%）和平均精确度（+ 11.1%）的急剧增加。  \n",
    "\n",
    "此外，为了确定文本检测的改进是由于新的检测模型引起的，而不仅仅是由于我们的合成数据集的大尺寸，我们在 Wild 数据集的 SynthText 上训练了Jaderberg 等人的方法，图 5 和表 2 显示，即使有 10 倍以上（合成）的训练数据，Jaderberg 等人的模型也仅略有提高(AP +0.8%，recall +2.1%)。  \n",
    "\n",
    "一种常见的失败模式是文本中不属于训练集的非常规字体。探测器也会被符号或图案的笔画宽度看起来像文本一样混淆，例如道路标志，棒图等。由于探测器不会缩放图像，所以不会检测到极小尺寸的文本实例。最后，由于字符之间的间距较大或较小，单词会分解为多个实例或合并为一个实例。  \n",
    "\n",
    "### 4.3. Synthetic Dataset Evaluation  \n",
    "\n",
    "我们调查合成文本场景数据生成流程的各个阶段对定位精度的贡献：我们生成三个合成训练数据集，其复杂程度越来越高，（1）放置在图像内的随机位置，（2）局限于局部颜色和纹理边界，以及（3）透视变形以匹配局部场景深度（同时也尊重上面（2）中的局部颜色和纹理边界）。数据集的所有其他方面保持不变-例如文字词典，背景图片，颜色分布。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/3/7.jpg?raw=true)  \n",
    "\n",
    "图 6 显示了我们的方法“FCRNall + multi-filt”在 SVT 数据集上的定位结果。与随机放置相比，将文本限制在局部颜色和纹理区域可显着提高最大recall（+6.8％），AP（+3.85％）和 最大F-measure（+2.1％）。通过增加透视失真观察到边缘改善：AP为+0.75％，最大F-measure为+0.55％，最大 recall 没有变化。这可能是由于 SVT 数据集中的大多数文本实例处于前端并行方向。ICDAR 2013 数据集观察到类似的趋势，但更多的差异可能是由于ICDAR 的文本实例比 SVT 简单得多，并且从更高级的数据集中受益较少。  \n",
    "\n",
    "### 4.4. End-to-End Text Spotting  \n",
    "\n",
    "文本识别受检测阶段的限制，因为最先进的裁剪文字图像识别精度超过了98%。我们利用我们对文本定位的改进来获取文本识别中的最新结果。  \n",
    "\n",
    "**评估协议** 除非另有说明，否则我们遵循 Wang 等人的标准评估协议。其中所有长度小于三个字符或包含非字母数字字符的单词都将被忽略。正检测需要至少 0.5 的重叠（IoU）。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/3/8.jpg?raw=true)  \n",
    "\n",
    "表 3 显示了使用“FCRN + multi-filt”和“FCRNall + multi-filt”方法的端到端文本识别任务的结果。为了识别，我们使用基于 Jaderberg 等人的词汇编码 CNN 的管道中间记录识别阶段的输出。我们改进了之前报道的结果（F-measure）：ICDAR 数据集 +8%，SVT数据集 +3%。鉴于我们方法的高 recall（如图 5 所示），SVT 中许多文本实例未标记的事实导致精度下降；因此，我们看到 SVT 收益较小，SVT-50 收益较差。  \n",
    "\n",
    "### 4.5. Timings  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/3/9.jpg?raw=true)  \n",
    "\n",
    "在测试时，FCRN 可以在单一尺度下每秒处理 20 幅图像（大小为 512×512 像素），并且在 GPU 上以多个比例（1,1 / 2,1 / 4,1 / 8）运行时每秒处理大约 15 幅图像。当作为 Jaderberg 等人的文本定位流程中的高质量提案使用时。它取代了区域建议阶段，每个图像通常需要约 3 秒。因此，我们在区域提案阶段加快了约 45 倍。此外，“FCRN +多重过滤”方法仅使用多尺度 FCRN 的高分值检测，并在检测和端到端文本识别中实现了最新的结果，从而减少了项目后期阶段区域提议数量的 10 倍：Jaderberg 等人提议阶段出了大约 2000 个盒子，这些盒子使用随机森林分类器快速过滤到可过滤到约200个提议，而多尺度 FCRN 的高分数检测通常小于 30。表 4 比较了端到端文本识别所需的时间；我们的方法比 Jaderberg 等人的要快 3 倍到 23 倍，这取决于变体。    \n",
    "\n",
    "## 5. Conclusion  \n",
    "\n",
    "我们开发了一种新的 CNN 架构，用于在图像中生成文字区域提议。在可用的带注释的数据集上训练这种体系结构是不可能的，因为它们包含的样本太少，但我们已经表明，可以合成地生成足够逼真的训练图像，并且仅在这些图像上训练的 CNN 超过在真实图像上进行检测和端到端文本识别的最先进的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Text in Natural Image with Connectionist Text Proposal Network  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "我们提出了一种新颖的连接文本提议网络（CTPN），它能够准确定位自然图像中的文本行。CTPN 直接在卷积特征映射中的一系列细粒度文本提议中检测文本行。我们开发了一个垂直 anchor 机制，联合预测每个固定宽度提议的位置和文本/非文本分数，大大提高了定位精度。序列提议通过循环神经网络自然地连接起来，该网络无缝地结合到卷积网络中，从而形成端到端的可训练模型。这使得 CTPN 可以探索丰富的图像上下文信息，使其能够检测极其模糊的文本。CTPN 在多尺度和多语言文本上可靠地工作，而不需要进一步的后处理，脱离了以前的自底向上需要多步后过滤的方法。它在 ICDAR 2013 和 2015 的基准数据集上达到了 0.88 和 0.61 的 F-measure，大大超过了最近的结果。通过使用非常深的 VGG16 模型，CTPN 的计算效率为 0.14s 每张图像。在线 demo 地址为：[http://textdet.com/](http://textdet.com/)。\n",
    "\n",
    "## 1 Introduction  \n",
    "\n",
    "在自然图像中阅读文本最近在计算机视觉中引起越来越多的关注。这是由于它的许多实际应用，如图像 OCR，多语言翻译，图像检索等。它包括两个子任务：文本检测和识别。这项工作的重点是检测任务，这是比在一个良好的裁剪文字图像上进行的识别更具有挑战性的任务。文本模式的大变化和高度杂乱的背景构成了精确文本定位的主要挑战。  \n",
    "\n",
    "目前的文本检测方法大多采用自下而上的流程。它们通常从低级别字符或笔画检测开始，后面通常会跟随一些后续步骤：非文本组件过滤，文本行构建和文本行验证。这些自底向上的多步骤方法通常复杂，鲁棒性和可靠性较差。它们的性能很大程度上依赖于字符检测的结果，并且已经提出了连接组件方法或滑动窗口方法。这些方法通常探索低级特征（例如，基于SWT，MSER 或 HoG）来区分候选文本和背景。但是，如果没有上下文信息，他们不能鲁棒的单独识别各个笔划或字符。例如，相比单个字符人们更信任一个字符序列，特别是当一个字符非常模糊时。这些限制在字符检测中通常会导致生成大量非文本检测，在后续步骤中的主要困难是处理它们。此外，正如[28]所指出的，这些误检很容易在自下而上的过程中连续累积。为了解决这些问题，我们利用强大的深度特征直接在卷积映射中检测文本信息。我们开发的文本 anchor 机制能在细粒度上精确预测文本位置。然后，我们提出了一种网内循环架构，用于按顺序连接这些细粒度的文本提议，从而允许它们编码丰富的上下文信息。  \n",
    "\n",
    "深度卷积神经网络（CNN）最近已经基本实现了一般物体检测。最先进的方法是 Faster Region-CNN（R-CNN）系统，其中提出了区域提议网络（RPN）直接从卷积特征映射中生成高质量类别不可知的目标提议。然后将 RPN 提议输入 Faster R-CNN 模型进行进一步的分类和微调，从而实现通用目标检测的最新性能。然而，很难将这些通用目标检测系统直接应用于场景文本检测，这通常需要更高的定位精度。在通用目标检测中，每个目标都有一个明确的封闭边界，而在文本中可能不存在这样一个明确定义的边界，因为文本行或单词是由许多单独的字符或笔划组成的。对于目标检测，典型的正确检测是松散定义的，例如，检测到的边界框与其实际边界框（例如，PASCAL 标准）之间的重叠 >0.5，因为人们可以容易地从目标的主要部分识别它。相比之下，综合阅读文本是一个细粒度的识别任务，需要正确的检测，覆盖文本行或字的整个区域。因此，文本检测通常需要更准确的定义，导致不同的评估标准，例如文本基准中常用的 Wolf 标准。  \n",
    "\n",
    "在这项工作中，我们通过将 RPN 架构扩展到准确的文本行定义来填补这个空白。我们提出了几种技术发展，针对我们的问题可以优雅地调整通用目标检测模型。我们通过提出一种网络内循环机制争取更进一步，使我们的模型能够直接在卷积映射中检测文本序列，避免通过额外昂贵的 CNN 检测模型进行进一步的后处理。  \n",
    "\n",
    "### 1.1 Contributions  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/4/1.jpg?raw=true) \n",
    "\n",
    "我们提出了一种新颖的 Connectionist Text Proposal Network （CTPN），它可以直接定位卷积层中的文本序列。这克服了以前的建立在字符检测基础上的自下而上方法带来的一些主要限制。我们利用强深度卷积特性和共享计算机制的优点，提出了如图1 所示的 CTPN 架构。主要贡献如下：  \n",
    "\n",
    "首先，我们将文本检测的问题转化为一系列细粒度的文本提议。我们开发了一个 anchor 回归机制，可以联合预测每个文本提议的垂直位置和文本/非文本分数，从而获得出色的定位精度。这背离了整个目标的 RPN 预测，RPN 预测难以提供令人满意的定位精度。  \n",
    "\n",
    "其次，我们提出了一种在卷积特征映射中优雅连接序列文本提议的网络内循环机制。通过这种连接，我们的检测器可以探索文本行有意义的上下文信息，使其能够可靠地检测极具挑战性的文本。  \n",
    "\n",
    "第三，两种方法无缝集成，以符合文本序列的性质，从而形成统一的端到端可训练模型。我们的方法能够在单个过程中处理多尺度和多语言的文本，避免进一步的后过滤或细化。  \n",
    "\n",
    "第四，我们的方法在许多基准数据集上达到了新的最先进效果，显著改善了最近的结果（例如，0.88 的 F-measure 超过了 2013 年 ICDAR 的中的 0.83，而 0.64 的 F-measure 超过了 ICDAR2015 上中的0.54 ）。此外，通过使用非常深的 VGG16 模型，每张图像运行 0.14s（在ICDAR 2013上），这在计算上是高效的。  \n",
    "\n",
    "## 2 Related Work  \n",
    "\n",
    "**文本检测** 过去在场景文本检测中的工作一直以自下而上的方法为主，一般建立在笔画或字符检测上。它们可以粗略地分为两类，基于连接组件（CC）的方法和基于滑动窗口的方法。基于 CC 的方法通过使用快速滤波器来区分文本和非文本像素，然后通过使用低级属性（例如强度，颜色，梯度等）将文本像素贪婪地分为笔划或候选字符。基于滑动窗口的方法通过在图像中密集地滑动多尺度窗口来检测候选字符。字符或非字符窗口通过预先训练的分类器，使用手动设计的特征或最近的 CNN 特征进行区分。然而，这两种方法通常都会受到较差的字符检测性能的影响，导致在接下来的组件过滤和文本行构建步骤中出现累积的错误。此外，鲁棒地过滤非字符组件或者自信地验证检测到的文本行本身就更加困难。另一个限制是通过在大量的滑动窗口上运行分类器，滑动窗口方法在计算上是昂贵的。  \n",
    "\n",
    "**目标检测** 卷积神经网络（CNN）近来在通用目标检测上已经取得了实质的进步。一个常见的策略是通过使用廉价的低级特征来生成许多目标提议，然后使用强 CNN 分类器来进一步对生成的提议进行分类和细化。生成类别不可知的目标提议的选择性搜索（SS）是目前领先的目标检测系统中应用最广泛的方法之一，如 CNN（R-CNN）及其扩展。最近，Ren 等人提出了 Faster R-CNN 目标检测系统。他们提出了一个区域提议网络（RPN），可以直接从卷积特征映射中生成高质量的类别不可知的目标提议。通过共享卷积计算 RPN 是快速的。然而，RPN 提议不具有判别性，需要通过额外的成本高昂的 CNN 模型（如Fast R-CNN模型）进一步细化和分类。更重要的是，文本与一般目标有很大的不同，因此很难直接将通用目标检测系统应用到这个高度领域化的任务中。  \n",
    "\n",
    "## 3 Connectionist Text Proposal Network  \n",
    "\n",
    "本节介绍连接文本提议网络（CTPN）的细节。它包括三个关键的贡献，使文本定位可靠和准确：检测细粒度提议中的文本，循环连接文本提议和边缘细化。  \n",
    "\n",
    "### 3.1 Detecting Text in Fine-scale Proposals  \n",
    "\n",
    "类似于区域提议网络（RPN），CTPN 本质上是一个全卷积网络，允许任意大小的输入图像。它通过在卷积特征映射中密集地滑动小窗口来检测文本行，并且输出一系列细粒度的（例如，宽度为固定的 16 个像素）文本提议，如图 1（b）所示。  \n",
    "\n",
    "我们以非常深的 16 层 vggNet（VGG16）为例来描述我们的方法，该方法很容易应用于其他深度模型。CTPN 的架构如图 1（a）所示。我们使用一个小的空间窗口 3×3 在最后的卷积层特征图上滑动（例如，VGG16 的 conv5）。conv5 特征映射的大小由输入图像的大小决定，而总步长和感受野分别固定为 16 个和 228 个像素。网络架构决定总步长和感受野。在卷积层中使用滑动窗口允许它共享卷积计算，这是减少昂贵的基于滑动窗口的方法的计算量的关键。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/4/2.jpg?raw=true) \n",
    "\n",
    "通常，滑动窗口方法采用多尺度窗口来检测不同尺寸的目标，其中一个窗口尺度被固定到与目标的尺寸相似。在[25]中，Ren 等人提出了一种有效的锚点回归机制，允许 RPN 使用单尺度窗口检测多尺度目标。关键点是单个窗口能够通过使用多个灵活的锚点来预测各种尺度和长宽比的目标。我们希望将这种有效的锚点机制扩展到我们的文本任务中。然而，实质上文本与普通目标不同，它们通常具有明确的封闭边界和中心，可以从它的一部分推断整个目标。文本是一个没有明显封闭边界的序列。它可能包含多层次的组件，如笔划，字符，单词，文本行和文本区域等，这些组件之间没有明确区分。文本检测是在单词或文本行级别中定义的，因此通过将其定义为单个目标（例如检测单词的一部分）可能很容易进行错误的检测。因此，直接预测文本行或单词的位置可能很难或不可靠，因此很难获得令人满意的准确性。一个例子如图 2 所示，其中 RPN 直接被训练用于定位图像中的文本行。  \n",
    "\n",
    "我们寻找文本的独特属性，能够很好地概括各个层次的文本组件。我们观察到由 RPN 进行的单词检测很难准确预测单词的水平边，因为单词中的每个字符都是孤立的或分离的，这使得查找单词的开始和结束位置很混乱。显然，文本行是一个序列，它是文本和通用目标之间的主要区别。很自然的将文本行视为一系列细粒度的文本提议，其中每个提议通常代表文本行的一小部分，例如宽度为 16 个像素的文本块。每个提议可能包含单个或多个笔划，字符的一部分，单个或多个字符等。我们认为，通过固定每个提议的水平位置来预测其垂直位置会更准确，水平位置更难预测。与预测目标 4 个坐标的 RPN 相比，这减少了搜索空间。我们开发了垂直锚点机制，可以同时预测每个细粒度提议的文本/非文本分数和 y 轴的位置。检测一般固定宽度的文本提议比识别分隔的字符更可靠，分隔字符容易与字符或多个字符的一部分混淆。此外，检测一系列固定宽度文本提议中的文本行也可以在多个尺度和多个长宽比的文本上可靠地工作。  \n",
    "\n",
    "为此，我们设计如下的细粒度文本提议。我们的检测器密集地检测onv5 中的每个空间位置。文本提议被定义为具有16个像素的固定宽度（在输入图像中）。这相当于在conv5的映射上密集地移动检测器，其中总步长恰好为 16 个像素。然后，我们设计 $k$ 个垂直锚点来预测每个提议的 $y$ 坐标。$k$ 个锚点具有相同的水平位置，固定宽度为 16 个像素，但其垂直位置有 $k$ 个不同的高度变化。在我们的实验中，我们对每个提议使用十个锚点 $k=10$，其高度在输入图像中从 11 个像素变化到 273 个像素（每次 $\\div 0.7$）。明确的垂直坐标是通过提议边界框的高度和 $y$ 轴中心来度量的，我们计算相对于锚点的边界框位置的相对预测垂直坐标（$\\textbf{v}$），如下所示：  \n",
    "\n",
    "$v_c = (c_y - c_y^a) / h^a, \\ v_h=\\log (h/h^a) \\tag{1} $  \n",
    "$v_c^* = (c_y^* - c_y^a)/h^a, \\ v_h^* = \\log (h^* / h_a) \\tag{2}$  \n",
    "\n",
    "其中 $\\textbf{v}=\\lbrace v_c,v_h \\rbrace$ 和 $\\textbf{v}^*=\\lbrace v_c^*, v_h^* \\rbrace$ 分别是相对预测坐标和实际坐标。$c_y^a$ 和 $h^a$ 是锚的中心（ $y$ 轴）和高度，可以从输入图像预先计算。$c_y$ 和 $h$ 是输入图像中预测的 $y$ 轴坐标，而 $c_y^*$ 和 $h^*$ 是实际坐标。因此，如图1（b）和图2（右）所示，每个预测文本提议都有一个大小为 $h \\times 16$ 的边界框（在输入图像中）。一般来说，文本提议在很大程度上要比它的有效感受野 228 $\\times$ 228 要小。  \n",
    "\n",
    "检测处理总结如下。给定输入图像，我们有 $W \\times H \\times C$ conv5 特征图（通过使用 VGG16 模型），其中 $C$ 是特征映射或通道的数目，并且 $W \\times H$ 是空间布置。当我们的检测器通过 conv5 密集地滑动 3$\\times$3 窗口时，每个滑动窗口使用 $3 \\times 3 \\times C$ 的卷积特征来产生预测。对于每个预测，水平位置（$x$ 轴坐标）和 $k$ 个锚点位置是固定的，可以通过将 conv5 中的空间窗口位置映射到输入图像上来预先计算。我们的检测器在每个窗口位置输出 $k$ 个锚点的文本/非文本分数和预测的 $y$ 轴坐标（$\\textbf{v}$）。检测到的文本提议是从具有 $>0.7$（具有非极大值抑制）的文本/非文本分数的锚点生成的。通过设计的垂直锚点和细粒度的检测策略，我们的检测器能够通过使用单尺度图像处理各种尺度和长宽比的文本行。这进一步减少了计算量，同时预测了文本行的准确位置。与 RPN 或 Faster R-CNN 系统相比，我们的细粒度检测提供更详细的监督信息，自然会导致更精确的检测。  \n",
    "\n",
    "### 3.2 Recurrent Connectionist Text Proposals  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/4/3.jpg?raw=true) \n",
    "\n",
    "为了提高定位精度，我们将文本行分成一系列细粒度的文本提议，并分别预测每个文本提议。显然，将每个孤立的提议独立考虑并不鲁棒。这可能会导致对与文本模式类似的非文本目标的误检，如窗口，砖块，树叶等（在文献[13]中称为类文本异常值）。还可以丢弃一些含有弱文本信息的模糊模式。图 3 给出了几个例子（上）。文本具有强大的序列特征，序列上下文信息对做出可靠决策至关重要。最近的工作已经证实了这一点，其中应用递归神经网络（RNN）来编码用于文本识别的上下文信息。他们的结果表明，序列上下文信息极大地促进了对裁剪的单词图像的识别任务。  \n",
    "\n",
    "受到这项工作的启发，我们认为这种上下文信息对于我们的检测任务也很重要。我们的检测器应该能够探索这些重要的上下文信息，以便在每个单独的提议中都可以做出更可靠的决策。此外，我们的目标是直接在卷积层中编码这些信息，从而实现细粒度文本提议优雅无缝的网内连接。RNN 提供了一种自然选择，使用其隐藏层对这些信息进行循环编码。为此，我们提出在 conv5 上设计一个 RNN 层，它将每个窗口的卷积特征作为序列输入，并在隐藏层中循环更新其内部状态：$H_t$，  \n",
    "\n",
    "$H_{t}=\\varphi(H_{t-1}, X_t), \\qquad t=1,2,...,W \\tag{3}$  \n",
    "\n",
    "其中 $X_t \\in R^{3\\times 3 \\times C}$ 是输入 conv5 第 $t$ 个滑动窗口(3$\\times$3)的特征。滑动窗口从左向右密集移动，导致每行产生 $t=1,2,...,W$ 个序列特征。$W$ 是 conv5 的宽度。$H_t$ 是从当前输入（$X_t$）和以 $H_{t-1}$ 编码的先前状态联合计算的循环内部状态。递归是通过使用非线性函数 $\\varphi$ 来计算的，它定义了循环模型的确切形式。我们利用长短时记忆（LSTM）架构作为我们的 RNN 层。通过引入三个附加乘法门：输入门，忘记门和输出门，专门提出了 LSTM 以解决梯度消失问题。细节可以在[12]中找到。因此，RNN 隐藏层中的内部状态可以访问所有先前窗口通过循环连接扫描的序列上下文信息。我们通过使用双向 LSTM 来进一步扩展 RNN 层，这使得它能够在两个方向上对递归上下文进行编码，以便连接感受野能够覆盖整个图像宽度，例如 228$\\times$ width。我们对每个 LSTM 使用一个 128 维的隐藏层，从而产生 256 维的 RNN 隐藏层状态 $H_t \\in R^{256}$。  \n",
    "\n",
    "$H_t$ 中的内部状态被映射到后面的 FC 层，并且输出层用于计算第 $t$ 个提议的预测。因此，我们与 RNN 层的集成非常优雅，从而形成了一种高效的模型，可以在无需额外成本的情况下进行端到端的训练。RNN 连接的功效如图3 所示。显然，它大大减少了错误检测，同时还能够恢复很多包含非常弱的文本信息的遗漏文本提议。  \n",
    "\n",
    "### 3.3 Side-refinement  \n",
    "\n",
    "我们的 CTPN 能够准确可靠地检测细粒度的文本提议。通过连接其文本/非文本分数为 $>0.7$ 的连续文本提议，文本行的构建非常简单。文本行构建如下。首先，我们为提议 $B_i$ 定义一个配对邻居（$B_j$）作为 $B_j->B_i$，当（i）$B_j$ 在水平距离最接近 $B_i$，（ii）该距离小于 50 像素，并且（iii）它们的垂直重叠 $>0.7$ 时。其次，如果 $B_j->B_i$ 和 $B_i->B_j$，则将两个提议分组为一对。然后通过顺序连接具有相同提议的对来构建文本行。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/4/4.jpg?raw=true) \n",
    "\n",
    "细粒度的检测和 RNN 连接可以预测垂直方向的精确位置。在水平方向上，图像被分成一系列相等的宽度为 16 个像素的提议。如图4 所示，当两个水平边的文本提议没有完全被实际文本行区域覆盖，或者某些边的提议被丢弃（例如文本得分较低）时，这可能会导致不准确的定位。这种不准确性在通用目标检测中可能并不重要，但在文本检测中不应忽视，特别是对于那些小型文本行或文字。为了解决这个问题，我们提出了一种边缘细化的方法，可以精确地估计左右两侧水平方向上的每个锚点/提议的偏移量（称为边缘锚点或边缘提议）。与 y 坐标预测类似，我们计算相对偏移为：  \n",
    "\n",
    "$o=(x_{side}-c_x^a)/w^a, \\quad o^*=(x_{side}^*-c_x^a)/w^a \\tag{4}$  \n",
    "\n",
    "其中 $x_{side}$ 是最接近水平边（例如，左边或右边）到当前锚点的预测的 $x$ 坐标。$x_{side}^*$ 是 $x$ 轴的实际（GT）边缘坐标，它是从实际边界框和锚点位置预先计算的。$c_x^a$ 是 $x$ 轴的锚点的中心。$w^a$ 是固定的锚点宽度，$w^a=16$。当我们将一系列检测到的细粒度文本提议连接到文本行中时，这些提议被定义为开始和结束提议。我们只使用边缘提议的偏移量来优化最终的文本行边界框。通过边缘细化改进的几个检测示例如图4 所示。边缘细化进一步提高了定位精度，从而使 SWT 和 Multi-Lingual 数据集上的性能提高了约 $2%$。请注意，我们的模型同时预测了边缘细化的偏移量，如图1 所示。它不是通过额外的后处理步骤计算的。  \n",
    "\n",
    "### 3.4 Model Outputs and Loss Functions  \n",
    "\n",
    "提出的 CTPN 有三个输出共同连接到最后的 FC 层，如图1（a）所示。这三个输出同时预测公式（2）中的文本/非文本分数（$\\textbf {s}$），垂直坐标（$\\textbf{v}=\\lbrace v_c, v_h\\rbrace$）和边缘细化偏移（$\\textbf{o}$）。我们将探索 $k$ 个锚点来预测它们在 conv5 中的每个空间位置，从而在输出层分别得到 $2k$，$2k$ 和 $k$ 个参数。  \n",
    "\n",
    "我们采用多任务学习来联合优化模型参数。我们引入了三种损失函数：$L_s^{cl}$，$L_v^{re}$和$l_o^{re}$，其分别计算文本/非文本分数，坐标和边缘细化。考虑到这些因素，我们遵循[5，25]中应用的多任务损失，并最小化图像的总体目标函数（$L$）：  \n",
    "\n",
    "$L(\\textbf{s}_i, \\textbf{v}_j, \\textbf{o}_k) =\\frac1{N_{s}}\\sum_iL^{cl}_{s}(\\textbf{s}_i, \\textbf{s}_i^*) +\\frac{\\lambda_1}{N_v}\\sum_j L^{re}_v(\\textbf{v}_j, \\textbf{v}_j^*) +\\frac{\\lambda_2}{N_o}\\sum_k L^{re}_o(\\textbf{o}_k, \\textbf{o}_k^*) \\tag{5}$  \n",
    "\n",
    "其中每个锚点都是一个训练样本，$i$ 是一个小批量数据中一个锚点的索引。$\\textbf{s}_i$ 是预测的锚点 $i$ 作为实际文本的预测概率。 $\\textbf{s}_i^*=\\lbrace 0,1\\rbrace$ 是真实值。$j$ 是 $y$ 坐标回归中有效锚点集合中锚点的索引，定义如下。有效的锚点是定义的正锚点（$\\textbf{s}_j^*=1$，如下所述），或者与实际文本提议重叠（IoU）$>0.5$。$\\textbf{v}_j$ 和 $\\textbf{v}_j^*$ 是与第 $j$ 个锚点关联的预测的和真实的 $y$ 坐标。$k$ 是边缘锚点的索引，其被定义为在实际文本行边界框水平距离的左侧或右侧（例如 32 个像素）内的一组锚点。$\\textbf{o}_k$ 和 $\\textbf{o}_k^*$ 是与第 $k$ 个锚点关联的 $x$ 轴的预测和实际偏移量。$L_s^{cl}$ 是我们使用 Softmax 损失区分文本和非文本的分类损失。$L_v^{re}$ 和 $L_o^{re}$ 是回归损失。我们遵循以前的工作，使用平滑 $L_1$ 函数来计算它们。$\\lambda_1$ 和 $\\lambda_2$ 是损失权重，用来平衡不同的任务，将它们经验地设置为 1.0 和 2.0。$N_{s}$，$N_{v}$ 和 $N_{o}$ 是标准化参数，表示 $L^{cl}_s$，$L^{re}_v$，$L^{re}_o$分别使用的锚点总数。  \n",
    "\n",
    "### 3.5 Training and Implementation Details  \n",
    "\n",
    "通过使用标准的反向传播和随机梯度下降（SGD），可以对 CTPN 进行端到端训练。与 RPN类似，训练样本是锚点，其位置可以在输入图像中预先计算，以便可以从相应的实际边界框中计算每个锚点的训练标签。  \n",
    "\n",
    "**训练标签** 对于文本/非文本分类，二值标签分配给每个正（文本）锚点或负（非文本）锚点。它通过计算与实际边界框的 IoU 重叠（除以锚点位置）来定义。正锚点被定义为：（i）与任何实际边界框具有 $>0.7$ 的 IoU 重叠；或者（ii）与实际边界框具有最高 IoU 重叠。通过条件（ii），即使是非常小的文本模式也可以分为正锚点。这对于检测小规模文本模式至关重要，这是 CTPN 的主要优势之一。这不同于通用目标检测，通用目标检测中条件（ii）的影响可能不显著。负锚点定义为与所有实际边界框具有 $<0.5$ 的 IoU 重叠。$y$ 坐标回归（$\\textbf{v}*$）和偏移回归（$\\textbf{o}*$）的训练标签分别按公式（2）和（4）计算。  \n",
    "\n",
    "**训练数据** 在训练过程中，每个小批量样本从单张图像中随机收集。每个小批量数据的锚点数量固定为 $N_s=128$，正负样本的比例为 1：1。如果正样本的数量少于 64，则会用小图像块填充负样本。我们的模型在 3000 张自然图像上训练，其中包括来自 ICDAR 2013 训练集的 229 张图像。我们自己收集了其他图像，并用文本行边界框进行了手工标注。在所有基准测试集中，所有自我收集的训练图像都不与任何测试图像重叠。为了训练，将输入图像的短边设置为 600 来调整输入图像的大小，同时保持其原始长宽比。  \n",
    "\n",
    "**实现细节** 我们遵循标准实践，并在 ImageNet 数据上探索预先训练的非常深的 VGG16 模型。我们通过使用具有 0 均值和 0.01 标准差的高斯分布的随机权重来初始化新层（例如，RNN 和输出层）。该模型通过固定前两个卷积层中的参数进行端对端的训练。我们使用 0.9 的动量和 0.0005 的权重衰减。在前 16K 次迭代中，学习率被设置为 0.001，随后以 0.0001 的学习率再进行 4K 次迭代。我们的模型在 Caffe 框架中实现。  \n",
    "\n",
    "## 4 Experimental Results and Discussions  \n",
    "\n",
    "我们在五个文本检测基准数据集上评估 CTPN，即 ICDAR 2011，ICDAR 2013，ICDAR 2015，SWT 和 Multilingual数据集。在我们的实验中，我们首先单独验证每个提议组件的效率，例如细粒度文本提议检测或网内循环连接。ICDAR 2013 用于该组件的评估。  \n",
    "\n",
    "### 4.1 Benchmarks and Evaluation Metric  \n",
    "\n",
    "ICDAR 2011 数据集由 229 张训练图像和 255 张测试图像组成，图像以字级别标记。ICDAR 2013 与 ICDAR 2011 类似，共有 462 张图像，其中包括 229 张训练图像和 233 张测试图像。ICDAR 2015（Incidental Scene Text - Challenge 4）包括使用 Google Glass 收集的 1500 张图像。训练集有 1000 张图像，剩余的 500 张图像用于测试。这个数据集比以前的数据集更具挑战性，包括任意方向，非常小的尺度和低分辨率的文本。Multilingual 场景文本数据集由[24]收集。它包含 248 张训练图像和 239 张测试图像。图像包含多种语言的文字，并且真实值以文本行级别标注。Epshtein 等人引入了包含 307 张图像的 SWT 数据集，其中包含许多极小尺度的文本。  \n",
    "\n",
    "我们遵循以前的工作，使用由数据集创建者或竞赛组织者提供的标准评估协议。对于 ICDAR 2011，我们使用[30]提出的标准协议，对 ICDAR 2013 的评估遵循[19]中的标准。对于 ICDAR 2015，我们使用了由组织者提供的在线评估系统[18]。SWT 和 Multilingual 数据集的评估分别遵循[3]和[24]中定义的协议。  \n",
    "\n",
    "### 4.2 Fine-Scale Text Proposal Network with Faster R-CNN  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/4/5.jpg?raw=true) \n",
    "\n",
    "我们首先讨论我们关于 RPN 和 Faster R-CNN 系统的细粒度检测策略。如表 1（左）所示，通过产生大量的错误检测（低精度），单独的 RPN 难以执行准确的文本定位。通过使用 Fast R-CNN 检测模型完善 RPN 提议，Faster R-CNN 系统显著提高了定位精度，其 F-measure 为 0.75。一个观察结果是 Faster R-CNN 也增加了原始 RPN 的召回率。这可能受益于 Fast R-CNN 的联合边界框回归机制，其提高了预测边界框的准确性。RPN 提议可以粗略定位文本行或文字的主要部分，但根据 ICDAR 2013 的标准这不够准确。显然，所提出的细粒度文本提议网络（FTPN）在精确度和召回率方面都显著改进了 Faster R-CNN，表明通过预测一系列细粒度文本提议而不是整体文本行，FTPN 更精确可靠。  \n",
    "\n",
    "### 4.3 Recurrent Connectionist Text Proposals  \n",
    "\n",
    "我们讨论循环连接对 CTPN 的影响。如图3 所示，上下文信息对于减少误检非常有用，例如类似文本的异常值。对于恢复高度模糊的文本（例如极小的文本）来说，这非常重要，这是我们 CTPN 的主要优势之一，如图6 所示。这些吸引人的属性可显著提升性能。如表1（左）所示，使用我们的循环连接，CTPN 大幅度改善了 FTPN，将 F-measure 从 0.80 的提高到 0.88。  \n",
    "\n",
    "**运行时间** 通过使用单个 GPU，我们的 CTPN（用于整个检测处理）的执行时间为每张图像大约 0.14s，固定短边为 600。没有 RNN 连接的 CTPN 每张图像 GPU 时间大约需要 0.13s。因此，所提出的网内循环机制稍微增加了模型计算，并获得了相当大的性能增益。  \n",
    "\n",
    "### 4.4 Comparisons with state-of-the-art results  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/4/6.jpg?raw=true) \n",
    "\n",
    "我们在几个具有挑战性的图像上的检测结果如图5 所示。可以发现，CTPN 在这些具有挑战性的情况上可以完美的工作，其中一些对于许多以前的方法来说是困难的。它能够有效地处理多尺度和多语言（例如中文和韩文）。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/4/7.jpg?raw=true) \n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/4/8.jpg?raw=true) \n",
    "\n",
    "全面评估是在五个基准数据集上进行的。图像分辨率在不同的数据集中显著不同。我们为 SWT 和 ICDAR 2015 设置图像短边为 2000，其他三个的短边为 600。我们将我们的性能与最近公布的结果进行了比较。如表1 和表2 所示，我们的 CTPN 在所有的五个数据集上都实现了最佳性能。在 SWT 上，我们的改进对于召回和 F-measure 都非常重要，并在精确度上取得了很小的收益。我们的检测器在 Multilingual 上比 TextFlow 表现更好，表明我们的方法能很好地泛化到各种语言。在 ICDAR 2013 上，它的性能优于最近的 TextFlow 和 FASText，将 F-measure 从 0.80 提高到了 0.88。精确度和召回率都有显著提高，改进分别超过 $+5%$ 和 $+7%$。此外，我们进一步与[8,11,35]比较了我们的方法，它们是在我们的首次提交后发布的。它始终在 F-measure 和召回率方面取得重大进展。这可能是由于 CTPN 在非常具有挑战性的文本上具有很强的检测能力，例如非常小的文本，其中一些甚至对人来说都很难。如图6 所示，我们的检测器可以正确地检测到那些具有挑战性的图像，但有些甚至会被真实标签遗漏，这可能会降低我们的评估精度。  \n",
    "\n",
    "我们进一步调查了各种方法的运行时间，在表2 中进行了比较。FASText 达到 0.15s 每张图像的 CPU 时间。我们的方法比它快一点，取得了 0.14s 每张图像，但是在 GPU 时间上。尽管直接比较它们是不公平的，但 GPU 计算已经成为主流，最近在目标检测方面的深度学习方法上取得了很大成功。无论运行时间如何，我们的方法都大大优于 FASText，F-measure 的性能提高了 $11%$。我们的时间可以通过使用较小的图像尺度来缩短。在 ICDAR 2013 中，使用450 的缩放比例时间降低到 0.09s 每张图像，同时获得 0.92/0.77/0.84的P/R/F，与 Gupta 等人的方法相比，GPU 时间为 0.07s 每张图像，我们的方法是具有竞争力的。  \n",
    "\n",
    "## 5 Conclusions  \n",
    "\n",
    "我们提出了连接文本提议网络（CTPN）— 一种可端到端训练的高效文本检测器。CTPN 直接在卷积映射的一系列细粒度文本提议中检测文本行。我们开发了垂直锚点机制，联合预测每个提议的精确位置和文本/非文本分数，这是实现文本准确定位的关键。我们提出了一个网内 RNN 层，可以优雅地连接顺序文本提议，使其能够探索有意义的上下文信息。这些关键技术的发展带来了检测极具挑战性的文本的强大能力，同时减少了误检。通过在五个基准数据集测试中实现了最佳性能，每张图像运行时间为 0.14s，CTPN 是有效的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextBoxes++: A Single-Shot Oriented Scene Text Detector  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "场景文本检测是场景文本识别系统的一个重要步骤，也是一个具有挑战性的问题。与一般对象检测不同，场景文本检测的主要挑战在于自然图像中任意方向，小尺寸和文本纵横比的显着变化。在本文中，我们提出了一个快速的端到端可训练的场景文本检测器，名为 TextBoxes ++，它可以在单个网络正向传递中检测任意导向的场景文本，同时具有高精度和高效率。除了有效的非极大值抑制之外，不涉及其他后处理。我们在四个公共数据集上评估了建议的TextBoxes++。在所有实验中，TextBoxes++ 在文本定位精度和运行时方面优于竞争方法。更具体地说，对于 1024×1024 ICDAR 2015 附带文本图像，TextBoxes++ 在 11.6fps 下实现了 0.817 的 f-measure，对于 768×768 COCO-Text 图像，在 19.8fps下 f-measure 为 0.5591。此外，与文本识别器相结合，TextBoxes++ 明显优于最新的基于流行基准测试的词语识别和端到端文本识别任务。代码位于：https：//github.com/MhLiao/TextBoxes_plusplus。  \n",
    "\n",
    "## I. INTRODUCTION  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/5/1.jpg?raw=true) \n",
    "\n",
    "场景文本是自然场景中最常见的视觉对象之一，常常出现在道路标志，车牌，产品包等中。读取场景文本有助于实现许多有用的应用程序，例如基于图像的地理定位。场景文本检测和识别的一些应用是[1]-[4]。尽管与传统的 OCR 相似，但由于前景文本和背景对象，任意方向，高宽比以及不可控制的光照条件等的巨大变化，场景文本阅读更加具有挑战性，如[5]中所述。由于这些不可避免的挑战和复杂性，传统的文本检测方法倾向于涉及多个处理步骤，例如， 字符/词候选生成[6]-[8]，候选过滤[8]和分组[9]。他们往往最终努力让每个模块正常工作，需要很多努力来调整参数和设计启发式规则，同时也会降低检测速度。  \n",
    "\n",
    "受目标检测最新发展的启发[10,11]，我们提出通过单端神经网络直接预测四边形边界框来检测文本，该神经网络是端到端可训练的。我们称之为 TextBoxes++。具体来说，我们用常规物体检测器中的矩形框表示法替换为四边形或定向矩形表示法。此外，为了获得覆盖通常较长的文本区域的更好的接受区域，我们设计一些“长”卷积核来预测边界框。TextBoxes++ 通过联合预测文本存在并将偏移量与坐标框[11]（也称为默认框[10]）协调预测，直接输出多个图层中的单词边界框。最终输出是所有边界框上的非最大抑制输出。网络中的单个正向传递密集地检测整个图像上的多尺度文本框。因此，探测器在速度上具有很大的优势。我们将通过实验展示 TextBoxes++ 在单一尺度输入上实现高精确度和高速度，并且在多尺度输入上执行多个传递时具有更高的精度。图 1 中描绘了几个具有挑战性图像的文本检测示例。  \n",
    "\n",
    "我们进一步将 TextBoxes++ 与 CRNN 结合在一起，这是一个开源的文本识别模块。识别器不仅产生额外的识别输出，而且还通过其语义水平意识调整文本检测，从而进一步提高了字识别的准确性。TextBoxes++ 和 CRNN 的组合在字词识别和端到端文本识别任务上都具有最先进的性能，这似乎是一种简单而有效的解决方案，可用于强大的文本阅读。  \n",
    "\n",
    "这项名为 TextBoxes 的研究的初步版本已在[13]中公开。目前的论文是[13]的扩展，它扩展了 TextBoxes 的四个主要改进：1）我们将 TextBoxes（一种水平文本检测器）扩展到可以处理任意导向文本的检测器；2）重新审视和完善网络结构和训练流程，进一步提升效果；3）为了进一步展示 TextBoxes++ 在自然图像中检测任意方向文本的效率，进行了更多的比较实验；4）通过提出一种新颖的分数来优化检测和识别的组合，从而优雅地利用检测和识别信息。  \n",
    "\n",
    "本文的主要贡献有三个方面：1）所提出的面向任意方向的文本检测器 TextBoxes++ 是准确的，快速的和端到端的可训练的。与密切相关的方法相比，TextBoxes++ 有一个简单而有效的管道。2）本文还对一些重要的设计选择和其他问题进行了比较研究，包括边界框表示，模型配置以及不同评估方法的效果。这些研究的结论可能会推广到其他文本阅读算法，并对一些重要问题提供见解。3）我们还介绍了使用识别结果进一步优化检测结果的思想，这要归功于识别文本的语义级意识。据我们所知，这种直观而有效的组合还没有被开发过。  \n",
    "\n",
    "本文的其余部分安排如下。第二节简要回顾了一些有关物体检测和场景文本检测的相关工作。所提出的方法在第 III 节中详细描述。我们在第四节介绍一些实验结果。第五节给出了与一些密切相关的方法的详细比较。最后，第六节得出结论。  \n",
    "\n",
    "## II. RELATED WORKS  \n",
    "\n",
    "### A. Object detection  \n",
    "\n",
    "目标检测是计算机视觉中的一个基本问题，它旨在检测图像中的一般物体。最近，在这个主题上有两种基于 CNN 的主流方法：基于 R-CNN 的方法和基于YOLO 的方法。  \n",
    "\n",
    "a）基于 R-CNN 的目标检测：R-CNN 将检测问题视为利用卷积神经网络（CNN）进行分类的问题。它首先通过选择性搜索生成提案，然后将提案提交给 CNN 以提取基于其应用 SVM 的深度特征用于分类。Fast R-CNN通过 RoI pooling 从特征图中提取深度特征来改进 R-CNN，而不是从原始图像中裁剪。这显着简化了 R-CNN 的管道。此外，回归分支也用于 fast R-CNN 以获得更精确的边界框。Faster R-CNN 通过引入基于锚箱的端到端可训练区域提议网络来生成提议而不是使用选择性搜索，进一步提高了 fast R-CNN 的速度。生成的提案然后被送入 Fast R-CNN 网络进行检测。  \n",
    "\n",
    "b）基于 YOLO 的物体检测：原始的 YOLO 直接对整个图像的特征图上的边界框进行回归。卷积层用于预测不同区域的边界框，而不是[11]中使用的 RPN 和RoI pooling。这会导致运行时间显着缩短。另一种基于 YOLO 的方法是 SSD，该方法使用不同长宽比的默认框在不同阶段预测对象边界框。默认框的概念类似于锚箱，它被固定为相应的基本事实的参考系统。用于回归的平移和尺度不变性可以通过在每个位置使用不同长宽比和尺度的默认方框来实现，从而减轻回归问题。SSD 进一步提高了原有 YOLO 的性能，同时保持其运行时间。  \n",
    "\n",
    "### B. Text detection  \n",
    "\n",
    "一个场景文本阅读系统通常由两个主要部分组成：文本检测和文本识别。前一个组件主要以字边界框的形式定位图像中的文本。后者将单词图像裁剪成机器可解释的字符序列。在本文中，我们涵盖了两个方面，但更多关注检测。通常，大多数文本检测器可以基于原始检测目标和目标包围盒的形状，分别按照两种分类策略大致分为几类。  \n",
    "\n",
    "- a）基于原始检测目标的分类策略：大多数文本检测方法大致可以分为三类：  \n",
    "\n",
    "1）基于字符：首先检测单个字符或文本的一部分，然后将其分组为单词。一个典型的例子是文献[7]提出的方法，它通过对极值区域进行分类来定位字符，然后通过穷举搜索方法对检测到的字符进行分组。其他流行的这种类型的例子是[19]-[23]中的作品；  \n",
    "\n",
    "2）基于单词：以与通用对象检测相似的方式直接提取单词[8],[24]-[26]。在具有代表性的作品[8]中，作者提出了一个基于 R-CNN 的框架，其中候选词首先由类别不可知的提案生成器生成，随后是随机森林分类器。然后采用边界框回归的卷积神经网络对边界框进行细化。YOLO 网络在[27]中使用，它也依赖于分类器和回归步骤来消除一些误报；  \n",
    "\n",
    "3）基于文本行：检测文本行，然后分解成文字。[21]，[28]-[30]中的作品就是这样的例子。在[28]中，作者提出使用文本的对称特征来检测文本行。这个想法在[29]中被进一步利用，通过使用全卷积网络来定位文本行。  \n",
    "\n",
    "- b）基于目标包围盒形状的分类策略：根据这种分类策略，文本检测方法可以分为两类：  \n",
    "\n",
    "1）水平或接近水平：这些方法专注于检测图像中的水平或接近水平的文本。在[31]中提出了基于 AdaBoost 的算法。然后，Yi 等人提出了一个三阶段框架，它由边界聚类，stroke 分割和字符串片段分类组成。一些受物体检测方法启发的例子是[8]，[27]。它们都使用水平矩形边界框作为预测目标，这与一般对象检测方法非常相似。另一种这种类型的流行方法是[33]中的工作，它检测几乎水平的文本部分，然后将它们链接在一起以形成候选词。此外，曹等人尝试使用去模糊技术来获得更强大的检测结果。  \n",
    "\n",
    "2）多方向：与水平或接近水平检测相比，多方向的文本检测更加稳健，因为场景文本可以在图像中任意定向。有几个作品试图检测图像中的多方向文字。在文献[19]中，作者提出了两组旋转不变特征来检测多面向文本。第一组是特征计算之前的估计中心，尺度，方向等组件级特征。第二个是链级特征，如尺寸变化，颜色自相似性和结构自相似性。Kang 等人建立 MSER 图，然后进行高阶相关聚类以生成多方向候选。文献[37]提出了一个用于多向文本检测和识别的统一框架。他们使用相同的功能进行文本检测和识别。最后，使用基于纹理的纹理分类器来区分文本和非文本候选。在[30]，[38]中，由专用分割网络给出的文本显着图生成了多向文本边界框。最近，Shi 等人建议用段和链接来检测文本。更确切地说，他们首先检测到一些名为段的文本部分，同时预测相邻段之间的链接关系。然后将相关的段连接在一起以形成文本边界框。U 形全卷积网络用于检测多方向的文本。在这项工作中，作者还介绍了 PVANet 的效率。四边形滑动窗口，一种 Monte-Carlo 方法和 smooth Ln loss 在文献[42]中被提出来检测多方向文本，这在复杂的情况下是有效的。  \n",
    "\n",
    "### C. TextBoxes++ Versus some related works  \n",
    "\n",
    "TextBoxes++ 是一个基于文字和多方向的文本检测器。与[8]相比，它由三个检测步骤组成，每个步骤都包含多个算法，TextBoxes++ 的管道更加简单。只需要一个网络的端到端培训。如第 II-B 部分所述，Tian 等人和 Shi 等人建议检测文本部分，然后将它们连接在一起。对于长词而言，他们都取得了不俗的成绩。然而，文献[33]中提出的方法由于双向长期短期记忆（BLSTM）的单一方向对面向文本的适应性有限，并且[39]中的工作有两个超级参数用于链接段这是由网格搜索确定的，难以调整。文献[40]中的方法被认为是当前最先进的方法。它依靠 U 形网络同时生成文本分割和边界框的分数图。然而，准确的文本区域分割本身就具有挑战性。此外，文本区域分割中涉及的额外金字塔式解卷积层需要额外的计算。而 TextBoxes++ 直接对卷积特征映射上的默认框进行分类和回归，其中与分割分数映射相比，保留了更丰富的信息。TextBoxes++ 因此简单得多，避免了金字塔式解卷积上的时间浪费。  \n",
    "\n",
    "与 TextBoxes++ 最相关的工作之一是 SSD，这是最近在对象检测领域的一个发展。事实上，TextBoxes++ 受 SSD 的启发。原始 SSD 旨在检测图像中的一般物体，但无法识别具有极高宽高比的字体。TextBoxes++ 依赖于专门设计的文本框图层来有效地解决这个问题。这会显着提高性能。此外，SSD 只能根据水平矩形生成边界框，而 TextBoxes++ 可以根据定向矩形或一般四边形生成任意方向的边界框，以处理文本。  \n",
    "\n",
    "另一个与 TextBoxes++ 密切相关的工作是[42]中的方法，该方法提出了四边形滑动窗口和用于检测面向文本的 Monte-Carlo 方法。在TextBoxes++ 中，我们使用水平矩形作为默认框，因此每个区域中的默认框少得多。受益于横向矩形默认框，TextBoxes++ 还可以使用更简单的策略来匹配默认框。此外，TextBoxes++ 同时对边界框和四边形边界框的最大水平矩形进行回归，这使得训练更加稳定。此外，我们通过结合检测和识别分数来提出新的评分，以进一步优化检测结果。  \n",
    "\n",
    "文本检测的最终目标是识别单词或识别图像中的文本。另一方面，发现的单词或识别单词的语义级意识也有助于进一步规范文本检测结果。遵循这个想法，我们建议将文本识别器与 TextBoxes++ 结合起来进行单词识别和端到端识别，并使用识别单词的置信度来规范 TextBoxes++ 的检测输出。为此，我们采用称为 CRNN 的最先进的文本识别器，其直接输出给定输入图像的字符序列，并且也是端到端可训练的。其他文字识别器如[8]也适用。这种用于单词识别和端到端识别的简单流水线与传统流水线截然不同。  \n",
    "\n",
    "## III. DETECTING ORIENTED TEXT WITH TEXTBOXES++  \n",
    "\n",
    "### A. Overview  \n",
    "\n",
    "TextBoxes++ 依靠端到端的可训练全卷积神经网络来检测任意方向的文本。其基本思想受到[10]中提出的物体检测算法 SSD 的启发。我们提出了一些特殊的设计，以适应 SSD 网络，以有效检测自然图像中的面向文本。更具体地说，我们建议用四边形[40]，[44]或定向矩形[45]来表示任意方向的文本。然后，我们调整网络以预测从默认框到以四边形或方向矩形表示的面向文本的回归。为了更好地覆盖某些区域可能密集的文本，我们建议使用垂直偏移来使默认框更加致密。此外，我们调整卷积核以更好地处理与通用对象检测相比通常是长对象的文本行。这些网络适配在第 III-B 节中详细介绍。第 III-C 部分介绍了针对任意方向文本检测的一些特定训练，包括在线 hard negative mining 和通过专为较小文本设计的新颖随机裁剪策略进行数据增强。TextBoxes++ 可以在 6 个阶段中检测 6 种不同尺度方向的文本。在测试阶段，多级检测结果通过基于四边形或方向矩形的 IOU 阈值的有效级联非最大抑制合并在一起（见第III-D节）。最后，我们还提出了一种直观而有效的想法，即利用识别文本的语义级意识，使用文本识别来进一步优化检测结果。这在第 III-E 节中讨论。 \n",
    "\n",
    "### B. Proposed network  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/5/2.jpg?raw=true) \n",
    "\n",
    "1）网络体系结构：TextBoxes++ 的体系结构如图 2 所示。它继承了流行的 VGG-16 体系结构，保留了来自 conv1_1 到 conv5_3 的层，并通过参数下采样将 VGG-16 的最后两个全连接层转换为卷积层（conv6和conv7）。另外 8 个卷积层在 conv7 之后被附加，分为四个阶段（conv8 到 conv11），通过 max-pooling 具有不同的分辨率。多个输出图层，我们称之为文本框图层，被插入到最后和一些中间卷积图层之后。它们也是卷积层来预测聚合输出，然后进行有效的非极大值抑制（NMS）过程。综上所述，TextBoxes++ 是一种全卷积结构，仅由卷积层和池化层组成。因此，TextBoxes++ 可以在训练和测试阶段适应任意大小的图像。与本文[13]中的初步研究相比，TextBoxes++ 用卷积层替换了最后一个全局平均池化层，这对于多尺度训练和测试更有利。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/5/3.jpg?raw=true) \n",
    "\n",
    "2）垂直偏移的默认框：文本框图层是TextBoxes++ 的关键组件。文本框图层同时预测文本的存在和边界框，以输入的特征图为条件。TextBoxes++ 的输出边界框包括定向边界框 {q} 或 {r} 和包含相应定向边界框的最小水平边界矩形 {b}。这可以通过预测每个位置的一些预先设计的水平默认框的偏移量的回归来实现（例如，参见图 3）。更准确地说，让 $b_0 =(x_0,y_0,w_0,h_0)$ 表示一个水平的默认框，它也可以写为 $q_0 =(x_{01}^q, y_{01}^q, x_{02}^q, y_{02}^q, x_{03}^q, y_{03}^q, x_{04}^q, y_{04}^q)$ 或 $r_0 =(x_{01}^r, y_{01}^r, x_{02}^r, y_{02}^r, h_0^r)$，其中 $(x_0,y_0)$ 表示默认框的中心点，w0 和 h0 表示默认框的宽度和高度。q0，r0 和 b0 之间的关系如下：  \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "x_{01}^q = x_0 - w_0/2, \\ y_{01}^q  = y_0 - h_0/2 \\\\\n",
    "x_{02}^q = x_0 + w_0/2, \\ y_{02}^q  = y_0 - h_0/2 \\\\\n",
    "x_{03}^q = x_0 + w_0/2, \\ y_{03}^q  = y_0 + h_0/2 \\\\\n",
    "x_{04}^q = x_0 - w_0/2, \\ y_{04}^q  = y_0 + h_0/2 \\\\\n",
    "x_{01}^r = x_0 - w_0/2, \\ y_{01}^r  = y_0 - h_0/2 \\\\\n",
    "x_{02}^r = x_0 + w_0/2, \\ y_{02}^r  = y_0 - h_0/2 \\\\\n",
    "h_0^r = h_0\n",
    "\\end{align} \\tag{1}\n",
    "$\n",
    "\n",
    "在特征图每个位置上，它以卷积方式将分类分数和偏移量输出到每个关联的默认框，表示为 q0 或 r0。对于定向文本的四边形表示，文本框图层预测 $(\\Delta x,\\Delta y,\\Delta w,\\Delta h,\\Delta x_1,\\Delta y_1,\\Delta x_2,\\Delta y_2,\\Delta x_3,\\Delta y_3,\\Delta x_4,c)$ ，表示在置信度 c 下检测到水平矩形 $b = (x,yw,h)$ 和四边形 $q = (x_1^q,y_1^q,x_2^q,y_2^q,x_3^q,y_3^q,x_4^q,y_4^q)$：  \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "x = x_0 + w_0 \\Delta x \\\\\n",
    "y = y_0 + h_0 \\Delta y \\\\\n",
    "w = w_0 \\exp(\\Delta w) \\\\\n",
    "h = h_0 \\exp(\\Delta h) \\\\\n",
    "x_n^q = x_{0n}^q + w_0 \\Delta x_n^q, \\ n = 1,2,3,4 \\\\\n",
    "y_n^q = y_{0n}^q + h_0 \\Delta y_n^q, \\ n = 1,2,3,4 \\\\\n",
    "\\end{align} \\tag{2}\n",
    "$\n",
    "\n",
    "当使用旋转矩形的表示时，文本框层预测 $(\\Delta x,\\Delta y,\\Delta w,\\Delta h,\\Delta x_1,\\Delta y_1,\\Delta x_2,\\Delta y_2,\\Delta h^r,c)$ 的值，并且旋转矩形 $r = (x_1^r,y_1^r,x_2^r,y_2^r,h^r)$ 计算如下：  \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "x_n^r = x_{0n}^r + w_0 \\Delta x_n^r, \\ n = 1,2 \\\\\n",
    "y_n^r = y_{0n}^r + h_0 \\Delta y_n^r, \\ n = 1,2 \\\\\n",
    "h^r = h_0^r \\exp(\\Delta h_r)\n",
    "\\end{align} \\tag{3}\n",
    "$\n",
    "\n",
    "在训练阶段，根据[10]中的匹配方案，根据盒子重叠将真实字框与默认框匹配。如图 3 所示，旋转矩形或四边形的最小边界水平矩形用于匹配默认框以提高效率。请注意，每个位置都有许多具有不同宽高比的默认方框。通过这种方式，我们可以通过纵横比有效地划分单词，从而允许 TextBoxes++ 学习处理类似高宽比的单词的特定回归和分类权重。因此，默认框的设计高度针对特定任务。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/5/4.jpg?raw=true) \n",
    "\n",
    "与一般物体不同，单词倾向于具有较大的宽高比。因此，[13] 中的 TextBoxes 的初步研究包括具有大纵横比的“长”默认框。具体而言，对于水平文本数据集，我们为默认框定义了 6 个高宽比，包括 1,2,3,5,7 和 10 不过，TextBoxes++ 旨在检测任意方向的文本。因此，我们将默认框的长宽比设置为1,2,3,5,1/2,1/3,1/5。此外，文本通常在某个区域密集，因此每个默认框都设置了一个垂直偏移量以更好地覆盖所有文本，这使得垂直方向上的默认框密集。一个例子如图 4 所示。在图4（a）中，正常的默认方框（黑色虚线）在中间不能同时处理靠近它的两个单词。在这种情况下，如果没有应用垂直偏移量，检测时则会丢失一个单词。在图4（b）中，普通默认方框根本不覆盖底部单词，这也表明使用具有垂直偏移的默认方框的必要性。  \n",
    "\n",
    "3）卷积层：在文[13]中对水平文本检测的初步研究中，我们在文本框图层中采用了不规则的 1×5 卷积滤波器而不是标准的 3×3 卷积滤波器。这是因为自然图像中的单词或文本行通常是长对象。但是，这些长卷积滤波器不适用于多方向文本。相反，我们使用 3×5 卷积滤波器来定向文本。这些 inception-style[47]过滤器使用矩形接受域，它更适合具有较大纵横比的文字。由于这些原因，也避免了方形感受野所带来的噪音信号。  \n",
    "\n",
    "### C. Adapted training for arbitrary-oriented text detection  \n",
    "\n",
    "1）真实值表示：对于第 III-B 节中描述的两个目标框表示，我们调整定向文本的真实值表示如下：  \n",
    "\n",
    "- a）四边形：对于每个定向文本的真实值 T，设 $G_b = (\\tilde x_0^b, \\tilde y_0^b, \\tilde w_0^b, \\tilde h_0^b)$ 为其水平矩形真实值（即最小水平矩形包围 T），其中 $(\\tilde x_0^b, \\tilde y_0^b)$ 为 Gb 的中心，$(\\tilde w_0^b, \\tilde h_0^b))$ 分别为 Gb 的宽度和高度。根据公式(1) 这个矩形的 ground truth 也可以表示为 $G_b = (b_1,b_2,b_3,b_4)$。我们使用定向文本 ground truth T 的四个顶点 $G_q =(q_1,q_2,q_3,q_4) = (\\tilde x_1^q, \\tilde y_1^q,\\tilde x_2^q, \\tilde y_2^q,\\tilde x_3^q, \\tilde y_3^q,\\tilde x_4^q, \\tilde y_4^q)$ 来表示 T。四个顶点 $(q_1,q_2,q_3,q_4)$ 也按顺时针顺序组织，使得四个点对 $(b_1,q_1)$ 之间的欧几里得距离之和是最小的。更准确地说，令 $(q'_1,q'_2,q'_3,q'_4)$ 按顺时针顺序表示相同的四边形 $G_q$，其中 $q'_1$ 是顶点（在 Gq 是矩形的情况下的左上角点）。那么 $q$ 和 $q'$之间的关系由下式给出：  \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "d_{i,\\Delta} = d_E(b_i,q'_{(i+\\Delta -1) \\% 4 + 1}) \\ \\Delta = 0,1,2,3 \\\\\n",
    "\\Delta_m = arg {min}_{\\Delta} (d_{1,\\Delta} + d_{2,\\Delta} + d_{3,\\Delta} + d_{4,\\Delta}) \\\\\n",
    "q_i = q'_{(i+\\Delta_m -1)\\% 4 + 1}\n",
    "\\end{align} \\tag{4}\n",
    "$\n",
    "\n",
    "  其中 $d_E(b,q')$ 是两点之间的欧几里德距离，$\\Delta_m$ 是给出 Gb 和 Gq 之间的四对对应点的距离的最小和的点的移动。  \n",
    "\n",
    "- b）旋转矩形：旋转矩形有几种不同的表示。一个流行的是由一个水平矩形和一个旋转角度，可以写成 $(x,y,w,h,\\theta)$。然而，由于数据集的偏倚，$\\theta$ 通常存在不均匀分布，这可能使模型依赖于数据集。为了缓解这个问题，我们建议使用[45]中提出的另一个表达式来表示真实旋转矩形 $G_r: G_r = (\\tilde x_1^r, \\tilde y_1^r, \\tilde x_2^r, \\tilde y_2^r,h^r)$，其中 $(\\tilde x_1^r, \\tilde y_1^r)$ 和 $(\\tilde x_2^r, \\tilde y_2^r)$ 是真实值的第一个和第二个顶点（即 Gq 的第一个和第二个顶点），$h^r$ 是旋转矩形的高度。  \n",
    "\n",
    "2）损失函数：我们采用类似于[10]中使用的损失函数。更具体地说，令 x 是匹配指示矩阵。 对于第 i 个默认方框和第 j 个真实框，$x_{ij} = 1$ 意味着它们之间的方框重叠之后的匹配，否则 $x_{ij} = 0$。令 $c$ 为置信度，$l$ 为预测位置，$g$ 为真实的位置。损失函数被定义为：  \n",
    "\n",
    "$L(x,c,l,g) = \\frac {1}{N} (L_{conf}(x,c) + \\alpha L_{loc}(x,l,g)) \\tag{5}$\n",
    "\n",
    "其中 N 是匹配 ground-truth 框的默认框的数量，并且为了快速收敛，$\\alpha$ 被设置为 0.2。 我们采用 $L_{loc}$ 的 smooth L1 损失和 $L_{conf}$ 的 2-class soft-max 损失。  \n",
    "\n",
    "3）在线 hard negative mining：一些纹理和符号与文本非常相似，很难区分网络。我们遵循[10]中使用的 hard negative mining 策略来抑制它们。更确切地说，相应数据集的训练分为两个阶段。第一阶段的负值和正值之间的比率设置为 3：1，然后在第二阶段更改为 6：1。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/5/5.jpg?raw=true) \n",
    "\n",
    "4）数据增强：与许多基于 CNN 的视觉问题类似，数据增强是增加训练集有限大小的经典和必要的方法。例如，在 crops 和真实值之间基于最小 Jaccard 重叠的随机 crop 增加在[10]中应用。但是，这种策略通常并不适用于很小的文本。这是因为 Jaccard 重叠约束难以满足小物体的要求。如图5（a-b）中的例子所示，对于小对象，即使 Jaccard 重叠得到满足，数据增大后调整大小的图像中的对象非常大，几乎覆盖整个图像。这不是自然图像中文本的常见情况。为了缓解这个问题，除了 Jaccard 重叠外，我们还建议添加一个名为 object coverage 的新重叠约束。对于裁剪边界框 B 和真值边界框 G，Jaccard 重叠 J 和对象覆盖 C 定义如下：  \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "J= |B \\cap G| / |B \\cup G|, \\\\\n",
    "C = |B \\cap G| / |G|\n",
    "\\end{align} \\tag{6}\n",
    "$\n",
    "\n",
    "在哪里$|\\cdot|$表示基数（即区域）。基于对象覆盖 C 的随机裁剪策略更适合小型对象，如自然图像中的大多数文本。 图5（c-d）给出了一个例子。在本文中，我们使用随机 crop 策略，将最小重叠或覆盖阈值随机设置为 0,0.1,0.3,0.5,0.7 和 0.9。请注意，设置为 0 的阈值意味着既不使用最小Jaccard 重叠也不使用对象覆盖约束。然后将每个裁剪区域的大小调整为固定大小的图像，并将其输入网络。  \n",
    "\n",
    "5）多尺度训练：为了提高训练速度，随机裁剪的区域被调整为尺寸相对较小的图像。然而，由于其全卷积体系结构，所提出的 TextBoxes++ 的输入图像可以具有任意大小。为了更好地处理多尺度文本，我们还在训练阶段的最后几千次迭代中使用较大的尺度输入尺寸。训练细节在第 IV-B 节中讨论。  \n",
    "\n",
    "### D. Testing with efficient cascaded non-maximum suppression  \n",
    "\n",
    "类似于经典的对象检测方法，我们在测试期间应用非极大值抑制（NMS）来提取任意方向文本的预测框。首先，我们将六个多尺度预测结果重新调整为输入图像的原始尺寸，并将这些调整后的结果融合成一个稠密的置信图。然后 NMS 操作应用于此合并的置信图。由于 NMS 在四边形或旋转矩形上的操作比在水平矩形上耗费更多时间，因此我们将此 NMS 操作分成两步以加速速度。首先，我们在包含预测四边形或旋转矩形的最小水平矩形上应用具有较高 IOU 阈值（例如0.5）的 NMS，水平矩形上的这种操作更省时并且移除许多候选框。然后，将四边形或旋转矩形上耗时的 NMS 应用于几个剩余候选框，使用较低 IOU 阈值（例如0.2）的。在第二次 NMS 操作之后的剩余方框被视为最终检测到的文本框。这种级联非最大抑制比直接在四边形或旋转矩形上应用 NMS 要快得多。  \n",
    "\n",
    "### E. Word spotting, end-to-end recognition, and detection refining  \n",
    "\n",
    "直观上，文本识别器将有助于消除一些不太可能是有意义的单词的假阳性检测结果，例如，重复的模式。特别是，当存在词典时，文本识别器可以有效地移除与任何给定词语不匹配的检测到的边界框。遵循这个直观的想法，我们建议通过词语识别和端到端识别来改进 TextBoxes++ 的检测结果。  \n",
    "\n",
    "1）单词识别和端到端识别：单词识别是对词典中给出的特定单词进行识别。端到端识别涉及检测和识别。只需将 TextBoxes++ 与文本识别器连接即可实现这两项任务。我们采用 CRNN 模型作为我们的文本识别器。CRNN 使用 CTC 作为其输出层，其估计以输入图像 I 为条件的序列概率，表示为 $p(w|I)$，其中 w 表示字符序列输出。如果没有给出词典，则将 w 视为识别的词，并且概率 $p(w|I)$ 测量图像与该特定词 w 的兼容性。CRNN 也支持使用词典。对于给定的词典 W，CRNN 输出测量输入图像 I 如何匹配每个词 #w \\in W# 的概率。我们在下面定义识别分数 $s_r$：  \n",
    "\n",
    "$ s_r = \\begin{cases} p(w|I), & \\text {Without lexicon} \\\\ {max}_{w \\in W} p(w|I), & \\text{With lexicon W} \\end{cases} \\tag{7}$\n",
    "\n",
    "请注意，在所提出的方法中使用词典不是必需的。我们只使用词典与其他方法进行公平比较  \n",
    "\n",
    "2）使用识别修正检测：我们建议通过将识别分数 $s_r$ 与原始检测 $s_d$ 分数相结合来改进识别检测。在实践中，识别得分的价值通常与检测得分的价值没有可比性。例如，检测分数 $s_d$ 的阈值通常被设置为 0.6，并且识别分数 $s_r$ 的阈值通常被设置为0.005。这两个分数的微不足道的组合会导致检测分数的严重偏差。在本文中，我们建议定义得分 S 如下：  \n",
    "\n",
    "$S = \\frac {2 \\times} e^{s_d + s_r}{e^{s_d} + e^{s_r}} \\tag{8}$\n",
    "\n",
    "方程有两个动机，首先，我们使用指数函数来使两个分数值具有可比性。然后，采用调和平均值来获得最终的综合评分。这个组合分数 S 比分别对两个分数应用网格搜索更方便。  \n",
    "\n",
    "## IV. EXPERIMENTS  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/5/6.jpg?raw=true) \n",
    "\n",
    "继承自对象检测，所有现有的场景文本检测基准都依赖于 IOU 阈值来评估文本检测器的性能。然而，由于检测文本的最终目的是文本识别，因此文本检测与对象检测完全不同。文本识别器可能会以相同的 IOU 产生完全不同的结果。例如，图6（a-c）中的三个检测结果具有相同的 IOU。然而，由于缺少文本部分，图6（a）和图6（b）中的检测结果未能正确识别底层文本。由于全文覆盖，图6（c）中的检测结果倾向于给出准确的识别结果。因此，除了经典的文本检测基准之外，我们还使用文本检测结果进行单词识别和端到端识别实验，以进一步演示 TextBoxes++ 的性能。  \n",
    "\n",
    "### A. Datasets & evaluation protocol  \n",
    "\n",
    "建议的 TextBoxes++ 检测任意方向的文本。我们已经在包括面向文本的两个数据集上测试了它的性能：ICDAR 2015 Incidental Text（IC15）数据集和COCO-Text 数据集。为了进一步证明 TextBoxes++ 的多功能性，我们在两个流行的水平文本数据集上进行了实验：ICDAR 2013（IC13）数据集和街景文本（SVT）数据集。除这些基准数据集外，SynthText 数据集也用于预训练我们的模型。下面给出所有这些相关数据集的简短描述（更多详细信息，请参阅相应的参考资料）。  \n",
    "\n",
    "**SynthText**：SynthText 数据集包含 800k 个合成文本图像，通过混合渲染字与自然图像创建。由于文本的位置和变换是通过学习算法仔细选择的，因此合成图像看起来很逼真。该数据集用于预先训练我们的模型。  \n",
    "\n",
    "**ICDAR 2015 Incidental Text (IC15):** ICDAR 2015 年强力阅读竞赛挑战赛 4 中的 ICDAR 2015 Incidental Text 数据集。该数据集由 1000 个训练图像和 500 个测试图像组成，这些图像由 Google glasses 以较低分辨率拍摄。每个图像可能包含多方向文本。根据单词边界框提供注释。这个数据集还提供了 3 个不同大小的词库，用于词语识别和端到端识别挑战：1）强大的词典，每个测试图像可以给出 100 个单词作为单个词典；2）整个测试集中包含数百个单词的弱词汇；3）带有 90k 字的通用词典。  \n",
    "\n",
    "COCO-Text：COCO-Text 数据集是目前用于场景文本检测和识别的最大数据集。它包含 43686 个训练图像和 20000 个用于验证/测试的图像。COCO-Text 数据集非常具有挑战性，因为此数据集中的文本是任意方向的。这个困难也适用于本文中不如其他测试数据集准确的注释。因此，即使这个数据集提供了定向注释，它的标准评估协议仍然依赖于水平边界矩形。对于TextBoxes++，我们利用水平边界矩形和四边形注解来训练我们的模型。为了评估，我们遵循基于水平边界矩形的标准协议。  \n",
    "\n",
    "ICDAR 2013（IC13）：ICDAR 2013 数据集包含 229 个不同分辨率的训练图像和 233 个测试图像。该数据集只包含水平或接近水平的文本。该数据集的词典设置与之前描述的 IC15 数据集相同。  \n",
    "\n",
    "街景文本（SVT）：由于图像分辨率较低，SVT 数据集比以前的 ICDAR 2013 数据集更具挑战性。在 SVT 数据集中有 100 个训练图像和 250 个测试图像。 图像只有水平或接近水平的文字。每个图像还提供一个包含 50 个单词的词典。请注意，并非所有数据集中的文本都被标记。因此，此数据集仅用于单词识别评估。  \n",
    "\n",
    "评估协议：用于文本检测，单词识别和端到端识别的经典评估协议都依赖于精度（P），召回（R）和 f 度量（F）。他们是由:  \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "P = \\frac {TP}{TP +FP} \\\\\n",
    "R = \\frac {TP}{TP +FN} \\\\\n",
    "F = 3 \\times \\frac {P \\times R}{P + R}\n",
    "\\end{align} \\tag{9}\n",
    "$\n",
    "\n",
    "其中 TP，FP 和 FN 分别是命中盒数，错误识别盒数和错过盒数。对于文本检测，如果 b 与真实框之间的 IOU 大于给定阈值（通常设置为0.5），则将检测到的框 b 视为命中框。点字识别和端到端识别中的命中框不仅需要相同的 IOU 限制，而且还需要正确的识别结果。由于精确度和召回率之间存在折衷，因此 f-度量衡是评估绩效最常用的衡量标准。  \n",
    "\n",
    "### B. Implementation details  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/5/7.jpg?raw=true) \n",
    "\n",
    "TextBoxes++ 是用 Adam 训练的。整个训练过程由三个阶段组成，如表格I 所示。首先，我们在 SynthText 数据集上预训练 TextBoxes++。然后在每个数据集的相应训练图像上继续训练。最后，我们继续使用较小的学习率和较大负样本比例继续进行训练。还要注意的是，在最后的训练阶段，使用更大的输入图像尺寸来更好地检测多尺度文本。对于所有测试数据集，预训练步骤的迭代次数固定为 60k。然而，这个数字在每个数据集自己的训练图像上进行的第二和第三训练阶段不同。这种差异取决于不同的数据集大小。  \n",
    "\n",
    "文本识别是使用预先训练的 CRNN 模型来执行的，该模型由本文作者实施和发布  \n",
    "\n",
    "本文中介绍的所有实验都是在装有单个 Titan Xp GPU 的 PC 上进行的。在 ICDAR 2015 Incidental Text 数据集中，整个训练过程（包括 SynthText 数据集上的预训练）大约需要 2 天，这是目前测试最多的数据集。  \n",
    "\n",
    "### C. Quadrilateral VS Rotated Rectangle  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/5/8.jpg?raw=true) \n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/5/10.jpg?raw=true) \n",
    "\n",
    "旋转的矩形是四边形的近似简化，其在表示任意方向的文本边界框中更加灵活。尽管这两种表示法可能同样适用于普通文本字体和样式，但四边形表示法可能更适合调整大小的图像。图 7 给出了一个例子，其中原始图像中由四边形和旋转矩形表示的边界框（参见图7（a-b））几乎相同。但是，如图7（c-d）所示，旋转后的矩形与调整大小的图像中的四边形的文本匹配程度较差。这是因为直接调整大小时，旋转后的矩形通常会变成平行四边形，导致尝试将其保留为旋转矩形时出现小偏差。从这个意义上说，具有四边形表示的 TextBoxes++ 比使用旋转矩形的变体更精确。  \n",
    "\n",
    "我们在广泛测试的 ICDAR 2015 Incidental 文本数据集上进行了实验，以比较这两种 TextBoxes++ 的变体；定量比较在表II 中给出。在标准文本检测评估之后，使用四边形表示的 TextBoxes++ 明显优于使用旋转矩形的版本，至少 2.5% 改进。还要注意，使用多尺度输入可以改进两个版本的 TextBoxes++。四边形版本仍然表现更好，特别是当匹配评估的 IOU 阈值设置为 0.7 时。此外，在如此高的 IOU 阈值设置下，四边形版本和带多尺度输入的旋转矩形版本之间的差异更为显着。这是因为对于较高的 IOU 阈值设置，预计会有更准确的文本检测器。这证实了四边形表示比 TextBoxes++ 的旋转矩形更准确。因此，我们在本文中选择使用四边形表示的 TextBoxes++ 作为剩余实验，并且在不存在歧义时将其简单地表示为 TextBoxes++。TextBoxes++\\_MS 代表具有多尺度输入的这个版本的 TextBoxes++。  \n",
    "\n",
    "### D. Text localization  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/5/9.jpg?raw=true) \n",
    "\n",
    "1）性能：我们首先评估了两种流行的面向文本数据集上提出的 TextBoxes++，以评估它在自然图像中处理任意方向文本的能力。为了进一步验证 TextBoxes++ 的多功能性，我们还测试了两个广泛使用的水平文本数据集。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/5/11.jpg?raw=true) \n",
    "\n",
    "**Oriented Text Dataset:** 第一个测试的定向文本数据集是广泛使用的 ICDAR 2015 附带文本数据集。一些定性的比较如图8 所示。如图所示，TextBoxes++ 比检测各种尺度的定向文本的竞争方法更加健壮。标准评估方案后的定量结果见表III。具有单输入比例的 TextBoxes++ 优于所有最先进的结果。更具体地说，当在两种方法中使用单一比例输入时，TextBoxes++ 将方法状态提高 3.5%。此外，TextBoxes++ 的单比例版本仍然比[40]的多比例版本低 1.0%。请注意，使用多尺度输入的 TextBoxes++ 可以获得更好的性能。  \n",
    "\n",
    "TextBoxes++ 也大大超过了 COCO-Text 数据集上的最新方法，其最新注释v1.4。如表IV 中所示，TextBoxes++ 在单一比例输入下胜过竞争方法至少16%。 此外，使用多尺度输入时，TextBoxes++ 的性能提高了 2.81%。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/5/12.jpg?raw=true) \n",
    "\n",
    "**Horizontal Text Dataset:** 我们还对 ICDAR 2013 数据集用 TextBoxes++ 进行了评估，该数据集是最受欢迎的水平文本数据集之一。表V 中描述了与一些现有技术方法的比较。请注意，在此数据集上评估的方法很多，但只显示了一些最佳结果。与此数据集上的[63]相比，TextBoxes++ 实现了至少1.0% 的改进。然而，Tang 等人使用包含两个网络的级联架构，每个图像需要 1.36 秒。而且，它只能检测水平文本。我们还将 TextBoxes++ 与一个最先进的通用对象检测器 SSD 进行了比较，这是最相关的方法。对于这种比较，使用 TextBoxes++ 的相同训练过程来训练 SSD。如表V 中所报道。这种直接适用于文本检测的 SSD 不如最先进的方法。特别是，我们观察到 SSD 无法正确检测宽高比较大的文字。TextBoxes++ 的表现要好得多，这要归功于为克服单词长度变化而设计的文本框图层。与 TextBoxes 相比，TextBoxes++ 通过单一尺度实现了几乎相同的性能，并且由于 TextBoxes++ 中采用了多尺度训练策略，因此多尺度下性能更好。但是，请注意，此数据集上的实验旨在验证 TextBoxes++ 尽管专用于面向任意方向的文本检测，但与专门为水平文本检测而设计的初步研究 TextBoxes 相比没有性能损失。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/5/13.jpg?raw=true) \n",
    "\n",
    "2）运行时间：TextBoxes++ 不仅精确而且高效。我们已将其运行时间与 ICDAR 2015 Incidental Text 数据集中的最新方法进行了比较。如表 6 所示，TextBoxes++ 以 11.6 fps 实现了 0.817 的 F-measure，与其他竞争方法相比，在运行时和性能方面具有更好的平衡性。请注意，Tian 等人提出的 ss-600方法意味着图像的短边被调整到 600。对于这种方法，ICDAR 2015 Incidental 数据集的最佳结果是使用 2000 的短边给出的，这会导致此方法的运行时间更慢。对于 Zhang 等等人，MS 表示他们在 MSRA-TD500 数据集上使用了三个标度（即200,500,1000）。在[40]中提出的方法与 PVANet 一起以 16.8 fps 执行，与 VGG-16 相比，速度更快。但是，性能比 TextBoxes++ 低 6 个百分点。为了提高性能，作者将 PVANet 的通道数量加倍，导致运行时间为13.2 fps。TextBoxes++ 具有类似的运行时间，但性能提高了 3.5%。此外，当应用相同的主干（VGG-16）时，[40]中的方法要低得多，仍然不如 TextBoxes++。对于 Shi 等人，报告的运行时间在 768×768 MSRA-TD 500 图像上进行测试，但报告的性能是通过 720×1280 ICDAR 2015 附带的文本图像实现的。请注意，768×768 COCO-Text 图像上 TextBoxes++ 的运行时间为 19.8 fps。TextBoxes++ MS 通过四种输入比例（384×384,768×768,1024×1024,1536×1536）达到约 2.3 fps。  \n",
    "\n",
    "### E. Word spotting and end-to-end recognition to refine text detection  \n",
    "\n",
    "1）词识别和端到端识别：在第四节开头，我们讨论了依赖于经典 IOU 阈值设置的标准文本检测评估协议的局限性。没有正确识别的情况下只检测文本是没有意义的。从这个意义上讲，基于文本检测最终目的的评估将进一步评估文本检测的质量。为此，我们还在字识别和端到端识别的框架中对提议的文本检测器 TextBoxes++ 和最近的文本识别器 CRNN 模型进行了评估。请注意，尽管单词识别与端到端识别类似，但单词识别和端到端识别的评估稍有不同。对于单词识别，只需要检测和识别一些指定的单词，这意味着单词识别通常比端到端识别更容易。我们测试了 TextBoxes++ 的流水线，接着是 CRNN 模型，对三种流行词识别或端对端识别基准数据集进行了测试：ICDAR 2015 Incidental Text 数据集，SVT 数据集和 ICDAR 2013 数据集。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/5/14.jpg?raw=true) \n",
    "\n",
    "**Oriented text datasets:** 由于 TextBoxes++ 可以在自然图像中检测任意方向的文本，我们首先评估它在 ICDAR 2015 Incidental 文本数据集中的词语识别和端到端识别。图9（a）给出了一些定性结果。一般来说，所提出的流水线正确识别大多数方向文本。表中描述了与其他竞争方法的定量比较。七。请注意，此表中竞争方法尚未发表论文。这些结果在 ICDAR 2017 比赛网站上公布，并且只显示了一些最好的结果。我们的方法在词汇识别和端到端识别的所有强，弱和通用词典下显着优于其他方法。更具体地说，对于强词汇，所提出的方法胜过两种任务的最佳竞争方法 6%。对于弱词汇，所提出的方法将最先进的结果提高了 6% 的字词识别率和 4% 的端对端识别率。当使用通用词典时，这种改进不太显着（分别为两个任务的 2.7% 和 1.2%）。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/5/15.jpg?raw=true) \n",
    "\n",
    "**Horizontal text datasets:** 我们还评估了在两个水平文本数据集上进行单词识别和端到端识别的方法：ICDAR 2013 数据集和 SVT 数据集。图9 描述了 ICDAR 2013 数据集的一些定性结果。一般来说，无论大小，纵横比，字体和复杂背景如何，TextBoxes++ 都可以在各种场合实现良好效果。表8 中给出了一些定量结果。我们的方法胜过最先进的方法。更具体地说，在 ICDAR 2013 数据集中，对于表中列出的所有评估协议，我们的方法胜过最佳竞争方法至少 2%。性能改进在 SVT 数据集上更为重要。在 SVT 和 SVT-50 上，TextBoxes++ 的性能优于最先进的方法至少 8%。这主要是因为 TextBoxes++ 在处理 SVT 中的低分辨率图像时更加健壮，这要归功于其对较低分辨率图像的训练。请注意，Jaderberg 和 FCRNall + filts 采用比我们的方法（90k字）小得多的词汇（50k字），但所提出的方法仍然表现更好。与 TextBoxes 相比，TextBoxes++ 在 ICDAR 2013 数据集上实现了更好的性能。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/5/16.jpg?raw=true) \n",
    "\n",
    "**2）使用识别修正检测结果:** 我们建议使用识别来进一步优化检测结果，将识别得分与检测得分相结合。我们已经在两个数据集上评估了这个想法。 如表IX 所示，没有词典的识别在 ICDAR 2013 和 ICDAR 2015 Incidental 文本数据集中分别提高了 TextBoxes++ 的检测结果 0.5% 和 1.3%。使用指定的词典进一步提高了这种改进，分别在 ICDAR 2013 和 ICDAR 2015 Incidental 文本数据集中分别达到 0.8% 和 1.9%。请注意，当前的文本识别器在处理垂直文本和识别低分辨率文本方面仍然有困难。预计使用更好的文本识别器可以进一步提高性能。  \n",
    "\n",
    "### F. Weaknesses  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/5/17.jpg?raw=true) \n",
    "\n",
    "正如前面的实验结果所证明的，TextBoxes++ 在大多数情况下都表现良好。但是，它仍然无法处理一些困难的情况，例如对象遮挡和大字符间距。由于缺少足够的垂直训练数据，TextBoxes++ 也无法检测到一些垂直文本。即使采用 hard negative mining，一些类似文字的区域仍然被错误地检测到。另一个失败案例是曲线文本检测。与一些 part-based 方法不同，例如[23]，由于四边形表示的限制，TextBoxes++ 很难适应曲面文本的精确边界。请注意，所有这些困难也适用于其他最先进的方法[39]，[40]。图10 显示了一些错误情况。  \n",
    "\n",
    "## V. COMPARISONS WITH RECENT WORKS  \n",
    "\n",
    "我们从两个方面比较了提出的 TextBoxes++ 与 EAST(这是先前最先进的方法之一)、DMPNet(最相关的方法之一)：简单性和性能。  \n",
    "\n",
    "### A. TextBoxes++ vs. EAST  \n",
    "\n",
    "- 1）简单性：EAST 使用 U 形网络生成文本区域图，或命名为分数地图。它还根据生成分数图的相同特征回归定向矩形或四边形。它是分割和检测的结合。这样，它依靠金字塔形反卷积层进行精确分割。这些额外的金字塔式解卷积层需要额外的计算。然而，TextBoxes++ 直接对卷积特征映射上的默认盒子进行分类和回归，这更加简单，避免了金字塔式反卷积的时间浪费。这可以通过 表VI 中显示的速度比较来证明，其中 TextBoxes++（带有VGG-16主干）速度为 11.6fps，而 EAST 的 VGG16 RBOX 版本以 6.52fps 运行。  \n",
    "\n",
    "- 2）性能：EAST 依靠准确的分割分数地图作为边界框的分数。然而，文本区域分割本身具有挑战性。如果分数图不够准确，很难获得正确的结果。例如，有可能将两个接近词之间的分割预测为分割评分图中的文本区域，在这种情况下，在检测中分离这两个词是相当困难的。为了缓解这个问题，EAST 缩小了分割评分图的真实的文本区域。TextBoxe ++ 不会受到这种限制。它依赖于默认盒子，并直接从卷积特征映射中回归边界盒子，其中与分割分数映射相比，更丰富的信息被保留。因此，TextBoxes++ 实现了更高的性能（见表III）。具体而言，TextBoxes++ 的性能优于单一尺度的3.5%（PVANET2x RBOX版本）和6%（VGG16 RBOX版本）。  \n",
    "\n",
    "### B. TextBoxes++ vs. DMPNet  \n",
    "\n",
    "- 1）简单：1）TextBoxes++ 使用水平矩形作为默认框，而不是使用[42]中使用的不同方向的四边形。通过这种方式，我们在每个区域使用的默认框数量要少得多。此外，我们认为卷积特征映射的感受域都是水平矩形，所以如果目标四边形与接受域匹配，它们可以很好地回归。对于一般的场景文本检测而言，[42]中采用的定向矩形默认框不是必需的。 2）受益于水平矩形默认框，TextBoxes++ 通过使用最大水平矩形而不是四边形来实现匹配默认框和真实框的更简单的策略。实际上，即使在[42]中使用了 MonteCarlo 方法，计算两个水平矩形之间的交集区域也比计算两个任意四边形之间的交集区域容易得多（仅使用减法运算和乘法运算一次）。  \n",
    "\n",
    "- 2）性能：1）TextBoxes++ 同时回归边界框和四边形边界框的最大水平矩形，这使得训练比[42]中的方法更稳定。2）与[42]中的方法相比，TextBoxes++ 进一步研究图像中的小文本。我们采用新的数据增强方案，这对小文本有益。3）在本文中，我们不仅关注场景文本检测，还关注检测与识别的结合。我们提出了一种新颖的分数，它有效地将检测分数和识别分数相结合。  \n",
    "\n",
    "  DMPNet [42]没有报告运行时间。但是，我们认为它比基于上述分析的 TextBoxes++ 慢。此外，就 ICDAR 2015 数据集的 Fmeasure 而言，TextBoxes++ 的性能优于 DMPNet [42] 11%（单一比例设置）（见表III）。  \n",
    "\n",
    "## VI. CONCLUSION  \n",
    "\n",
    "我们展示了 TextBoxes++，这是一个任意方向文本检测的端到端完全卷积网络，它非常稳定且高效地针对混乱的背景生成字提议。所提出的方法通过四边形表示通过新的回归模型直接预测任意导向的词边界框。对文本检测，单词识别和端到端场景文本识别的一些流行基准数据集进行综合评估和比较，明确验证了 TextBoxes++ 的优势。在所有的实验中，TextBoxes++ 对水平文本数据集和面向文本数据集均实现了高效的高效性能。今后，我们计划调查几乎所有最先进的文本检测器面临的常见失败案例（例如，大字符间距和垂直文本）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EAST: An Efficient and Accurate Scene Text Detector  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "以前的场景文本检测方法已经在各种基准测试中取得了前景。然而，即使在配备深度神经网络模型时，它们在处理具有挑战性的场景时也通常不足，因为整体性能取决于管道中多个阶段和组件的相互作用。在这项工作中，我们提出了一个简单而强大的管道，可以在自然场景中进行快速准确的文本检测。流水线直接预测全图像中任意方位和四边形形状的单词或文本行，通过单个神经网络消除不必要的中间步骤（例如候选聚合和单词分割）。我们管道的简单性使我们能够集中精力设计损失函数和神经网络结构。包括 ICDAR 2015，COCO-Text 和 MSRA-TD500 在内的标准数据集的实验表明，所提出的算法在准确性和效率方面都明显优于最先进的方法。在 ICDAR 2015 数据集中，所提出的算法在 720p 分辨率下以 13.2fps 获得 0.7820 的 F-分数。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/6/1.jpg?raw=true) \n",
    "\n",
    "最近，提取和理解在自然场景中体现的文本信息变得越来越重要和普遍，ICDAR 系列竞赛的参与者数量空前大量以及 NIST 发起的 TRAIT 2016 评估都证明了这一点。  \n",
    "\n",
    "作为后续过程的先决条件，文本检测在文本信息提取和理解的整个过程中起着至关重要的作用。先前的文本检测方法已经在该领域的各种基准上取得了有希望的表现。文本检测的核心是设计区分文本和背景的特征。传统上，手动设计特征以捕捉场景文本的特征，而在基于深度学习的方法中，有效特征直接从训练数据中学习。  \n",
    "\n",
    "然而，现有的方法，无论是传统的还是基于深度神经网络的方法，大多由几个阶段和组件组成，这些阶段和组件可能是次优的并且耗时的。因此，这些方法的准确性和效率还远远不能令人满意。  \n",
    "\n",
    "在本文中，我们提出了一个快速而准确的场景文本检测流水线，它只有两个阶段。该管道利用全卷积网络（FCN）模型，该模型直接产生单词或文本行级预测，排除冗余和慢速中间步骤。生成的文本预测（可以是旋转矩形或四边形）会发送到非极大值抑制以产生最终结果。根据标准基准的定性和定量实验，与现有方法相比，该算法性能显着提高，运行速度更快。  \n",
    "\n",
    "具体来说，该算法在 ICDAR 2015（多尺度测试时为 0.8072），MSRA-TD500 为 0.7608，COCO-Text 为 0.3945 时 F-score 为 0.7820，优于先前的性能最先进的算法，同时平均花费更少的时间（在 Titan-X GPU 上为 720p 分辨率下的 13.2fps 为我们的最佳性能模型，在我们最快的模型下为 16.8fps）。  \n",
    "\n",
    "这项工作的贡献有三个方面：  \n",
    "\n",
    "- 我们提出了一个由两个阶段组成的场景文本检测方法：全卷积网络和 NMS 合并阶段。FCN 直接生成文本区域，不包括冗余和耗时的中间步骤。  \n",
    "\n",
    "- 流水线可灵活生成字级或线级预测，其几何形状可根据具体应用进行旋转框或四边形。  \n",
    "\n",
    "- 所提出的算法在准确性和速度上明显优于最先进的方法。  \n",
    "\n",
    "## 2. Related Work  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/6/2.jpg?raw=true) \n",
    "\n",
    "场景文本检测和识别一直是计算机视觉领域长期以来的研究热点。许多鼓舞人心的想法和有效的方法已被调查。综合评论和详细分析可以在调查报告中找到。本节将重点介绍与算法相关的主要工作。  \n",
    "\n",
    "常规方法依赖于手动设计的特征。笔划宽度变换（SWT）和最大稳定极值区域（MSER）的方法通常通过边缘检测或极值区域提取寻找候选字符。Zhang 等人 利用了文本的局部对称性，设计了文本区域检测的各种特征。FASText 是一种快速文本检测系统，适用于修改众所周知的 FAST 关键点检测器以进行笔画提取。然而，就精度和适应性而言，这些方法落后于那些基于深度神经网络的方法，特别是在处理具有挑战性的场景时，例如低分辨率和几何失真。  \n",
    "\n",
    "最近，场景文本检测领域进入了一个新的时代，即基于深度神经网络的算法已逐渐成为主流。Huang 等人首先使用 MSER 找到候选区域，然后使用深度卷积网络作为强分类器来修剪误报。Jaderberg 等人的方法以滑动窗口方式扫描图像，并利用卷积神经网络模型为每个尺度生成密集热图。后来，Jaderberg 等人同时使用 CNN 和 ACF 来寻找候选单词并使用回归进一步细化。Tian 等人开发了垂直锚点，并构建了 CNN-RNN 联合模型来检测水平文本线。与这些方法不同的是，Zhang 等人提出利用 FCN 生成热图，并使用分量投影进行方向估计。这些方法在标准基准测试中获得了卓越的性能但是，如图2（a-d）所示，它们主要由多个阶段和组成部分组成，例如通过后期过滤进行假阳性删除，候选聚合，行结构和词分区。多个阶段和组件可能需要彻底调整，导致次优性能，并增加整个管道的处理时间。  \n",
    "\n",
    "在本文中，我们设计了一个基于 FCN 的深度流水线，直接针对文本检测的最终目标：文字或文本级别检测。如图2（e）所示，该模型放弃了不必要的中间组件和步骤，并允许进行端到端的训练和优化。由此产生的系统配备一个轻量级的神经网络，在性能和速度上都明显优于先前的所有方法。  \n",
    "\n",
    "## 3. Methodology  \n",
    "\n",
    "该算法的关键组件是一个神经网络模型，它被训练直接预测文本实例及其全图像的几何形状的存在。 该模型是一种适用于文本检测的全卷积神经网络，可输出单词或文本行的密集每像素预测。 这消除了候选提案，文本区域形成和文字分区等中间步骤。 后处理步骤仅包括预测几何形状上的阈值和NMS。 该检测器被命名为EAST，因为它是一个高效和准确的场景文本检测管道。  \n",
    "\n",
    "### 3.1. Pipeline  \n",
    "\n",
    "图2（e）概述了我们的流程。该算法遵循 DenseBox 的一般设计，其中图像被馈送到 FCN 中，并且生成像素级文本分数图和几何图形的多个通道。  \n",
    "\n",
    "预测的 channels 之一是评分图，其像素值在[0,1]的范围内。其余通道表示从每个像素的视角围绕单词的几何图形。该分数代表在相同位置预测的几何形状的置信度。  \n",
    "\n",
    "我们已经对文本区域，旋转框（RBOX）和四边形（QUAD）进行了两种几何形状的实验，并为每个几何体设计了不同的损失函数。然后将阈值应用于每个预测区域，其中评分超过预定阈值的几何被认为是有效的并保存以用于随后的非最极大值抑制。NMS 之后的结果被认为是流程的最终产出。  \n",
    "\n",
    "### 3.2. Network Design  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/6/3.jpg?raw=true) \n",
    "\n",
    "设计用于文本检测的神经网络时必须考虑几个因素。由于如图5 所示，单词区域的大小差别很大，因此确定大单词的存在将需要来自神经网络后期的特征，而预测包含小单词的准确几何图形在早期阶段需要低级别信息。因此，网络必须使用不同级别的特征来满足这些要求。HyperNet 在特征映射上符合这些条件，但是在大型特征映射上合并大量通道会显着增加后期计算开销。  \n",
    "\n",
    "为了弥补这一点，我们采用 U 形的思想逐步合并特征映射，同时保持较小的上采样分支。我们一起建立了一个网络，既可以利用不同级别的特征，又可以节省计算成本。  \n",
    "\n",
    "我们模型的示意图如图3 所示。模型可以分解为三个部分：特征提取器，特征合并分支和输出层。  \n",
    "\n",
    "stem 可以是在 ImageNet 数据集上预先训练的卷积网络，具有交错卷积和合并层。从 stem 中提取四个级别的特征映射，表示为 fi，其分别为输入图像的大小为 $1/32,1/16,1/6$ 和 $1/4$。在图 3 中，描绘了 PVANet。在我们的实验中，我们也采用了众所周知的 VGG16 模型，特征图从 pooling-2 to pooling-5 中提取。  \n",
    "\n",
    "在特征合并分支中，我们逐渐合并它们：  \n",
    "$\n",
    "\\begin{align}\n",
    "g_i = \\begin{cases} unpool(h_i), & \\text {if $i \\leq 3$} \\\\ {conv}_{3 \\times 3}(h_i), & \\text{if i = 4} \\end{cases} \\tag{1} \\\\ \n",
    "h_i = \\begin{cases} f_i, & \\text {if $i = 1$} \\\\ {conv}_{3 \\times 3}({conv}_{1 \\times 1}([g_{i-1;f_i}])), & \\text{otherwise} \\end{cases} \\tag{2} \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "其中 $g_i$ 是合并基础，$h_i$ 是合并的特征映射，并且运算符 $[\\cdot;\\cdot]$ 表示沿通道轴的连接。在每个合并阶段，来自最后一个阶段的特征映射首先被输入到一个 unpooling 层来扩大其大小，然后与当前特征映射进行连接。接下来，conv1×1 bottleneck[8]减少通道数量并减少计算量，接下来是conv3×3，将信息融合以最终产生该合并阶段的输出。在最后一个合并阶段之后，conv3×3 图层会生成合并分支的最终特征映射并将其馈送到输出图层。  \n",
    "\n",
    "每个卷积的输出通道数量如图3 所示。我们将分支中卷积的通道数保持为 small，这只增加了 stem 的一部分计算开销，使得网络计算效率更高。最终的输出层包含若干个 conv1×1 操作，以将 32 个通道的特征图投影到 1 个通道图 Fs 和一个多通道几何图 Fg。几何输出可以是 RBOX 或 QUAD 中的任意一种，总结在表1 中。    \n",
    "\n",
    "对于 RBOX，geometry 由 4 个轴对齐边界框（AABB）R 和 1 个通道旋转角度 $\\theta$ 表示。R 的公式与[9]中的公式相同，其中 4 个通道分别表示从像素位置到矩形的顶部，右侧，底部，左侧边界的 4 个距离。  \n",
    "\n",
    "对于 QUAD Q，我们使用 8 个数字来表示从四边形的四个角顶点 $\\{p_i|i \\in \\{1,2,3,4\\}\\}$ 到像素位置的坐标移位。由于每个距离偏移量包含两个数字（Δxi，Δyi），因此几何输出包含 8 个通道。  \n",
    "\n",
    "## 3.3. Label Generation  \n",
    "\n",
    "### 3.3.1 Score Map Generation for Quadrangle  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/6/4.jpg?raw=true) \n",
    "\n",
    "不失一般性，我们只考虑几何体是四边形的情况。分数图上四边形的正面区域被设计为大致缩小版本，如图4（a）所示。  \n",
    "\n",
    "对于四边形 $Q = \\{p_i|i \\in \\{1,2,3,4\\}\\}$，其中 $p_i = \\{x_i,y_i\\}$ 是顺时针顺序在四边形上的顶点。为了缩小 Q，我们首先计算每个顶点 pi 的参考长度 ri，  \n",
    "\n",
    "$r_i = min(D(p_i,p_{(i \\ mod \\ 4) + 1}), D(p_i,p_{(i+2 \\ mod \\ 4) + 1})) \\tag{3}$\n",
    "\n",
    "其中 $D(p_i,p_j)$ 是 pi 和 pj 之间的 L2 距离。  \n",
    "\n",
    "我们首先收缩一个四边形的两个较长的边缘，然后缩小两个较短的边缘。对于每一对相对的两条边，我们通过比较它们长度的平均值来确定“较长”对。 对于每个边 $(p_i,p_{(i \\ mod \\ 4) + 1})$，我们通过将它的两个端点沿着边向内移动 $0.3r_i$ 和 $0.3r_{(i \\ mod \\ 4) + 1}$ 来收缩它。  \n",
    "\n",
    "### 3.3.2 Geometry Map Generation  \n",
    "\n",
    "正如在 3.2 节中讨论的，几何图是 RBOX 或 QUAD 中的一种。图4（c-e）说明了 RBOX 的生成过程。  \n",
    "\n",
    "对于其文本区域以 QUAD 样式注释的数据集（例如 ICDAR 2015），我们首先生成一个旋转矩形，以最小面积覆盖该区域。然后，对于每个有正分数的像素，我们计算它到文本框 4 个边界的距离，并将它们放到 RBOX ground truth 的4个通道中。对于 QUAD ground truth，8 通道几何图中具有正分数的每个像素的值是其从四边形的 4 个顶点的坐标移位。  \n",
    "\n",
    "### 3.4. Loss Functions  \n",
    "\n",
    "损失可以表述为  \n",
    "\n",
    "$L = L_s + \\lambda_g L_g \\tag{4}$\n",
    "\n",
    "其中 Ls 和 Lg 分别表示分数图和几何图形的损失，$\\lambda_g$ 表示两个损失之间的重要性。在我们的实验中，我们将 $\\lambda_g$ 设置为1。  \n",
    "\n",
    "#### 3.4.1 Loss for Score Map  \n",
    "\n",
    "在大多数最先进的检测管线中，训练图像通过均衡采样和 hard negative mining 仔细处理，以解决目标对象的不均衡分布。这样做可能会提高网络性能。 然而，使用这种技术不可避免地会引入一个不可区分的阶段和更多的参数来调整和更复杂的流水线，这与我们的设计原理相矛盾。  \n",
    "\n",
    "为了简化训练过程，我们使用[38]中介绍的等级交叉熵，由下式给出  \n",
    "\n",
    "$L_s = balanced-xent (\\hat Y, Y^∗) = - \\beta Y^* \\log \\hat y - (1- \\beta)(1 - Y^*) \\log( 1- \\hat Y) \\tag{5}$\n",
    "\n",
    "其中 $\\hat Y = F_s$ 是分数图的预测，$Y^*$ 是 ground truth。 参数 $\\beta$ 是正负样本之间的平衡因子，由下式给出  \n",
    "\n",
    "$\\beta = 1 - \\frac {\\sum_{y^* \\in Y^*}y^*}{|Y^*|} \\tag{6}$\n",
    "\n",
    "这种平衡的交叉熵损失首先被 Yao 等人作为分数图预测的目标函数在文本检测中采用。 我们发现它在实践中运作良好。  \n",
    "\n",
    "#### 3.4.2 Loss for Geometries  \n",
    "\n",
    "文本检测的一个挑战是自然场景图像中文本的大小差别很大。直接使用 L1 或 L2 损失进行回归会导致损失偏向较大和较长的文本区域。由于我们需要为大文本区域和小文本区域生成准确的文本几何预测，因此回归损失应该是尺度不变的。因此，我们采用 RBOX 回归的 AABB 部分的 IoU 损失和 QUAD 回归的 scale-normalized smoothed-L1 loss。  \n",
    "\n",
    "**RBOX** 对于 AABB 部分，我们采用[46]中的 IoU 损失，因为它对不同尺度的物体是不变的。  \n",
    "\n",
    "$L_{AABB} = - \\log {IOU}(\\hat R, R^*) = - \\log{\\frac {|\\hat R \\cap R^*|}{|\\hat R \\cup R^*|}} tag{7}$\n",
    "\n",
    "其中 R 代表预测的 AABB 几何，$R^*$ 是其相应的 ground truth。很容易看出相交矩形 $|\\hat R \\cap R^*|$ 的宽度和高度是  \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "w_i = min(\\hat d_2, d_2^*) + min(\\hat d_4, d_4^*) \\\\\n",
    "h_i = min(\\hat d_1, d_1^*) + min(\\hat d_3, d_3^*)\n",
    "\\end{align} \\tag{8}\n",
    "$\n",
    "\n",
    "其中 d1，d2，d3 和 d4 分别表示从一个像素到其对应的矩形的顶部，右侧，底部和左侧边界的距离。并集区域由下式计算  \n",
    "\n",
    "$|\\hat R \\cup R^*| = |\\hat R| + |R^*| - |\\hat R \\cap R^*| \\tag{9}$\n",
    "\n",
    "因此，可以很容易地计算出 intersection/union 区域。接着，计算旋转角度的损失  \n",
    "\n",
    "$L_{\\theta}(\\hat \\theta, \\theta^*) = 1 - \\cos(\\hat \\theta - \\theta^*) \\tag{10}$\n",
    "\n",
    "其中 $\\hat \\theta$ 是对旋转角度的预测，$\\theta^*$ 代表 ground truth。最后，总体几何损失是 AABB 损失和角度损失的加权和，由下式给出  \n",
    "\n",
    "$L_g = L_{AABB} + \\lambda_{\\theta} L_{\\theta} \\tag{11}$\n",
    "\n",
    "在我们的实验中 $\\lambda_{\\theta}$ 设置为10。  \n",
    "\n",
    "请注意，我们计算 $ L_{AABB}$ 而不考虑旋转角度。当完全预测角度时，这可以看作是四边形 IoU 的近似值。尽管在训练期间情况并非如此，但它仍然可以为网络施加正确的梯度以学习预测 $\\hat R$.  \n",
    "\n",
    "**QUAD** 我们通过增加一个额外的标准化术语来扩展文献[6]中提出的平滑 L1 损失，这个术语是为单个四边形设计的，这个术语通常在一个方向上较长。让 Q 的所有坐标值都是一个有序集合  \n",
    "\n",
    "$C_Q = \\{x_1,y_1,x_2,y_2,...,x_4,y_4\\} \\tag{12}$\n",
    "\n",
    "loss 可以写作以下形式  \n",
    "\n",
    "$L_g = L_{QUAD}(\\hat Q,Q^*) = {min}_{\\tilde Q \\in P_{Q^*}} \\sum_{c_i \\in C_Q} \\frac{{smoothed}_{L1}(c_i - \\tilde c_i)}{8 \\times N_{Q^*}} \\tag{13}$\n",
    "\n",
    "其中归一化项 $N_{Q^*}$是四边形的短边长度，由下式给出  \n",
    "\n",
    "$N_{Q^*} = {min}_{i=1}^4 D(p_i,p_{i \\ mod 4} + 1) \\tag{14}$\n",
    "\n",
    "$P_Q$ 是具有不同顶点排序的 $Q^*$ 的所有等价四边形的集合。由于公共训练数据集中四边形的注释不一致，因此需要此排序置换。  \n",
    "\n",
    "## 3.5. Training  \n",
    "\n",
    "网络使用 ADAM 优化器进行端对端训练。为了加快学习速度，我们从图像中统一采样 512x512 crops，形成 24 大小的 minibatch。ADAM 的学习速率从 1e-3 开始，每 27300 minibatches 衰减到十分之一，并停在 1e-5，训练网络直到性能停止改善。  \n",
    "\n",
    "### 3.6. Locality-Aware NMS  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/6/5.jpg?raw=true) \n",
    "\n",
    "为了形成最终结果，阈值处理后的几何图形应该由 NMS 合并。一个简单的 NMS 算法在 $O(n^2)$ 中运行，其中 n 是候选几何体的数量，这是不可接受的，因为我们正面临来自密集预测的成千上万个几何体。  \n",
    "\n",
    "在假设来自附近像素的几何图形倾向于高度相关的情况下，我们建议逐行合并几何图形，并且在合并同一行中的几何图形时，我们将迭代合并当前遇到的几何图形与最后一个合并图形。这种改进的技术在最佳场景中以 $O(n)$ 运行。尽管最糟糕的情况与最初的情况相同，只要局部性假设成立，算法在实践中运行得足够快。该过程在算法1 中进行了总结  \n",
    "\n",
    "值得一提的是，在 WEIGHTEDMERGE(g,p) 中，合并四边形的坐标通过两个给定四边形的得分进行加权平均。具体而言，如果 a = WEIGHTEDMERGE(g,p)，则 $a_i = V(g)g_i + V(p)p_i$ 和 $V(a) = V(g) + V(p)$，而 V(a) 是几何 a 的分数。  \n",
    "\n",
    "实际上，我们正在“平均”而不是“选择”几何形状，这一点存在细微的差异，因为在标准的 NMS 程序中会起到投票机制的作用，反过来在提供视频时引入稳定效应。尽管如此，我们仍然采用“NMS”这个词来描述功能。  \n",
    "\n",
    "## 4. Experiments  \n",
    "\n",
    "为了将所提出的算法与现有方法进行比较，我们对三个公共基准：ICDAR2015，COCO-Text 和 MSRA-TD500 进行了定性和定量实验。  \n",
    "\n",
    "### 4.1. Benchmark Datasets  \n",
    "\n",
    "ICDAR 2015 用于 ICDAR 2015 强劲阅读竞赛的挑战赛 4。它共包含 1500 张照片，其中 1000 张用于训练，其余用于测试。文本区域由四边形的 4 个顶点注释，对应于本文中的 QUAD 几何。我们还通过拟合具有最小面积的旋转矩形来生成 RBOX 输出。Google Glass 以随机拍摄这些图像。因此，场景中的文本可能处于任意方向，或遭受运动模糊和低分辨率。我们还使用了 ICDAR 2013 的 229 张训练图像。  \n",
    "\n",
    "COCO-Text 是迄今为止最大的文本检测数据集。它重用 MS-COCO 数据集中的图像。总共有 63,686 张图像被注释，其中 43686 被选作训练集，其余 20,000 张用于测试。字区域以轴对齐边界框（AABB）的形式注释，这是 RBOX 的特例。对于这个数据集，我们将角度θ设置为零。我们使用与 ICDAR 2015 相同的数据处理和测试方法。  \n",
    "\n",
    "MSRA-TD500 是一个包含 300 个训练图像和 200 个测试图像的数据集。文本区域是任意的方向，并在句子级别进行注释。与其他数据集不同，它包含英文和中文文本。文本区域以 RBOX 格式注释。由于训练图像的数量太少而无法学习深层模型，因此我们也将 HUSTTR400 数据集中的 400 幅图像作为训练数据。 \n",
    "\n",
    "### 4.2. Base Networks  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/6/6.jpg?raw=true) \n",
    "\n",
    "除 COCO-Text 之外，与通用对象检测的数据集相比，所有文本检测数据集相对较小，因此如果所有基准都采用单一网络，则可能会出现过拟合或欠拟合。 我们对所有数据集上的三种不同输出几何结构的基础网络进行了实验，以评估所提出的框架。这些网络汇总在表2。  \n",
    "\n",
    "VGG16 被广泛用作许多任务的基础网络，以支持随后的特定任务的微调，包括文本检测。这个网络有两个缺点：（1）这个网络的接受领很小，在 conv5_3 输出中的每个像素仅具有 196 的接受域；（2）这是一个相当大的网络。  \n",
    "\n",
    "PVANET 是[17]中引入的一个轻量级网络，旨在替代 Faster-RCNN 框架中的特征提取器。由于他对使用 GPU 对于充分利用计算并行性来说太小，我们也采用 PVANET2x，它将原始 PVANET 的通道加倍，从而利用更多的计算并行性，同时运行速度略低于 PVANET。这在 4.5 节详细说明。最后一个卷积层输出的感受域为 809，远大于 VGG16。  \n",
    "\n",
    "这些模型是在 ImageNet 数据集上预先训练的。  \n",
    "\n",
    "### 4.3. Qualitative Results  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/6/7.jpg?raw=true) \n",
    "\n",
    "图5 描述了所提出的算法的几个检测示例。 它能够处理各种具有挑战性的场景，例如不均匀照明，低分辨率，不同方向和透视失真。 此外，由于NMS程序中的投票机制，所提出的方法对于具有各种形式的文本实例的视频显示出高水平的稳定性2。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/6/8.jpg?raw=true) \n",
    "\n",
    "所提出的方法的中间结果如图6 所示。可以看出，训练模型产生高度精确的几何图和分数图，其中易于形成在不同方向上的文本实例的检测。  \n",
    "\n",
    "### 4.4. Quantitative Results  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/6/9.jpg?raw=true) \n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/6/10.jpg?raw=true) \n",
    "\n",
    "如表3 和表4 所示，我们的方法在 ICDAR 2015 和 COCO-Text 上大大优于先前的最先进方法。  \n",
    "\n",
    "在 ICDAR 2015 挑战赛4 中，当图像以其原始比例进行馈送时，所提出的方法实现 0.7820 的 Fscore。当使用相同的网络在多种尺度下测试时，我们的方法在 F 评分中达到 0.8072，比最佳方法高近 0.16。  \n",
    "\n",
    "使用 VGG16 网络比较结果，当使用 QUAD 输出时，所提出的方法也优于以前的最佳工作，在使用 QUAD 输出时提高 0.0924，在使用 RBOX 输出时提高0.116。同时这些网络非常有效，如第 4.5 节所示。  \n",
    "\n",
    "在 COCO 文本中，所提出算法的三个设置都比之前的方法更高的准确性。具体而言，考虑到 COCO-Text 是迄今为止最大且最具挑战性的基准，Fscore 比[41]的改进为 0.0614，而召回的改进为 0.053，这证实了所提算法的优点。请注意，我们也包含了来自[36]的结果作为参考，但这些结果实际上不是有效的基线，因为方法（A，B和C）用于了数据注释。  \n",
    "\n",
    "与先前的方法相比，改进的算法证明了一个简单的文本检测流水线直接针对最终目标并消除冗余流程，可以击败精细的流水线，即使是那些与大型神经网络模型集成的流水线。  \n",
    "\n",
    "如表5 所示，在 MSRA-TD500 上，我们方法的三种设置都取得了优异的结果。最佳的模型（Ours + PVANET2x）的 F 分数略高于[41]。与 Zhang 等人的方法相比，先前发表的最先进的系统，最佳的模型（Ours + PVANET2x）的 F 评分得分提高 0.0208，精确度提高 0.0428。  \n",
    "\n",
    "请注意，在 MSRA-TD500 上，我们配备 VGG16 的算法比 PVANET 和 PVANET2x（0.7023对0.7445和0.7608）差很多，主要原因是 VGG16 的有效接受场比PVANET 和 PVANET2x 小，MSRA-TD500 的评估协议要求文本检测算法输出行级别，而不是字级预测。  \n",
    "\n",
    "另外，我们还在 ICDAR 2013 基准测试中评估了 Ours + PVANET2x。它的召回率，精度和 F 分数分别达到 0.8267,0.9264 和 0.8737，这与先前的最新方法[34]相当，在召回率，精度和 F 分数方面分别获得0.8298,0.9298和0.876。  \n",
    "\n",
    "### 4.5. Speed Comparison  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/6/11.jpg?raw=true) \n",
    "\n",
    "整体速度比较在表6 中。我们报告的数据是使用我们最佳性能网络以原始分辨率（1280x720）运行来自 ICDAR 2015 数据集的 500 个测试图像的平均值。 这些实验是在服务器上使用具有 Maxwell 架构的单个 NVIDIA Titan X 图形卡和 Intel E5-2670 v3 @ 2.30GHz CPU 进行的。对于所提出的方法，后处理包括阈值处理和 NMS，而另一些则应参考其原始文件。  \n",
    "\n",
    "虽然所提出的方法明显优于最先进的方法，但计算成本保持非常低，归因于简单且高效的流水线。从表6 中可以看出，我们方法的最快设置以 16.8 FPS 的速度运行，而最慢的设置以 6.52 FPS 运行。即使是性能最好的模型 Ours + PVANET2x 也能以 13.2 FPS 的速度运行。这证实了我们的方法是在基准测试中达到最高性能的最有效的文本检测器。  \n",
    "\n",
    "### 4.6. Limitations  \n",
    "\n",
    "检测器可以处理的文本实例的最大大小与网络的接受域成正比。这限制了网络预测更长的文本区域的能力，例如跨越图像的文本行。  \n",
    "\n",
    "此外，该算法可能会遗漏或给出不精确的垂直文本实例预测，因为它们只采用 ICDAR 2015 训练集中的一小部分文本区域。  \n",
    "\n",
    "## 5. Conclusion and Future Work  \n",
    "\n",
    "我们已经提出了一个场景文本检测器，可以从单幅神经网络的完整图像中直接生成单词或线条级别的预测结果。通过合并适当的损失函数，检测器可以预测文本区域的旋转矩形或四边形，具体取决于具体应用。标准基准上的实验证实，所提出的算法在准确性和效率方面基本上优于先前的方法。  \n",
    "\n",
    "未来研究的可能方向包括：（1）调整几何公式以允许直接检测曲线文本；（2）将检测器与文本识别器集成；（3）将想法扩展到一般对象检测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Shot Text Detector with Regional Attention  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "我们提出了一种新颖的 single-shot 文本检测器，可以直接输出自然图像中的字级边界框。我们提出了一个关注机制，通过自动学习的注意图粗略识别文本区域。这基本上抑制了卷积特征中的背景干扰，这是产生词的精确推断的关键，特别是在极小的尺寸下。这导致了一个基本上以粗到细的方式工作的单一模型。它不同于最近基于 FCN 的文本检测器，它们级联多个 FCN 模型以实现准确的预测。此外，我们开发了一个分级 inception 模块，可以有效地聚合多尺度初始特征。这增强了局部细节，并对强大的上下文信息进行编码，使检测器可以在单尺度图像上以多尺度和多方位文本进行可靠工作。我们的文本检测器在 ICDAR 2015 基准测试中实现了 77% 的 F-measure，提高了[18,28]的最新成果。demo：http://sstd.whuang.org/。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "最近的研究表明，野外文本阅读已经引起了计算机视觉界越来越多的关注。它在图像检索，工业自动化，机器人导航和场景理解方面具有众多潜在应用。 最近的工作集中在自然图像中的文本检测，这仍然是一个具有挑战性的问题。主要困难在于文本的规模，方向，照明和字体的巨大差异，这往往伴随着非常复杂的背景。  \n",
    "\n",
    "以前的文本检测工作主要采用自底向上的方法，它们通常包含多个连续步骤，包括字符或文本成分检测，然后是字符分类或过滤，文本行构建和分词。目标检测和过滤步骤在这种自下而上的方法中起着关键作用。先前的方法通常使用基于连接分量的方法（例如，笔画宽度或极值区域）或滑动窗口方法来识别字符或文本分量候选。但是，这两类方法通常都会受到两个主要限制，这会显着降低效率和性能。首先，文本检测是建立在识别单个字符或组件的基础上的，因此很难探索区域上下文信息。这通常导致 recall 较低，其中容易丢弃模棱两可的字符。这也会导致精度的降低，因为会产生大量的错误检测结果。其次，多个连续步骤使得系统高度复杂，并且在后面的步骤中容易积累错误。  \n",
    "\n",
    "深度学习技术大大提高了文本检测的性能。最近的一些方法建立在全卷积网络（FCN）上，通过对文本或非文本进行像素预测。我们将这组方法称为基于像素的文本检测器，它将先前的基于字符的检测投影到文本语义分割的问题中。这使得他们可以探索丰富的区域背景信息，从而更强大地检测歧义文本的能力，并大幅减少错误检测的次数。尽管有效识别粗略文本区域，但这些基于 FCN 的方法无法用单一模型生成准确的单词级预测。主要挑战是精确识别检测到的粗略文本区域中的单个词。如[28]所示，文本检测的任务往往需要比一般对象检测更高的定位精度。为了提高精度，通过级联两个 FCN 模型开发了一个由粗到细的检测流程。第二个 FCN 在由第一个检测到的裁剪文本区域上生成单词或字符级预测。这不可避免地增加了系统的复杂性：（i）从预测的热图正确地显示出文本，文字或字符的区域是非常启发式但复杂的；（ii）构建文本行/文字仍需要多个自下而上的步骤。  \n",
    "\n",
    "最近，另一组文本检测器被开发用于直接预测文本边界框，通过扩展最先进的对象检测器，如 Faster RCNN 和 SSD。它们都旨在直接通过滑动窗口来预测文本框的卷积特征，并且我们将它们称为基于盒子的文本检测器。与基于像素的方法（其中提供文本掩码）相比，基于盒子的文本检测器通常仅通过使用边界框注释来训练，所述边界框注释可能太粗糙，基于像素的方法通常提供了字符的 mask。这使得模型难以详细地学习足够的单词信息，导致单词 one-shot 预测的准确度损失，特别是对于那些小尺度的单词预测。因此，他们可能会再次提出多个后处理步骤来提高性能。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/7/1.jpg?raw=true) \n",
    "\n",
    "### 1.1. Contributions  \n",
    "\n",
    "这些相关的方法激发了目前的工作，其目的是在 one shot 中直接估计一个字级边界框。我们通过引入新的注意力模块将级联 FCNs 检测器集成到单个模型中，该模块支持在训练中通过 mask 明确编码详细文本信息进行监督训练，以及在测试中隐式文本区域检测功能。这优雅地弥合了基于像素的方法和基于盒子的文本检测器之间的差距，从而形成了基本上以粗到细的方式工作的 single-shot 模型。我们开发了一个 hierarchical inception 模块，进一步增强了卷积特性。该论文的主要贡献有三个。  \n",
    "\n",
    "首先，我们提出了一种新的文本注意模块，它引入了一种新的辅助损失，建立在聚合的 inception 卷积特征上。它使用以像素为单位的文本 mask 来显式编码强文本特定的信息，从而使模型能够在文本区域上学习粗糙的从上到下的空间注意力。本文的区域关注显着地抑制了卷积特征中的背景干扰，其结果是减少了错误的检测并且突出了具有挑战性的文本模式。  \n",
    "\n",
    "其次，我们开发了一个 hierarchical inception 模块，可以有效地聚合多尺度 inception 特征。具有扩张卷积的初始体系结构[34]应用于每个卷积层，使模型能够捕获多尺度图像内容。多层聚合进一步增强了局部细节信息并编码了丰富的上下文信息，从而为单词预测提供了更强大的深层特征。  \n",
    "\n",
    "第三，所提出的文本特定模块无缝地结合到 SSD 框架中，该框架可以优雅地将其定制为快速，准确和单短文本检测。这产生了一个功能强大的文本检测器，可以在单尺度输入的多尺度和多方向文本上可靠地工作。它在标准的 ICDAR 2013 和 ICDAR 2015 基准测试中获得了最先进的性能，在 704×704 的图像上运行时间约为 0.13 秒。  \n",
    "\n",
    "## 2. Related Work  \n",
    "\n",
    "以前关于场景文本检测的工作主要集中在自下而上的方法上，这些方法通过使用手工特征或滑动窗口方法来检测图像中的字符或文本成分。它们通常涉及文本/非文本的像素级二进制分类并生成文本显著图。然后，多个自下而上的步骤被设计为将与文本相关的像素分组为字符，这些字符是进一步形成的字符对，然后是文本行。每一步都可以跟随一个文本/非文本过滤器或分类器。这些步骤中的大多数建立在启发式或手工制作的特征上，例如渐变，笔画宽度，协方差描述符等。这些自下而上的方法很复杂，并且使用低级特征既不健壮也不可靠。  \n",
    "\n",
    "深度学习技术在过去几年显著提高了文本检测的性能。这些方法基本上是以滑动窗口的方式工作的，有两个关键的发展：（i）它们利用深度特征，与分类器共同学习，以使文本的强大表示；（ii）共享卷积机制用于显着降低计算成本。通过这两个改进，已经提出了许多基于全卷积网络（FCN）的方法。他们计算文本或非文本的像素语义估计，从而使快速文本检测器能够探索丰富的区域上下文信息。然而，这些基于像素的文本检测器很难通过使用单个模型来提供足够的定位精度。此外，来自预测热图的文本的准确分割是复杂的，并且通常需要许多启发式后处理步骤。  \n",
    "\n",
    "我们的工作还涉及最新的方法，这些方法从最先进的物体探测器（如 SSD 和 Fast R-CNN）扩展而来，这些方法都旨在从卷积特征中预测文本边界框。Liao 等人通过扩展文本检测的 SSD 模型提出了一个 TextBox，但是它们的性能受到为通用对象检测而设计的 SSD 架构的限制。在[18]中提出了 Deep Matching Prior Network（DMPNet），通过引入四边形滑动窗口来处理多向文本。准确的文本定位是通过使用多步粗略至精细检测以及后期调整来实现的。Tian 等人提出了连接主义文本建议网络（CTPN），它通过预测一系列精细文本组件来检测文本行。CTPN 很难处理多方位文本，并需要自下而上的步骤将文本组件分组为文本线。Gupta 等人开发了一个全卷积回归网络（FCRN），用于预测图像中的单词边界框。但是，FCRN 需要三阶段后处理步骤，这大大降低了系统的效率。例如，后处理步骤大约需要 1.2s/图像，而对于边界框估计，则需要 0.07s/图像。我们的工作与这些方法明显不同，提出了两个文本特定的模块。 它具有许多吸引人的属性，可以超越这些相关方法：（i）它是一个 single-shot 检测器，可直接输出单词边界框，填补语义文本分割与单词框直接回归之间的空白；（ii）它是高效的，并且不需要任何自下而上或启发式的后处理步骤；（iii）它在多方位文本上可靠工作；（iv）速度快但准确，并且在标准ICDAR 2015 基准测试中明显优于那些相关方法。  \n",
    "\n",
    "## 3. Methodology  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/7/2.jpg?raw=true) \n",
    "\n",
    "我们展示了提出的 single-shot 文本检测器的细节，除了简单的 NMS 之外，它直接输出没有后处理的字级边界框。我们的检测器由三个主要部分组成：卷积组件，文本特定组件和盒子预测组件。卷积和盒子预测组件主要继承 SSD 检测器。我们建议包含两个新模块的文本特定组件：文本注意模块和分层 inception 模块。SSD 的卷积架构从 16 层 VGGNet 扩展，通过用几个卷积层替换全连接（FC）层。所提出的模块可以很容易地集成到 SSD 的卷积组件和盒子预测组件中，从而形成一个端到端的可训练模型，如图 2 所示。两个文本特定模块可以优化定制 SSD 框架字检测。与最近的方法相比，我们通过实验证明，我们的特定设计提供了一个更有理论基础的解决方案，泛化性能更好。  \n",
    "\n",
    "### 3.1. Text Attention Mechanism  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/7/3.jpg?raw=true) \n",
    "\n",
    "我们的注意力模块能够自动从卷积特征中学习粗略的文本空间区域。然后将这种对文本的注意力直接编码回卷积特征中，其中文本相关特征通过减少卷积映射中的背景干扰而强烈增强，如图3 所示。  \n",
    "\n",
    "注意模块建立在 Aggregated Inception Feature特征（AIF）上（如3.2节所述）。它会生成一个逐像素概率热图，用于指示每个像素位置处的文本概率。 该概率热图被称为注意图，其具有与输入图像相同的尺寸，并且将针对每个预测层被下采样。注意模块包括两个 pad 为 1 3×3 卷积层，一个将 AIF 连接到注意图的反卷积（双线性操作的上采样）层。然后通过在解卷积特征上使用 softmax 激活函数来生成注意图。具体来说，给定一个 512×512 的输入图像，我们得到第一层 AIF 特征，$F_{AIF_1} \\in R^{64 \\times 64 \\times 512}$。注意图 $\\alpha^+ \\in R^{512 \\times 512}$ 通过以下方法进行计算，  \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "D_{AIF_1} = {deconv}_{3 \\times 3}(F_{AIF_1}), \\tag{1} \\\\\n",
    "\\overline D_{AIF_1} = {conv}_{1 \\times 1}(D_{AIF_1}), \\tag{2} \\\\\n",
    "\\alpha = softmax(\\overline D_{AIF_1}). \\tag{3}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "其中 $D_{AIF_1} \\in R^{512 \\times 512 \\times 512}$ 是解卷积后的特征映射，使用 1×1 内核进一步投影到 2 通道映射 $\\overline D_{AIF_1} \\in R^{512 \\times 512 \\times 512}$ ，然后是 softmax 函数。然后，softmax 图的正数部分 $\\alpha^+$ 是注意图，表示像素是文字的可能性。注意图进一步编码到 AIF 中，方法是将其与空间大小一样重新调整大小，  \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\hat \\alpha^+ = resize(\\alpha^+), \\tag{1} \\\\\n",
    "\\hat F_{AIF_1} = \\hat \\alpha^+ \\bigodot F_{AIF_1}. \\tag{3}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "其中 $\\hat \\alpha^+ \\in R^{64 \\times 64}$ 是重新调整大小的注意力图，并且指示在 AIF 图的所有通道上的元素点生成。FAIF1 是编码文本区域关注的结果特征图。有和没有文字注意的 AIF 如图3 所示，其中当注意被编码时文本信息被清楚地呈现。  \n",
    "\n",
    "文字注意信息在训练过程中自动学习。我们引入了一种辅助损失，它通过在每个像素位置指示文本或非文本的二进制掩码提供对文本的直接和详细监视。 softmax 函数用于优化这个注意图到提供的文本掩码，明确地将强文本信息编码到注意模块中。请注意，提议的注意模块是在一个统一的框架中制定的，该框架通过允许计算通过所有层的反向传播来端对端训练。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/7/4.jpg?raw=true) \n",
    "\n",
    "建议的注意模块将这项工作与以前的像素方式和盒式方式的文本检测器区别开来。它优雅地处理了这两组方法的主要局限性，从而形成了一个高效的单次文本检测器，可以生成准确的字级文本检测。图4 展示了所提出的关注模块的效率，其中提出了基线模型和信号模型的检测结果。显然，提出的关注模块在三个方面提高了性能：（i）减少了错误检测的次数；（ii）它允许模型检测更多不明确的文本；（iii）它提高了字级检测的准确性。  \n",
    "\n",
    "### 3.2. Hierarchical Inception Module  \n",
    "\n",
    "在 CNN 模型中，较低层的卷积特征通常集中在局部图像细节上，而深层特征通常捕获更多高级抽象信息。SSD 检测器可以预测多层卷积图上的对象边界框，从而可以从单一尺度输入中定位多尺度对象。文本通常在尺寸上有很大的变化，并且显着不同的纵横比，使得单一尺度的文本检测极具挑战性。在最近的工作中，分层 RNN 被纳入卷积层，使探测器能够利用到整行文本丰富的上下文信息。这种基于 RNN 的文本检测器功能强大，可检测 near-horizontal  的文本行，但难以在多向文本中可靠地工作。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/7/5.jpg?raw=true) \n",
    "\n",
    "受谷歌网络 inception 架构的启发，我们提出了一个能够聚合更强大的卷积特征的 hierarchical inception 模块。首先，我们开发了一个类似的 inception 模块，应用于每个卷积层，如图5 所示。这使得它可以通过使用多尺度接受域来捕获更丰富的上下文信息。其次，我们聚合来自多个层次的 inception 特征并生成最终的聚合启发特征（AIF）。  \n",
    "\n",
    "inception 构建块的细节在图5 中描述。它用于预测单词边界框的卷积层。通过四个不同的卷积运算处理层中的卷积映射：1×1-conv，3×3-conv，3×3-pool，1×1conv 和 5×5-conv。5×5-conv 被分解为 1×5 和 5×1 卷积层。扩张卷积，它支持接受领域的指数扩展而不损失分辨率或覆盖范围。每个卷积操作将特征通道的数量减少到 128 个。通过简单地连接四个 128 通道特征来生成最终的 inception 特征，从而产生 512 通道 inception 特征。通过使用具有通道拼接的多个卷积运算，inception 特征具有多尺度接受域，因此可以集中在各种尺度的图像内容上。  \n",
    "\n",
    "由 HyperNet 推动[5]，我们通过聚合多层 inception 特征来进一步增强卷积特征，这些特征在三个关键卷积层产生最终的 AIF，如图2 所示。每个 AIF 通过融合当前层与两个直接相邻的层的 inception 特征计算得到。下采样和上采样分别应用于 lower layer 和 higher layer。这些采样操作可确保三个inception 特征的相同特征分辨率，这些特征使用通道拼接组合在一起。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/7/6.jpg?raw=true) \n",
    "\n",
    "所提出的 hierarchical inception 模块与[19]中开发的 skip 体系结构相关，该体系结合了用于处理多尺度对象的多层卷积特征。我们通过利用有效的分层 inception 模块提出了一种两步聚合方法。这导致更强大的 AIF，更丰富的本地细节编码和更强大的多尺度能力。这两种改进对于文本任务都很重要，使我们的模型能够识别非常小尺度的文本，并且在多尺度文本上可靠地工作，这些文本通常具有比一般对象更广泛的尺度范围。与[28]中开发的分层 RNN 方法相比，AIF 编码更多局部细节信息，更好地推广到多方位文本。图6 展示了所提出的分层开始模块的有效性。  \n",
    "\n",
    "### 3.3. Word Prediction Module  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/7/7.jpg?raw=true) \n",
    "\n",
    "建议的文本特定模块直接并入 SSD 框架。它可以通过简单地跟随 SSD 进行训练，并对盒子预测部分稍作修改。如[17]中所示，我们使用 softmax 函数对文本或非文本进行二值分类，并应用 smooth l1 损失来回归每个单词边界框的 5 个参数，包括方向参数。我们的模型预测 inception 或 AIF 地图的每个空间位置处的 N 个边界框。预测是通过图2 显示的所有 inception 和 AIF 图计算出来的，N 是预先定义的默认字框的数量，可以预先计算。默认框由其比例和高宽比来定义。由于不同的 inception 图具有各种大小的感受野，所以默认框的大小随着不同的 inception 或 AIF 图而变化。其长度由输入图像中的像素数量来衡量。为了更好地将默认盒子与真实盒子相匹配，我们对每一层使用三种不同的尺度，所有 inception 和 AIF 层的尺度列于表1 中。  \n",
    "\n",
    "由于文本的显着差异，默认框的纵横比设置在很宽的范围内（在表1中）。所有图层都使用相同的纵横比。为了处理多方位文字，尺度可以是默认方框的宽度或高度。这导致每个图层总共有 45 个默认框，这允许检测器处理大形状差异的文本。默认框的中心与初始/卷积映射的当前空间位置相同。  \n",
    "\n",
    "## 4. Experimental Results  \n",
    "\n",
    "我们的方法在三个标准基准上评估，ICDAR 2013，ICDAR 2015 和 COCO-Text 数据集。通过开展 exploration 研究来调查每个提议组件的有效性。将全部结果与三项基准测试中的最新性能进行比较。  \n",
    "\n",
    "### 4.1. Datasets and Implementation Details  \n",
    "\n",
    "**数据集** ICDAR 2013 包含 229 个训练图像和 233 个测试图像，并提供字级注释。它是评估近水平文本检测的标准基准。我们使用两种标准的评估协议：新的 ICDAR13 标准和 DetEval。我们的结果是通过将预测边界框上传到官方评估系统获得的。ICDAR 2015（Incidental Scene Text Challenge 4）是使用 Google Glass 收集的，共有 1,500 幅图像：1,000 幅用于训练的图像和剩余的 500 幅图像用于测试。这个新的基准设计用于评估多方位文本检测。提供单词级注释，每个单词以其四个角的坐标以顺时针方式标记。此数据集更具挑战性，并且具有任意方向的图像，运动模糊和低分辨率文本。我们根据在线评估系统评估我们的结果。COCO-Text 是最大的文本检测数据集，共有 63,686 个注释图像：训练 43,686，其余 20,000 个用于测试。  \n",
    "\n",
    "**训练数据集** 我们的训练样本是从 ICDAR 2013 和 ICDAR 2015 的训练集中收集的。我们还将从互联网收集的图像作为训练数据添加，并用字级注释手动标记。我们共有 13,090 张训练图像。我们没有使用 COCO 文本的训练数据。  \n",
    "\n",
    "**实施细节** 我们的检测网络通过使用小批量随机梯度下降算法进行端对端培训，批量大小设置为32，动量为0.9。我们使用[17]中的预先训练的模型初始化我们的模型。学习率设置为 0.001，在 15,000 次迭代后衰减到 1/10。该模型通过固定前四个卷积层进行训练，当损失不再减少时停止训练。  \n",
    "\n",
    "用于用了 SSD 中的数据增强策略。通过从图像中随机采样一个片段，并将最小 Jaccard 与实际字边界框重叠为{0.1,0.3,0.5,0.7,0.9}。然后将采样补丁大小调整为 704×704，并随机镜像并产生颜色失真。NMS 阈值和置信度阈值分别设置为 0.3 和 0.7。我们的方法是使用 Caffe 和 TITAN X GPU 实现的。  \n",
    "\n",
    "### 4.2. Exploration Study  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/7/9.jpg?raw=true) \n",
    "\n",
    "我们分别评估提议的文本注意模块（TAM）和层次初始模块（HIM）的有效性。我们将它们与我们的基准模型进行比较，基准模型通过简单修改单词预测模块（如3.3节所述）从 SSD 框架扩展而来。表2 比较了 ICDAR 2013 的实验结果，精度，召回率和 f-measure。首先，我们将我们的基准模型与在文本数据上训练的原始 SSD 进行比较，而不对模型进行任何修改。我们的基准模型获得了显著的性能改进，并且胜过了最近的 TextBoxes，这也是从 SSD 扩展而来的。其次，所提出的 TAM 和 HIM 都在召回方面取得了很大的改进，表明所提出的文本特定模块可以提高模型的准确性并提高识别具有挑战性的单词的能力，如图4 和图6 所示。第三，通过将 TAM 和 HIM 整合到一个模型中，最终的单次探测器进一步提高了召回和精确度的性能，获得了 0.87 的最终 F-measure。这表明 TAM 和 HIM 都可以在我们的模型中互相补偿。请注意，ICDAR2013 的性能已经饱和，并且每个组件获得的改进都很显著。  \n",
    "\n",
    "### 4.3. Comparisons with state-of-the-art methods  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/7/8.jpg?raw=true) \n",
    "\n",
    "**定性结果** 图7 展示了很多非常具有挑战性的图像的检测结果，其中我们的检测器能够正确识别许多极具挑战性的单词，其中一些对于人类来说甚至是困难的。重要的是要指出，我们的探测器进行的单词级别检测特别准确，即使对于那些相互关闭的非常小规模的单词也是如此。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/7/10.jpg?raw=true) \n",
    "\n",
    "**在 ICDAR 2013上** 我们将我们的表现与最近公布的表3 中的结果进行比较。我们所有的结果都报告了单尺度测试图像。在设计用于近水平文本检测的 ICDAR 2013 上，我们的检测器在两种评估标准上都达到了最先进的性能。通过使用侧重于词级评估的 ICDAR 2013 标准，我们的方法获得了 0.87 的F-measure，优于所有其他比较方法，包括最近的 FCRN，CTPN 和 TextBoxes。通过使用 DetEval 标准，我们的方法可与 CTPN 相媲美，并且与其他方法相比，它们又有了很大的改进。它必须指出，DetEval 标准也允许进行文本行级别评估，并且不比 ICDAR 2013 标准严格。我们的方法鼓励在词级上进行更准确的检测，从而在 ICDAR 2013 标准中获得更好的性能。  \n",
    "\n",
    "此外，通过将我们的检测器与文献[6]中提出的最新词识别模型直接结合，进一步评估了我们在 ICDAR 2013 上进行端对端词语识别的方法。对于一般情况，我们获得 0.83 的准确度，这与最近的结果相当：[11]为0.79，[4]为0.85。  \n",
    "\n",
    "**在 ICDAR 2015 上** 在 ICDAR 2015 上验证了多方位文本工作的能力。我们的方法获得 0.77 的 F-measure，比 CTPN 取得的超过 0.61 的显着改进。这清楚地表明我们的方法比 CTPN 更好地推广多方位文本。在 ICDAR 2015 上，我们的方法在召回率，精度和 F-measure 方面获得了最先进的性能，超过了最近公布的[18]（0.71 F-measure）结果。  \n",
    "\n",
    "在所有实施中，我们的方法比其他方法获得更高的召回率。这表明所提出的文本特定模块使得我们的检测器能够以更高的词级精度检测极具挑战性的文本，如图7 所示，其中包括非常小的，显着的或高度模糊的单词。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/7/11.jpg?raw=true) \n",
    "\n",
    "**在 COCO 文本上** 我们在 COCO-Text 上进一步评估了我们的探测器，这是一个大型的文本数据集。我们以 0.37 的 F-分数获得了最先进的表现，这略微改善了近期的最新成果[32]。这显示了我们的方法在看不见的场景中在大规模图像上的强泛化能力。再一次，我们的方法比所有比较方法的召回率高得多。  \n",
    "\n",
    "**运行时间** 我们比较了 2013 年 ICDAR 上各种方法的运行时间（见表3）。我们的检测器使用单个 GPU 实现 0.13s/图像的运行时间，比使用 0.14s/图像的 CTPN 稍快。它比 TextBoxes 慢得多，但性能有显着提高。此外，TextBoxes 未在多方向文本上进行测试。此外，FCRN 以 0.07s/图像预测字框，但是对于生成的框进行后处理需要1.2s/图像。所有最近的基于 CNN 的方法都是在 GPU 时间进行比较，这已成为深度学习技术成功的主流。最快的基于 CPU 的文本检测器是 FASText，使用 0.15s/图像。但其性能明显低于最近的基于CNN的方法。  \n",
    "\n",
    "## 5. Conclusion  \n",
    "\n",
    "我们提供了一个快速而准确的文本检测器，可以一次预测字级边界框。我们提出了一种新的文本注意机制，它对训练中的文本进行强有力的监督信息编码。 这使模型能够自动学习文本注意图，该图可以隐含地识别测试中的粗略文本区域，从而使其基本上以有粗到细的方式工作。我们开发了一个 hierarchical inception 模块，可以有效地聚合多尺度初始特征。这通过编码更多的本地细节和更强的上下文信息进一步增强了卷积特性。这两种文本特定的开发结果都产生了一个功能强大的文本检测器，可以在多尺度和多方向的文本上可靠地工作。我们的方法在 ICDAR 2013，ICDAR 2015 和 COCO-Text 基准测试中取得了最新的最新成果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Curve Text in the Wild: New Dataset and New Solution  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "场景文本检测近年来取得了很大进展。检测方式正在从轴对准的矩形演变为旋转的矩形，并进一步演变为四边形。然而，目前的数据集包含很少的曲线文本，可以在场景图像中广泛观察到，例如 signboard，产品名称等。为了提高阅读野外环境曲线文本的能力，本文构建了一个名为 CTW1500 的曲线文本数据集，其中包含 1500 幅图像中的超过 10k 个文本标注（1000 个用于训练，500 个用于测试）。基于此数据集，我们开创性地提出了一种基于多边形的曲线文本检测器（CTD），它可以直接检测曲线文本而无需经验组合。此外，通过无缝集成横向和纵向偏移连接（TLOC），所提出的方法可以进行端到端的训练，以了解位置偏移之间的内在联系。这使得 CTD 能够探索上下文信息，而不是单独预测，从而导致更加平滑和准确的检测。我们还提出了两个简单而有效的后处理方法，称为非多边形抑制（NPS）和多边形非极大值抑制（PNMS），以进一步提高检测精度。此外，本文提出的方法是以通用的方式设计的，也可以用矩形或四边形边界框进行训练而无需额外的努力。CTW-1500 的实验结果表明，我们的方法仅具有轻微的骨架，可以大幅度超越最先进的方法。通过仅评估曲线或非曲线子集，CTD + TLOC 仍然可以获得最佳结果。代码可在 https://github.com/Yuliang-Liu/Curve-Text-Detector获得。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/8/1.jpg?raw=true) \n",
    "\n",
    "野外文本可以传达有价值的信息，可用于实时多语言翻译，行为分析，产品识别，汽车辅助等。最近，许多文本数据集的出现，这些文本数据集是在特定的任务和场景上构建的，为文本检测和识别方法的发展做出了重大贡献。有趣的是，据观察，来自新兴数据集的文本边界框的标签也从矩形发展到灵活的四边形。例如，ICDAR 2013“Focus Scene Text”和 SVT 中的水平矩形标签；在MSRA-TD500 和USTB-SV1K 中旋转矩形标签；ICDAR 2015“附带文本”，RCTW-17 和最近的 MLT 竞争数据集中的四个点标签。类似地，场景文本检测方法的改进也从基于轴对齐矩形改变为基于旋转矩形和基于四边形。文献[21]指出，一旦边界框变得更加紧密和灵活，它可以提高检测的置信度，降低被后处理抑制的风险并有利于后续的文本识别。  \n",
    "\n",
    "要识别场景文本，强烈要求文本可以事先严格和强健地进行定位。然而，目前的数据集只有很少的曲线文本，并且使用四边形（更不用矩形）来标记这些文本是有缺陷的。例如，如图1 所示，使用曲线包围盒有三个显著的优点：  \n",
    "\n",
    "- **避免不必要的重叠** 由于文本可能以多种方式出现，传统的四点定位可能无法很好地处理这种难以捉摸的特点。如图1（a）所示，四边形边界框无法避免产生的重叠，曲线边界框则可以避免。  \n",
    "\n",
    "- **较少的背景噪音** 如图1（b）所示，如果文本以曲线形式出现，则四边形边界框遭受背景噪声干扰。  \n",
    "\n",
    "- **避免多条文本行** 最近流行的识别方法都需要在每个边界框中有单行文本行。然而，在图1（c）的一些情况下，四边形包围盒不能避免多条文本线互相干扰，而曲线包围盒可以很好地解决这个问题。  \n",
    "\n",
    "事实上，曲线文字在我们的现实世界中也很常见。例如，大多数柱状物体（瓶子，石桩等），球形物体，折起的平面（衣服等），硬币，徽标，招牌等等中的文字。但是，据我们所知，目前的方法不能直接检测曲线文本。链接方法可以检测文本的组成部分，然后将它们分组在一起以匹配曲线边界框。但是，如果有许多文本叠加在一起，在很多情况下（如图1（a）），经验连接规则几乎不可能将小组件合理分组，这些方法在实践中总是会带来比直接检测方式更多的误报  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/8/2.jpg?raw=true) \n",
    "\n",
    "因此，在本文中，我们从各种自然场景，网站或图像库收集文本，例如 google open-image，并构建一个名为 CTW1500 的新数据集。该数据集包含 1500 多个包含超过 10k 文本注释的图像，每个图像至少包含一个曲线文本。为了评估和比较，我们将 1000 张图像分为训练集和 500 个进行测试。根据我们的观察，对于各种曲线文本区域，如图1 和图2 所示，对于各种曲线文本区域，可以使用 14 点多边形进行定位。通过使用参考的等宽线，不需要太多人力就可以标记为。  \n",
    "\n",
    "基于提出的数据集，我们提出了一种简单而有效的基于多边形的曲线文本检测器（CTD），它可能是一种可以直接检测曲线文本的开创性方法。与传统的检测方法不同，CTD 分离宽度/高度偏移预测的分支，可以以 13 FPS 的速度在 4GB 视频内存以下运行。此外，网络架构可以与我们提出的巧妙方法无缝集成，即横向和纵向偏移连接（TLOC），它使用 RNN 学习定位点之间的固有连接，使得检测更加准确和平滑。CTD 也被设计为通用方法，可以使用矩形和四边形边界框进行训练，无需额外的手动标签。为了进一步加强 CTD 的泛化能力，提出了两种简单而有效的后处理方法：非多边形抑制（NPS）和多边形非极大值抑制（PNMS）。  \n",
    "\n",
    "在所提出的数据集上，结果表明，使用轻度降低的 resnet-50 的 CTD 可以有效地检测曲线文本，并大幅度超越最先进的方法。通过使用 TLOC，我们的方法可以显著提高性能。此外，我们还对单纯曲线或非曲线测试子集（通过将其他文本作为不关心区域）进行评估，并且 CTD + TLOC 仍然可以获得最佳结果。  \n",
    "\n",
    "## 2. Related Work  \n",
    "\n",
    "在过去的十几年中，Scene 文本检测方法取得了很大的进步。这种不断进步的主要原因之一是基准数据集的发展-数据变得越来越难，数量越来越大，标签变得越来越紧密。从 2003 年起，ICDAR'03，ICDAR'11，ICDAR'13 和 COCO-Text 等矩形标记数据集吸引了大量的研究工作。2010 年以后，出现了旋转矩形标签（NEOCR，OSTD，MSRA-TD500 和 USTBSV1K）的多方位数据集，刺激了文献中多种有影响的多方位检测方法。2015 年，首个四边形标记数据集 ICDAR 2015“Incidental Scene 文本”出现，根据其评估网站以及最近的许多进展，空前引人注目。从那以后，在 ICDAR 2017 比赛中，像 RCTW-17（用于中英文文本的数据集），DOST（真实环境中的视频观看的场景文本）等大量更具挑战性的四边形标记数据集，和 MLT（用于多语言文本的数据集）似乎成为下一个主流数据集。  \n",
    "\n",
    "有趣的是，检测方式的发展也显示出与数据集类似的进化趋势。虽然矩形方法仍然受到关注，但它们似乎变得不那么主流。自 2011 年以来，几乎每年都有提出旋转矩形边界框的方法。在 2017 年，出现了丰富的基于四边形的检测方法。基本上，基于四边形的检测方法也可以通过旋转数据集或水平数据集（通过评估外接矩形）实现最佳性能，并且它们都显示出一个事实，即它们可以在多方向数据集中击败水平方式（通过使用外接矩形来训练和测试），特别是在召回率方面。这主要是因为对四边标记方法进行更强的监督可以避免很多背景噪音，不合理的抑制和信息丢失。Mask-RCNN 也可以找到更强的监督有助于检测的观点，通过与分割分支联合训练来提高检测结果，Ren 等人也表明训练与识别可以有利于文本检测。  \n",
    "\n",
    "然而，目前的文本检测方法，甚至是基于四边形的方法都在曲线文本中表现出令人失望的表现，这些文本通常出现在我们的现实世界中，如第1 节介绍的。像[27]这样的链接方法无法检测到强烈的弯曲文本（图3中的第二行图像）。一个原因是，所有当前数据集都包含非常少的曲线文本，并且许多曲线文本都标注了不令人满意的矩形。另一个原因是目前的基于四点的检测方法只能松散地检测曲线文本，这可能会引起严重的相互干扰，如图1（c）所示。因此，为了解决在野外检测曲线文本这一具有挑战性的问题，我们构建了一个新的基于曲线文本的数据集 CTW1500，并提出了一种能够有效检测曲线文本的新方法。  \n",
    "\n",
    "## 3. CTW1500 Dataset and Annotation  \n",
    "\n",
    "**数据描述** CTW1500 数据集包含 1500 个图像，10,751 个边界框（3,530 个是曲线边界框），每个图像至少有一个曲线文本。图像是从互联网，像谷歌开放图像和我们自己手机相机收集手动采集，其中还包含大量的水平和多方向文本。图像的分布多种多样，包括室内，室外，天生的数字，模糊，透视变形文本等等。此外，我们的数据集主要是中文和英文文本的多语种。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/8/3.jpg?raw=true) \n",
    "\n",
    "**标注** 我们用我们的标签工具手动标记文本。为了标记水平或四边形形状的文本，只需点击两次或四次即可。为了围绕曲线文本，我们创建了 10 条等距参考线，以帮助标注额外的 10 个点（实际上我们发现额外的 10 个点足以标注各种曲线文本，如图 2 所示）。我们使用等距线的原因是为了减轻标签工作量，并减少主观干扰。为了评估定位性能，我们只需遵循 PASCAL VOC 协议，该协议使用 0.5 IoU 阈值来确定真或假阳性。唯一的区别是我们计算多边形之间的精确交集（IoU），而不是轴对齐的矩形。  \n",
    "\n",
    "标记过程如图3 所示。首先，我们点击标记为 1,2,3,4 的四个顶点，并且将自动创建所引用的虚线（蓝色）。将鼠标的一条参考线（水平和垂直黑色虚线）移动到适当的位置（两条参考线的交点），然后单击以确定下一个点，以此类推剩下的点。我们粗略地计算了表1 中三种形式的文本的标注时间，其显示标记一个曲线文本比用四边形标记消耗大约三倍的时间。CTW1500 数据集可以在 https://github.com/Yuliang-Liu/Curve-Text-Detector下载。  \n",
    "\n",
    "## 4. Methodology  \n",
    "\n",
    "本节介绍我们的曲线文本检测器（CTD）的详细信息。我们首先将说明 CTD 的架构以及我们如何使用多边形标签。之后，我们将描述如何将循环神经网络（RNN）无缝连接到 CTD，并随后介绍此方法的普遍性。最后，我们将介绍我们的两个简单而有效的后处理方法，可以进一步提高性能。  \n",
    "\n",
    "### 4.1. Network Architecture  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/8/4.jpg?raw=true) \n",
    "\n",
    "我们的 CTD 的整体结构如图4 所示，可以分为三部分：主干，RPN 和回归模块。骨干通常采用 ImageNet 预训练的流行模型，然后使用相应的模型进行微调，如 VGG-16，ResNet等。区域建议网络（RPN）和回归模块分别连接到骨干；前者提出粗略的文本定位提议，后者则精细调整提议。  \n",
    "\n",
    "在本文中，我们使用轻量级的 ResNet-50（简单地删除最后一个残留块）作为我们的主干，这需要较少的内存并且可以更快。在 RPN 阶段，我们使用默认的矩形锚来召回文本，但是我们设置了非常松散的 RPN-NMS 阈值以避免过早抑制。为了检测具有多边形的曲线文本，CTD 只需要通过添加曲线定位点来修改回归模块，这受到 DMPNet 和 East 的启发，采用四边形回归分支与外接矩形回归分离。矩形分支可以很容易地通过网络学习，并让它快速交流，这也可以大致检测到高级文本区域，并缓解下面的回归。相反，四边形分支提供更强的监督，以指导网络更加准确。  \n",
    "\n",
    "类似于[25,21]，我们也回归每个点的相对位置。与[21]不同，我们使用外接矩形的最小 x 和最小 y 作为基准点。因此，每个点的相对长度 wi 和hi大于零，这在实践中更容易训练。另外，我们分别预测了偏移量 w 和 h，它们不仅可以减小参数，而且对于顺序学习更合理，如下一小节所介绍的。回归项目总数为 32；而 28 是 14 点的偏移量，4 是外接矩形的 x，y 最小值和最大值。下面列出了 14 个偏移（dwi和dhi）的参数：  \n",
    "\n",
    "$\n",
    "\\begin{cases} \n",
    "d_{w_i} = \\frac {p^*_{w_i} - p_{w_i}}{w_{chr}} \\\\ \n",
    "d_{h_i} = \\frac {p^*_{h_i} - p_{h_i}}{h_{chr}}\n",
    "\\end{cases}  \\text {$i \\in (1,2,...,14)$} \\tag{1}\n",
    "$\n",
    "\n",
    "其中，$p^*$ 和 p 分别是真实偏移和预测的偏移量。此外，wchr 和 hchr 是外接矩形的宽度和高度。对于边界回归，我们遵循与 Faster R-CNN 的一样的处理流程。值得注意的是，28 个值足以确定 14 个点的位置，但在相对回归模式下，32 个值可以更容易地检索剩下的 14 个点并提供更强的监督。  \n",
    "\n",
    "### 4.2. Recurrent Transverse and Longitudinal Offset Connection (TLOC)  \n",
    "\n",
    "CTPN 证明了在文本检测任务中使用循环连接能够学习潜在的微小提议序列，并产生更好的结果。但是，CTPN 是一种基于链接的方法，需要经验性连接。此外，其连接主义方案需要固定的图像大小以确保 RNN 输入时间序列的固定数量。与 CTPN 不同，我们的方法可以直接定位曲线区域而无需外部连接，并且RNN 的时间序列数量不受输入图像大小的限制。这是因为 RNN 连接到 position sensitiveRoI Pooling（PSROIPooling）的输出，并且输出目标的数量是固定的（14个宽度偏移和14个高度偏移）。基本上，PSROIPooling 用于预测和投票类概率和定位偏移量，它们将每个 RoI 均匀分配到 p×p 个 bins 以估计位置信息。输入卷积层的维度应该是$(class + 1) p^2$，因此 PSROIPooling 可以为每个类别生成一个 $p^2$ 评分图。对于横向和纵向偏移预测，我们移除背景类定位分数图并使用 7×7 的 bin，因此输入卷积维数为 14×7×7。$(1;j)$-th bin中的每个值是通过使用平均池化从第 $(1;j)$ 个分数图中的对应位置计算出的：  \n",
    "\n",
    "$r_c(i,j|\\Theta) = \\sum_{(x,y) \\in bin(i,j)} \\frac{s_{i,j,c}(x + x_{min},y + y_{min}|\\Theta)}{n} \\tag{2}$\n",
    "\n",
    "其中，$r_c(i,j|\\Theta)$ 是类别 c 的第 $(i,j)$ 个 bin 中的 pool 值，$c,s_{i,j,c}$ 表示来自对应维度的分数图。$(x_{min},y_{min})$ 表示 RoI 的左上坐标，n 表示 bin 中像素的数量，$\\Theta$ 表示所有网络参数。在 PSROIPooling 过程之后，CTD 将通过在 $p^2$ 位置敏感分数图上的全局pooling 获得每个 RoI 的分数或估计偏移量：$r_c(\\Theta) = \\sum \\frac{r_c(i,j|\\Theta)}{p^2}$，其产生 $(C+1)$ 维向量。然后通过所有类别的 softmax 操作计算投票类别分数并输出最终置信度：  \n",
    "\n",
    "$s_c(\\Theta) = e^{r_c(\\Theta)} / \\sum_{c'=0}^C e^r c'^{\\Theta} \\tag{3}$\n",
    "\n",
    "定位偏移量将被输入到定位损失函数中。在训练阶段，我们选择类似的多任务损失函数进行评分和偏移预测，如下所示：  \n",
    "\n",
    "$L(c,c^*,b,b^*,w,w^*,h,h^*) = 1/N (\\lambda \\times L_{cls}(c,c^*) + L_{loc}(b,b^*)) + \\mu/N_p (L_{loc}(h,h^*) + L_{loc}(w,w^*)) \\tag{4}$\n",
    "\n",
    "其中 N 是与特定重叠范围匹配的正面和负面提议的数量，Np 是正面提议的数量，因为没有必要改进负面提议。此外，$\\lambda$ 和 $\\mu$ 是衡量分类和检测损失之间重要性的平衡因子（Lcls表示分类损失函数; Lloc是定位损失函数，可以是平滑 L1 损失或平滑 Ln 损失）。实际上，我们将 $\\lambda$ 设置为 3 或更多，以平衡具有更多目标的定位损失。此外（c，b，w，h）分别表示预测类，估计边界框，宽度和高度偏移量，$(c^*,b^*,w^*,h^*)$表示相应的真实情况。\n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/8/5.jpg?raw=true) \n",
    "\n",
    "为了提高检测性能，我们将横向和纵向分支分开以预测用于定位文本区域的偏移量。直观地说，每个点都受到最后和下一个点以及文本区域的限制。例如，在图3 情况下，第六标记点的偏移宽度应大于第五点并小于第七点。独立预测每个偏移量可能会导致文本区域不光滑，并且不知何故它会带来更多的错误检测。因此，我们假设每个点的宽度/高度都有关联的上下文信息，并使用 RNN 学习它们的潜在特征。我们将此方法命名为循环横向和纵向偏移连接（TLOC）。TLOC 结构在图4 中以紫色模式显示，我们还列出了图5 中的一些示例，以显示是否添加 TLOC 的差异。要采用 TLOC，我们发现 PSROIPooling 的输出适合编码偏移上下文信息。以宽度偏移分支为例，首先，PSROIPooling 输出 14 个 $p^2$ 分数图用于每个提议的投票 w1，...，w14，并且第 i 个分数图的 $p^2$ bins 具有来自各个位置的 $p^2$ 个投票值，其可以被编码为 wi；RNN 比每个点的宽度偏移特征作为顺序输入，并且循环地更新隐藏层内的固有状态 Lt，即  \n",
    "\n",
    "$L_t = \\phi(L_{t-1},)_t), t = 1,2,...,14 \\tag(5)$\n",
    "\n",
    "其中 $O_t\\in R^{P^2}2$ 是来自对应的 psroi-pooling 输出通道的第 t 个预测偏移量。Lt 是从当前输入（Ot）和在 Lt-1 中编码的最后状态计算的循环内部状态。通过使用非线性函数 $\\phi$ 来计算递推，其中我们采用双向长期短期记忆（BLSTM）架构作为我们的 RNN。RNN 隐藏层内的内部状态将连续的上下文信息与所有先前估计的偏移量通过循环连接相关联，并且我们凭经验使用 256D BLSTM 隐藏层，因此 $Lt\\in R^{256}$。最后，BLSTM 的输出是一个14维 1×256 向量，它被全局（1×256）内核汇集起来以输出最终预测。  \n",
    "\n",
    "### 4.3. Long Side Interpolation  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/8/6.jpg?raw=true) \n",
    "\n",
    "正如 4.1 介绍的那样，对于每个边界框，CTD 输出 14 个点的偏移量。但是，几乎所有当前的基准数据集都只有两个或四个顶点信息标签。为了训练这些数据集，我们可以很容易地插入最大侧和相反侧的等分点，如图6 所示。  \n",
    "\n",
    "有希望的是，通过简单地插入边界框中的点，我们的 CTD 可以用所有文本区域进行有效训练，并且可以实现更好的结果，如实验部分所示。  \n",
    "\n",
    "### 4.4. Polygonal Post Processing  \n",
    "\n",
    "**非多边形抑制（NPS）** 假阳性检测结果是制约文本检测性能的重要原因之一。但是，在 CTD 中，一些不一致的误报会出现无效的形状（对于有效的多边形，没有任何相交的边）。另外，相交面几乎没有任何场景文字出现，这些无效的多边形几乎不可能识别。因此，我们简单地抑制所有这些无效的多边形，并将其命名为非多边形抑制（NPS），它可以在不影响召回率的情况下稍微提高准确度。  \n",
    "\n",
    "**多边形非最大抑制（PNMS）** 非极大值抑制被证明对目标检测任务非常有效。由于曲线场景文本的特殊性，矩形 NMS 仅限于处理如图1（a）和（c）所示的密集多方向文本。为了解决这个问题，[38]提出了一个局部感知的 NMS，[4]设计了一个 Mask-NMS 来抑制最终的输出结果。在本文中，我们还通过计算多边形之间的重叠区域（称为多边形非最大抑制（PNMS））来改进 NMS，这在以下实验中证明是有效的。  \n",
    "\n",
    "## 5. Experiments  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/8/8.jpg?raw=true) \n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/8/9.jpg?raw=true) \n",
    "\n",
    "在本节中，我们将对建议的 CTW1500 数据集进行实验以测试我们的方法。测试环境为 Ubuntu 16.04 64bit，配有单个 Nvidia 1080 GPU。所有的文本检测结果都使用第3 节中介绍的协议进行评估，该协议准确计算多边形之间的 IoU 而不是轴对齐矩形。为了公平比较，所有实验仅使用提供的训练数据而没有任何数据增加。  \n",
    "\n",
    "TLOC 和 PNMS 的有效性。我们首先评估提议的 TLOC 和 PNMS 的可用性，结果在表3 中给出。在这个表格中，我们可以发现无论使用 CTD 还是 CTD+TLOC，PNMS 总是可以略微超越经典的 NMS。另一方面，结果还表明，通过简单地添加 TLOC，所提出的 CTD 在 Hmean 方面可以提高约 4%。请注意，我们并没有比较 NPS，因为它是评估多边形重叠区域的先决条件后处理方法，可以稍微提高精度。  \n",
    "\n",
    "与最先进的方法进行比较。为了全面评估我们的方法，我们将提出的 CTD 和 CTD+TLOC 与几种最先进的和通用的文本检测方法进行比较。请注意，对于East 和 Seglink，我们基于来自 github 的非官方源代码重新实现这些方法，并且为了公平比较，我们不使用庞大的合成数据来预训练可能降低性能的seglink。对于 DMPNet 和 CTPN，我们都使用 Caffe 框架来重新实现这些方法。此外，因为这些方法都不能用曲线文本区域进行训练，所以我们遵循传统的外接矩形边界框来使得标签能够被训练。表2 列出了实验结果。整个 CTW-1500 测试集的结果表明，就 Hmean 而言，所提出的 CTD+TLOC 可超过最先进的方法超过 10%。为了进一步评估效果，我们还将整个测试集中的曲线和非曲线文本分开，只需将其他类型的文本简化为（不在意）并进行比较。有希望的是，在曲线子集中，所提出的 CTD+TLOC 可以在至少 28% 的 Hmean 的情况下胜过最先进的方法，同时，该方法在仅检测非曲线文本方面也可以取得最好的结果，表明其稳健性和普遍性。请注意，对于我们的方法，子集的结果都小于整个集合，因为将其他文本视为困难的方式会降低精度。  \n",
    "\n",
    "此外，我们比较了表2 最后一栏的检测速度和结果（13.3或15.2 FPS），结果表明我们的方法是第二快的方法，这也证明了我们方法的有效性。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/8/7.jpg?raw=true) \n",
    "\n",
    "检测结果的例子在图7 中可见。在该图的最后一列中，我们还使用 CTD 来检测来自其他数据集的曲线文本，这定性地表明其检测曲线文本的强大能力以及其泛化能力  \n",
    "\n",
    "## 6. Conclusions and Future Work  \n",
    "\n",
    "曲线文本在我们的现实世界中很常见，但目前很少有数据集或方法针对曲线文本检测。为了便于在野外阅读曲线文本这一新的具有挑战性的研究，本文提出了一种新的数据集 CTW1500，它是一种主要由曲线文本构成的新型数据集。这个数据集中的曲线文本用多边形近似标记，不需要太多人力。另外，我们提出了一种新的 CTD 方法，可能是第一次尝试直接检测曲线文本。通过设计横向和纵向偏移连接（TLOC）方法，CTD 可以与 RNN 无缝连接，这显着提高了检测性能。我们还提出了一种简单但有效的长边插值技术，它使 CTD 成为一种通用方法，不需要额外的手动操作，也可以用矩形或四边形边界框进行训练。最后，我们设计了两种后处理方法，这些方法也被证明是有效的。  \n",
    "\n",
    "将来，所提出的数据集可以被放大为基于曲线文本的识别数据集，因为标记方式似乎对识别有好处。此外，尽管像我们的CTD那样灵活的检测方式可能比基于矩形的刚性检测器稍慢，但前者可以解决检测曲线文本和取得更好结果等更复杂的问题，值得进一步探讨。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考  \n",
    "\n",
    "- 1 [An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition](https://arxiv.org/abs/1507.05717) \n",
    "- 2 [CRNN论文翻译——中英文对照](https://blog.csdn.net/Quincuntial/article/details/77679419)  \n",
    "- 3 [STN-OCR: A single Neural Network for Text Detection and Text Recognition](https://arxiv.org/abs/1707.08831)  \n",
    "- 4 [Synthetic Data for Text Localisation in Natural Images](https://arxiv.org/abs/1604.06646)  \n",
    "- 5 [Detecting Text in Natural Image with Connectionist Text Proposal Network](https://arxiv.org/abs/1609.03605)  \n",
    "- 6 [TextBoxes++: A Single-Shot Oriented Scene Text Detector](https://arxiv.org/abs/1801.02765)  \n",
    "- 7 [EAST: An Efficient and Accurate Scene Text Detector](https://arxiv.org/abs/1704.03155)\n",
    "- 8 [Single Shot Text Detector with Regional Attention](https://arxiv.org/abs/1709.00138)  \n",
    "- 9 [Detecting Curve Text in the Wild: New Dataset and New Solution](https://arxiv.org/abs/1712.02170)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
