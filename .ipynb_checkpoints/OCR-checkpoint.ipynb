{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "基于图像的序列识别一直是计算机视觉中长期存在的研究课题。在本文中，我们研究了场景文本识别的问题，这是基于图像的序列识别中最重要和最具挑战性的任务之一。提出了一种将特征提取，序列建模和 transcription 整合到统一框架中的新型神经网络架构。与之前的场景文本识别系统相比，所提出的架构具有四个不同的特性：（1）与大多数现有的组件需要单独训练和协调的算法相比，它是端对端训练的。（2）它自然地处理任意长度的序列，不涉及字符分割或水平尺度标准化。（3）它不仅限于任何预定义的词汇，并且在无词典和基于词典的场景文本识别任务中都取得了显著的表现。（4）它产生了一个有效而小得多的模型，这对于现实世界的应用场景更为实用。在包括 IIIT-5K，Street View Text 和 ICDAR 数据集在内的标准基准数据集上的实验证明了提出的算法比现有的技术更有优势。此外，提出的算法在基于图像的音乐乐谱识别任务中表现良好，这显然证实了模型的泛化能力。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "最近，社区已经看到神经网络的强大复兴，这主要受到深度神经网络模型，特别是深度卷积神经网络（DCNN）在各种视觉任务中的巨大成功的推动。然而，最近大多数与深度神经网络相关的工作主要致力于检测或对象分类。在本文中，我们关注计算机视觉中的一个经典问题：基于图像的序列识别。在现实世界中，稳定的视觉对象，如场景文字，手写字符和乐谱，往往以序列的形式出现，而不是孤立地出现。与一般的对象识别不同，识别这样的序列化对象通常需要系统预测一系列对象标签，而不是单个标签。因此，可以自然地将这样的对象的识别作为序列识别问题。序列化对象的另一个独特之处在于它们的长度可能会有很大变化。例如，英文单词可以由 2 个字符组成，如“OK”，或由 15 个字符组成，如“congratulations”。因此，最流行的深度模型像 DCNN 不能直接应用于序列预测，因为 DCNN 模型通常对具有固定维度的输入和输出进行操作，因此不能产生可变长度的标签序列。  \n",
    "\n",
    "已经针对特定的序列化对象（例如场景文本）进行了一些尝试来解决该问题。例如，[35,8]中的算法首先检测单个字符，然后用 DCNN 模型识别这些检测到的字符，并使用标注的字符图像进行训练。这些方法通常需要训练强字符检测器，以便从原始单词图像中准确地检测和裁剪每个字符。一些其他方法（如[22]）将场景文本识别视为图像分类问题，并为每个英文单词（总共 9 万个词）分配一个标签，结果是一个大的训练模型中有很多类，这很难泛化到其它类型的序列化对象，如中文文本，音乐配乐等，因为这种序列的基本组合数目可能大于 100 万。总之，目前基于 DCNN 的系统不能直接用于基于图像的序列识别。  \n",
    "\n",
    "循环神经网络（RNN）模型是深度神经网络家族中的另一个重要分支，主要设计来处理序列数据。RNN 的优点之一是在训练和测试中不需要序列目标图像中每个元素的位置。然而，将输入目标图像转换成图像特征序列的预处理步骤通常是必需的。例如，Graves 等人从手写文本中提取一系列几何或图像特征，而 Su 和 Lu 将字符图像转换为序列 HOG 特征。预处理步骤独立于流程中的后续组件，因此基于 RNN 的现有系统不能以端到端的方式进行训练和优化。 \n",
    "\n",
    "一些不是基于神经网络的传统场景文本识别方法也为这一领域带来了 insightful 的想法和新颖的效果。例如，Almazan 等人和 Rodriguez-Serrano 等人提出将单词图像和文本字符串嵌入到公共向量子空间中，并将词识别转换为检索问题。Yao 等人和 Gordo 等人使用中层特征进行场景文本识别。虽然在标准基准数据集上取得了有效的性能，但是前面的基于神经网络的算法以及本文提出的方法通常都优于这些方法。  \n",
    "\n",
    "本文的主要贡献了一种新颖的神经网络模型，其网络架构专门用于识别图像中的序列化对象。所提出的神经网络模型被称为卷积循环神经网络（CRNN），因为它是 DCNN 和 RNN 的组合。对于序列化对象，CRNN 与传统神经网络模型相比具有一些独特的优点：1）可以直接从序列标签（例如单词）学习，不需要详细的标注（例如字符）；2）直接从图像数据学习信息表示时具有与 DCNN 相同的性质，既不需要手工特征也不需要预处理步骤，包括二值化/分割，组件定位等；3）具有与 RNN 相同的性质，能够产生一系列标签；4）对类序列对象的长度无约束，只需要在训练阶段和测试阶段对高度进行归一化；5）与现有技术相比，它在场景文本（字识别）上获得更好或更具竞争力的表现。6）它比标准 DCNN 模型包含的参数要少得多，占用更少的存储空间。  \n",
    "\n",
    "## 2. The Proposed Network Architecture  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/1.jpg?raw=true)  \n",
    "\n",
    "如图 1 所示，CRNN 的网络架构由三部分组成，包括卷积层，循环层和 transcription 层，从底向上。  \n",
    "\n",
    "在 CRNN 的底部，卷积层自动从每个输入图像中提取特征序列。在卷积网络之上，构建了一个循环网络，用于对卷积层输出的特征序列的每一帧进行预测。CRNN 顶部的 transcription 层将循环层的每帧预测转化为标签序列。虽然 CRNN 由不同类型的网络架构（如 CNN 和 RNN）组成，但可以通过一个损失函数进行联合训练。   \n",
    "\n",
    "### 2.1. Feature Sequence Extraction  \n",
    "\n",
    "在 CRNN 模型中，通过采用标准 CNN 模型（去除全连接层）中的卷积层和最大池化层来构造卷积层的组件。这样的组件用于从输入图像中提取序列特征表示。在进入网络之前，所有的图像需要缩放到相同的高度。然后从卷积层组件产生的特征图中提取特征向量序列，这些特征向量序列作为循环层的输入。具体地，特征序列的每一个特征向量在特征图上按列从左到右生成。这意味着第 i 个特征向量是所有特征图第 i 列的拼接。在我们的设置中每列的宽度固定为单个像素。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/2.jpg?raw=true)  \n",
    "\n",
    "由于卷积层，最大池化层和元素激活函数在局部区域上执行，因此它们是平移不变的。因此，特征图的每列对应于原始图像的一个矩形区域，并且这些矩形区域与特征图上从左到右的相应列具有相同的顺序。如图 2 所示，特征序列中的每个向量关联一个感受野，并且可以被认为是该区域的图像描述符。  \n",
    "\n",
    "鲁棒的，丰富的和可训练的深度卷积特征已被广泛应用于各种视觉识别任务。一些以前的方法已经使用 CNN 来学习诸如场景文本之类的序列化对象的鲁棒表示。然而，这些方法通常通过 CNN 提取整个图像的整体表示，然后收集局部深度特征来识别序列化对象的每个分量。由于 CNN 要求将输入图像缩放到固定尺寸，以满足其固定的输入尺寸，因为它们的长度变化很大，因此不适合序列化对象。在 CRNN 中，我们将深度特征传递到序列表示中，以便对类序列对象的长度变化保持不变。  \n",
    "\n",
    "### 2.2. Sequence Labeling  \n",
    "\n",
    "一个深度双向循环神经网络是建立在卷积层的顶部，作为循环层。循环层预测特征序列 $x=x_1,...,x_T$ 中每一帧 $x_t$ 的标签分布 $y_t$。循环层的优点有三个。首先，RNN 具有很强的捕获序列内上下文信息的能力。对于基于图像的序列识别使用上下文提示比独立处理每个符号更稳定且更有帮助。以场景文本识别为例，宽字符可能需要一些连续的帧来完全描述（参见图 2）。此外，一些模糊的字符在观察其上下文时更容易区分，例如，通过对比字符高度更容易识别“il”而不是分别识别它们中的每一个。其次，RNN 可以将误差差值反向传播到其输入，即卷积层，从而允许我们在统一的网络中共同训练循环层和卷积层。第三，RNN 能够从头到尾对任意长度的序列进行操作。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/3.jpg?raw=true)  \n",
    "\n",
    "传统的 RNN 单元在其输入和输出层之间具有自连接的隐藏层。每次接收到序列中的帧 $x_t$ 时，它将使用非线性函数来更新其内部状态 $h_t$，该非线性函数同时接收当前输入 $x_t$ 和过去状态 $h_{t−1}$ 作为其输入：$h_t = g(x_t,h_{t-1})$。那么预测 $y_t$ 是基于 $h_t$ 的。以这种方式，过去的上下文 $\\{x_{t'}\\}_{t'< t}$ 被捕获并用于预测。然而，传统的 RNN 单元有梯度消失的问题，这限制了其可以存储的上下文范围，并给训练过程增加了负担。长短时记忆（LSTM）是一种专门设计用于解决这个问题的 RNN 单元。LSTM（图 3 所示）由一个存储单元和三个多重门组成，即输入，输出和遗忘门。在概念上，存储单元存储过去的上下文，并且输入和输出门允许单元长时间地存储上下文。同时，单元中的存储可以被遗忘门清除。LSTM 的特殊设计允许它捕获长距离依赖，这经常发生在基于图像的序列中。  \n",
    "\n",
    "LSTM 是定向的，它只使用过去的上下文。然而，在基于图像的序列中，两个方向的上下文是相互有用且互补的。因此，我们遵循[17]，将两个 LSTM，一个向前和一个向后组合到一个双向 LSTM 中。此外，可以堆叠多个双向 LSTM，得到如图 3.b 所示的深双向 LSTM。深层结构允许比浅层抽象更高层次的抽象，并且在语音识别任务中取得了显著的性能改进。  \n",
    "\n",
    "在循环层中，误差在图 3.b 所示箭头的相反方向传播，即反向传播时间（BPTT）。在循环层的底部，传播差异的序列被连接成映射，将特征映射转换为特征序列的操作进行反转并反馈到卷积层。实际上，我们创建一个称为“Map-to-Sequence”的自定义网络层，作为卷积层和循环层之间的桥梁。  \n",
    "\n",
    "### 2.3. Transcription\n",
    "\n",
    "转录是将 RNN 所做的每帧预测转换成标签序列的过程。数学上，转录是根据每帧预测找到具有最高概率的标签序列。在实践中，存在两种转录模式，即无词典转录和基于词典的转录。词典是一组标签序列，预测受拼写检查字典约束。在无词典模式中，预测时没有任何词典。在基于词典的模式中，通过选择具有最高概率的标签序列进行预测。  \n",
    "\n",
    "#### 2.3.1 Probability of label sequence  \n",
    "\n",
    "我们采用 Graves 等人提出的 Connectionist Temporal Classification（CTC）层中定义的条件概率。按照每帧预测结果 $y = y_1,...,y_T$ 对计算标签序列 $l$ 的概率，并忽略 $l$ 中每个标签所在的位置。因此，当我们使用这种概率的负对数似然作为训练网络的目标函数时，我们只需要图像及其相应的标签序列，避免了标注单个字符位置的劳动。  \n",
    "\n",
    "条件概率的公式简要描述如下：输入是序列 $y = y_1,...,y_T$，其中 T 是序列长度。这里，每个 $y_t \\in R^{\\mid L' \\mid}$ 是在集合 $L' = L \\bigcup$ 上的概率分布，其中 L 包含了任务中的所有标签（例如，所有英文字符），以及由 - 表示的“空白”标签。序列到序列的映射函数 BB 定义在序列 $\\pi \\in L'^T$ 上，其中 T 是长度。BB 通过首先删除重复的标签，然后删除 blank，将 $\\pi$ 映射到 $l$ 上。例如，BB 将“–hh-e-l-ll-oo–”（- 表示blank）映射到“hello”。然后，条件概率被定义为由 BB 将所有 $\\pi$ 映射到 $l$ 上的概率之和：  \n",
    "\n",
    "$p(l|y) = \\sum_{\\pi:B(\\pi)=1} p(\\pi|y) \\tag{1}$\n",
    "\n",
    "其中，$\\pi$ 的概率定义为 $p(\\pi|y) = \\quad_{t=1}{T} y_{\\pi_t}^t, \\ y_{\\pi_t}^t$ 是时刻 t 时有标签 $\\pi_t$ 的概率。由于存在指数级数量的求和项，直接计算上述方程在计算上是不可行的。然而，使用[15]中描述的前向算法可以有效的进行计算。  \n",
    "\n",
    "#### 2.3.2 Lexicon-free transcription  \n",
    "\n",
    "在这种模式下，将具有方程 1 中定义的最高概率的序列 $l^*$ 作为预测。由于不存在用于精确找到解的可行方法，我们采用[15]中的策略。序列 $l^*$ 通过 $l^* \\approx B(arg max_{\\pi} p(\\pi|y))$ 近似发现，e.g 在每个时间戳 t 采用最大概率的标签 $\\pi_t$，并将结果序列映射到 $l^*$。  \n",
    "\n",
    "#### 2.3.3 Lexicon-based transcription  \n",
    "\n",
    "在基于字典的模式中，每个测试样本与词典 D 相关联。基本上，通过选择词典中具有方程 1 中定义的最高条件概率的序列来识别标签序列。然而，对于大型词典，例如 5 万个词的 Hunspell 拼写检查词典，对词典进行详尽的搜索是非常耗时的，即对词典中的所有序列计算方程 1，并选择概率最高的一个。为了解决这个问题，我们观察到，2.3.2 中描述的通过无词典转录预测的标签序列通常在编辑距离度量下接近于实际结果。这表示我们可以将搜索限制在最近邻候选目标 $N_{\\delta}(l')$，其中 $\\delta$ 是最大编辑距离，$l'$ 是在无词典模式下从 y 转录的序列：  \n",
    "\n",
    "$l^* = arg max_{l \\in N_{\\delta}(l')} p(l|y) \\tag{2}$\n",
    "\n",
    "可以使用 BK 树数据结构有效地找到候选目标 $N_{\\delta}(l')$，这是一种专门适用于离散度量空间的度量树。BK 树的搜索时间复杂度为 $O(log|D|)$，其中 |D| 是词典大小。因此，这个方案很容易扩展到非常大的词典。在我们的方法中，一个词典离线构造一个 BK 树。然后，我们使用树执行快速在线搜索，通过查找具有小于或等于 $\\delta$ 编辑距离来查询序列。\n",
    "\n",
    "### 2.4. Network Training  \n",
    "\n",
    "用 $X = \\{I_i,l_j\\}_i$ 表示训练数据集，其中 $I_i$ 是训练图像，$l_i$ 是真实标签序列。目标是最小化真实条件概率的负对数似然：  \n",
    "\n",
    "$O = - \\sum_{I_i,l_i \\in X} \\log p(l_i|y_i) \\tag{3}$\n",
    "\n",
    "$y_i$ 是循环层和卷积层从 $I_i$ 生成的序列。目标函数直接从图像和它的真实标签序列计算代价值。因此，网络可以在成对的图像和序列上进行端对端训练，去除了在训练图像中手动标记所有单独组件的过程。  \n",
    "\n",
    "用随机梯度下降（SGD）训练网络。梯度由反向传播算法计算。特别是，在转录层中，错误差异使用前向后向算法反向传播，如[15]中所述。在循环层中，应用反向传播时间（BPTT）来计算误差差异。  \n",
    "\n",
    "为了优化，我们使用 ADADELTA 自动计算每维度学习率。与传统动量方法相比，ADADELTA 不需要手动设置学习速率。更重要的是，我们发现使用 ADADELTA的优化比动量方法更快收敛。  \n",
    "\n",
    "## 3. Experiments  \n",
    "\n",
    "为了评估提出的 CRNN 模型的有效性，我们在场景文本识别和乐谱识别的标准基准数据集上进行了实验，这些都是具有挑战性的视觉任务。数据集和训练测试的设置见 3.1 小节，场景文本图像中 CRNN 的详细设置见 3.2 小节，综合比较的结果在 3.3 小节报告。为了进一步证明 CRNN 的泛化性，在 3.4 小节我们在乐谱识别任务上验证了提出的算法。  \n",
    "\n",
    "### 3.1. Datasets  \n",
    "\n",
    "对于场景文本识别的所有实验，我们使用 Jaderberg 等人发布的合成数据集（Synth）作为训练数据。数据集包含 8 百万训练图像及其对应的实际单词。这样的图像由合成文本引擎生成并且是非常现实的。我们的网络在合成数据上进行了一次训练，并在所有其它现实世界的测试数据集上进行了测试，而没有在其训练数据上进行任何微调。即使 CRNN 模型是在纯合成文本数据上训练，但它在标准文本识别基准数据集的真实图像上工作良好。  \n",
    "\n",
    "有四个流行的基准数据集用于场景文本识别的性能评估，即 ICDAR 2003（IC03），ICDAR 2013（IC13），IIIT 5k-word（IIIT5k）和 Street View Text (SVT)。  \n",
    "\n",
    "IC03 测试数据集包含 251 个具有标记文本边界框的场景图像。我们忽略包含非字母数字字符或少于三个字符的图像，并获得具有 860 个裁剪的文本图像的测试集。每张测试图像与由 Wang 等人定义的 50 词的词典相关联。通过组合所有的每张图像词汇构建完整的词典。此外，我们使用由 Hunspell 拼写检查字典中的单词组成的 5 万个词的词典。  \n",
    "\n",
    "IC13 测试数据集继承了 IC03 中的大部分数据。它包含 1015 个实际的裁剪单词图像。  \n",
    "\n",
    "IIIT5k 包含从互联网收集的 3000 张裁剪的词测试图像。每张图像关联一个 50 词的词典和一个 1000 词的词典。  \n",
    "\n",
    "SVT 测试数据集由从 Google 街景视图收集的 249 张街景图像组成。从它们中裁剪出了 647 张词图像。每张单词图像都有一个由 Wang 等人定义的 50 个词的词典。  \n",
    "\n",
    "### 3.2. Implementation Details  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/4.jpg?raw=true)  \n",
    "\n",
    "在实验中我们使用的网络配置总结在表 1 中。卷积层的架构是基于 VGG-VeryDeep 的架构。为了使其适用于识别英文文本，对其进行了调整。在第 3 和第4 个最大池化层中，我们采用 1×2 大小的矩形池化窗口而不是传统的平方形。这种调整产生宽度较大的特征图，因此具有更长的特征序列。例如，包含 10 个字符的图像通常为大小为 100×32，可以从其生成 25 帧的特征序列。这个长度超过了大多数英文单词的长度。最重要的是，矩形池窗口产生矩形感受野（如图 2 所示），这有助于识别一些具有窄形状的字符，例如 i 和 l。  \n",
    "\n",
    "不仅有卷积层，而且还有循环层。众所周知两者都难以训练。我们发现批归一化技术对于训练这种深度网络非常有用。分别在第 5 和第 6 卷积层之后插入两个批归一化层。使用批归一化层训练过程大大加快。  \n",
    "\n",
    "我们在 Torch7 框架内实现了网络，使用定制实现的 LSTM 单元（Torch7/CUDA），转录层（C++）和 BK 树数据结构（C++）。实验在具有 2.50 GHz Intel（R）Xeon E5-2609 CPU，64GB RAM 和 NVIDIA（R）Tesla(TM) K40 GPU 的工作站上进行。网络用 ADADELTA 训练，将参数ρ设置为 0.9。在训练期间，所有图像都被缩放为 100×32，以加快训练过程。训练过程大约需要 50 个小时才能达到收敛。测试图像缩放的高度为 32。宽度与高度成比例地缩放，但至少为 100 像素。平均测试时间为 0.16s/样本，在 IC03 上测得的，没有词典。近似词典搜索应用于 IC03 的 50k 词典，参数 $delta$ 设置为3。测试每个样本平均花费 0.53s。    \n",
    "\n",
    "### 3.3. Comparative Evaluation  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/6.jpg?raw=true)  \n",
    "\n",
    "提出的 CRNN 模型在上述四个公共数据集上获得的所有识别精度以及最近的包括基于深度模型的最新技术的识别精度，如表 2 所示。  \n",
    "\n",
    "在有约束词典的情况中，我们的方法始终优于大多数最新的方法，并且平均打败了[22]中提出的最佳文本阅读器。具体来说，与[22]相比，我们在 IIIT5k 和 SVT 上获得了卓越的性能，仅在 IC03 上通过“Full”词典实现了较低性能。请注意，[22]中的模型是在特定字典上训练的，即每个单词都与一个类标签相关联。与[22]不同，CRNN 不限于识别已知字典中的单词，并且能够处理随机字符串（例如电话号码），句子或其他诸如中文单词的脚本。因此，CRNN 的结果在所有测试数据集上都具有竞争力。  \n",
    "\n",
    "在无约束词典的情况下，我们的方法在 SVT 上仍取得了最佳性能，但在 IC03 和 IC13 上仍然落后于一些方法。注意，表 2 的“none”列中的空白表示这种方法不能应用于没有词典的识别，或者在无约束的情况下不能报告识别精度。我们的方法只使用具有单词级标签的合成文本作为训练数据，与PhotoOCR 非常不同，后者使用 790 万个具有字符级标注的真实单词图像进行训练。[22]中报告的最佳性能是在无约束词典的情况下，受益于它的大字典，然而，它不是前面提到的严格的无约束词典模型。在这个意义上，我们在无限制词典表中的结果仍然是有前途的。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/5.jpg?raw=true)  \n",
    "\n",
    "为了进一步了解与其它文本识别方法相比，所提出算法的优点，我们提供了在一些特性上的综合比较，这些特性名称为 E2E Train，Conv Ftrs，CharGT-Free，Unconstrained 和 Model Size，如表 3 所示。  \n",
    "\n",
    "E2E Train：这一列是为了显示某种文字阅读模型是否可以进行端到端的训练，无需任何预处理或经过几个分离的步骤，这表明这种方法对于训练是优雅且干净的。从表 3 可以看出，只有基于深度神经网络的模型，包括[22,21]以及CRNN具有这种性质。  \n",
    "\n",
    "Conv Ftrs：这一列用来表明一个方法是否使用从训练图像直接学习到的卷积特征或手动特征作为基本的表示。  \n",
    "\n",
    "CharGT-Free：这一列用来表明字符级标注对于训练模型是否是必要的。由于 CRNN 的输入和输出标签是序列，因此字符级标注是不必要的。  \n",
    "\n",
    "Unconstrained：这一列用来表明训练模型是否受限于一个特定的字典，是否不能处理字典之外的单词或随机序列。注意尽管最近通过标签嵌入[5, 14]和增强学习学习到的模型取得了非常有竞争力的性能，但它们受限于一个特定的字典。  \n",
    "\n",
    "Model Size：这一列报告了学习模型的存储空间。在 CRNN 中，所有的层有权重共享连接，不需要全连接层。因此，CRNN 的参数数量远小于 CNN 变体所得到的模型，与[22,21]相比，模型要小得多。我们的模型有 830 万个参数，只有 33MB RAM（每个参数使用 4 字节单精度浮点数），因此可以轻松地移植到移动设备上。  \n",
    "  \n",
    "表 3 详细列出了不同方法之间的差异，充分展示了 CRNN 与其它竞争方法的优势。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/7.jpg?raw=true)  \n",
    "\n",
    "另外，为了测试参数 $\\delta$ 的影响，我们在方程 2 中实验了 $\\delta$ 的不同值。在图 4 中，我们将识别精度绘制为 $\\delta$ 的函数。更大的 $\\delta$ 导致更多的候选目标，从而基于词典的转录更准确。另一方面，由于更长的 BK 树搜索时间，以及更大数量的候选序列用于测试，计算成本随着 $\\delta$ 的增大而增加。实际上，我们选择 $\\delta = 3$ 作为精度和速度之间的折衷。  \n",
    "\n",
    "### 3.4. Musical Score Recognition  \n",
    "\n",
    "乐谱通常由排列在五线谱的音符序列组成。识别图像中的乐谱被称为光学音乐识别（OMR）问题。以前的方法通常需要图像预处理（主要是二值化），五线谱检测和单个音符识别。我们将 OMR 作为序列识别问题，直接用 CRNN 从图像中预测音符的序列。为了简单起见，我们仅识别音调，忽略所有和弦，并假定所有乐谱具有相同的大调音阶（C 大调）。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/8.jpg?raw=true)  \n",
    "\n",
    "据我们所知，没有用于评估音调识别算法的公共数据集。为了准备 CRNN 所需的训练数据，我们从[2]中收集了 2650 张图像。每个图像中有一个包含 3 到20 个音符的乐谱片段。我们手动标记所有图像的真实标签序列（不是的音调序列）。收集到的图像通过旋转，缩放和用噪声损坏增强到了 265k 个训练样本，并用自然图像替换它们的背景。对于测试，我们创建了三个数据集：1）“纯净的”，其中包含从[2]收集的 260 张图像。实例如图 5.a 所示；2）“合成的”，使用“纯净的”创建的，使用了上述的增强策略。它包含 200 个样本，其中一些如图 5.b 所示；3）“现实世界”，其中包含用手机相机拍摄的音乐书籍中的 200 张图像。例子如图 5.c 所示。 \n",
    "\n",
    "由于我们的训练数据有限，因此我们使用简化的 CRNN 配置来减少模型容量。与表1 中指定的配置不同，我们移除了第 4 和第 6 卷积层，将 2 层双向 LSTM 替换为 2 层单向 LSTM。网络对图像对和对应的标签序列进行训练。使用两种方法来评估识别性能：1）片段准确度，即正确识别的乐谱片段的百分比；2）平均编辑距离，即预测音调序列与真实值之间的平均编辑距离。为了比较，我们评估了两种商用 OMR 引擎，即 Capella Scan 和 PhotoScore。    \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/9.jpg?raw=true)  \n",
    "\n",
    "表 4 总结了结果。CRNN 大大优于两个商业系统。Capella Scan 和 PhotoScore 系统在干净的数据集上表现相当不错，但是它们的性能在合成和现实世界数据方面显著下降。主要原因是它们依赖于强大的二值化来检五线谱和音符，但是由于光线不良，噪音破坏和杂乱的背景，二值化步骤经常会在合成数据和现实数据上失败。另一方面，CRNN 使用对噪声和扭曲具有鲁棒性的卷积特征。此外，CRNN 中的循环层可以利用乐谱中的上下文信息。每个音符不仅自身被识别，而且被附近的音符识别。因此，通过将一些音符与附近的音符进行比较可以识别它们，例如对比他们的垂直位置。  \n",
    "\n",
    "结果显示了 CRNN 的泛化性，因为它可以很容易地应用于其它的基于图像的序列识别问题，需要极少的领域知识。与 Capella Scan 和 PhotoScore 相比，我们的基于 CRNN 的系统仍然是初步的，并且缺少许多功能。但它为 OMR 提供了一个新的方案，并且在音高识别方面表现出有前途的能力。  \n",
    "\n",
    "## 4. Conclusion  \n",
    "\n",
    "在本文中，我们提出了一种新颖的神经网络架构，称为卷积循环神经网络（CRNN），其集成了卷积神经网络（CNN）和循环神经网络（RNN）的优点。CRNN 能够获取不同尺寸的输入图像，并产生不同长度的预测。它直接在粗粒度的标签（例如单词）上运行，在训练阶段不需要详细标注每一个单独的元素（例如字符）。此外，由于 CRNN 放弃了传统神经网络中使用的全连接层，因此得到了更加紧凑和高效的模型。所有这些属性使得 CRNN 成为一种基于图像序列识别的极好方法。  \n",
    "\n",
    "在场景文本识别基准数据集上的实验表明，与传统方法以及其它基于 CNN 和 RNN 的算法相比，CRNN 实现了优异或极具竞争力的性能。这证实了所提出的算法的优点。此外，CRNN 在光学音乐识别（OMR）的基准数据集上显著优于其它的竞争者，这验证了 CRNN 的泛化性。  \n",
    "\n",
    "实际上，CRNN 是一个通用框架，因此可以应用于其它的涉及图像序列预测的领域和问题（如汉字识别）。进一步加快 CRNN，使其在现实应用中更加实用，是未来值得探索的另一个方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Recurrent Nets with Attention Modeling for OCR in the Wild  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "我们提出了带有 attention modeling 递归循环神经网络，用于无约束自然场景图像光学字符识别。所提出的方法的主要优点是：（1）使用递归卷积神经网络（CNN），相同的参数容量下能够更加有效地提取图片特征；（2）隐式学习的字符级语言模型，体现在循环神经网络中，避免进行 N 元文法分析；（3）使用 soft-attention 机制，使模型能够有选择地利用图片特征，并且可以使用标准的反向传播来进行端到端训练。  \n",
    "\n",
    "我们在具有挑战性的基准数据集上实现了最佳的性能，验证了我们方法的有效性：Street View Text, IIIT5k, ICDAR 和 Synth90k。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "照片光学字符识别（照片 OCR）旨在读取自然图像中的场景文字，是各种计算机视觉任务的重要步骤，并在多个商业应用中取得了显著成功。其中包括自动导航系统的路牌识读，盲人辅助技术（如产品标签阅读），手机上的实时文本识别和翻译，以及在网络上搜索/索引庞大的图像和视频语料库。  \n",
    "\n",
    "照片 OCR 领域之前主要集中在具有手工设计图像特征的受限场景中。（这里，约束意味着在推理期间存在固定词典和已知长度的词）。具体而言，约束文本识别方法的例子包括基于区域的二值化或聚类，具有 HOG 特征的图形结构，具有 SIFT 描述符的整数编程，带有条件随机场（CRF）的 HOG 特征，具有二元和连通元件特征的马尔可夫模型。一些早期的工作尝试在手工制作的特征上学习局部中间表示，并且[48,19,16]中的一些方法结合了深度卷积神经网络（CNN）提供更好的图像特征提取。这些方法在测试阶段已知候选真实词字符串的情况下工作得很好，但是不能泛化到根本不存在于词典列表中的单词。  \n",
    "\n",
    "Jaderberg 等人提出了超越这种约束条件的最新技术进展。在[17]中。作者通过构建两组 CNN - 一个用于对字符序列进行建模，一个用于 N-gram 语言统计 - 然后用一个 CRF 图形模型来结合他们的激活，在无约束的环境中报告结果。这种方法取得了巨大的成功，为照片 OCR 领域树立了新的标准。然而，尽管取得了这些成功，但[17]中的系统确实有一些缺点。例如，使用两个不同的 CNN 会导致相对较大的内存和计算成本。此外，手动定义的 N-gram CNN 模型具有大量输出节点（N = 4 的 10k 输出单元），这增加了训练复杂度-需要增量训练过程和基于 N-gram 频率的启发式梯度重新缩放。  \n",
    "\n",
    "受到[17]的启发，我们继续将精力集中在无约束的场景文本识别任务上，并且我们开发了一个带有 attention modeling（R2AM）系统的递归循环神经网络，该系统直接执行图像序列（字符串）学习。本文提出的三项主要贡献是：  \n",
    "\n",
    "（1）具有相同参数容量的递归 CNN 具有权重共享，能够更加有效地提取图片特征。  \n",
    "\n",
    "（2）从上述递归 CNN 提取图像特征之上的递归神经网络（RNN），以执行字符级语言模型的隐式学习。RNN 可以从训练数据中自动学习字符串中自然存在的字符的顺序动态特性，而无需从字典中手动定义 N-gram。  \n",
    "\n",
    "（3）在字符序列被读取时执行““soft”确定性图像特征选择的序列 attention-based 模型机制，并且可以在标准反向传播内端对端地进行训练。  \n",
    "\n",
    "本文在 Street View Text， IIIT5K， ICDR 以及 Synth90k 这几个数据集上进行了实验，详细分析了所提出模型组件的性能，实验得出文中提出的网络结构获得了最优的结果，超出了之前 unconstrained 文本识别的最佳结果，例如：在 Street View Text 上提高了 9%，在 ICDAR 2013 上提高了 8.2%。  \n",
    "\n",
    "## 2. Methodology  \n",
    "\n",
    "在本文中，我们着重于场景文本识别任务，预测单个词的裁剪图像中的所有字符。我们在本文的其余部分将裁剪的字区域称为输入图像。当前部分描述了相关文献和提议的体系结构：Recursive Recurrent Nets with Attention Modeling。图 1 显示了我们的整体系统架构。  \n",
    "\n",
    "### 2.1. Character sequence model review  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/2/3.jpg?raw=true) \n",
    "\n",
    "许多文本识别方法在系统流程的第一步总是获得一个词汇的每一个字符，然后使用统计语言模型或者视觉结构预测来修正错误分类的字符。然而，由于每个字符与这个词汇中的其他字符在位置上密切相连，使得这里存在一个很大的挑战，因此经典的字符识别组件需要处理大量类间和类内混淆，这一点在中的图3 给出了很多的解释。即使在采用基于 CRF 或马尔可夫模型的高阶语言先验的复杂字词识别系统中，整个系统的性能还是由系统处理流程的第一步的能力决定：字符识别组件。  \n",
    "\n",
    "Goodfellow 等人第一次使用带有位置敏感字符分类器的 CNN 对街道编号进行分类。最近，Jaderberg 等人提出的字符序列模型中，使用深度 CNN 直接对单词中每个位置上的字符进行编码，从而预测图像区域中的字符序列。这种方法很大程度上克服了上述问题，方法是直接模拟场景角色中的自然间距和重叠模式，这些模式不能通过基于滑动窗口的字符识别方法轻易实现。有关此字符序列模型的详细信息，请参阅[17]。我们在本文的其余部分将此基准方法称为Base CNN（并在图 3 中标记为 Base CNN）。我们提出的系统建立在这个基本的 CNN 模型之上；我们描述了 2.2 节中的新颖图像编码的扩展，2.3 节中的字符级语言建模以及 2.4 节中的基于注意力的机制。  \n",
    "\n",
    "### 2.2. Recursive CNNs for image feature extraction  \n",
    "\n",
    "#### 2.2.1 Recursive convolutional layers  \n",
    "\n",
    "前述字符序列模型取得巨大成功的关键之一是通过采用对整个输入图像进行操作的多个卷积层来捕获字符预测期间的上下文相关性的能力  \n",
    "\n",
    "改进此基本 CNN 模型以便为字符预测启用更大范围上下文依赖性的一种可能方法是考虑对每个卷积层或更深的网络使用更大的内核大小，从而增加相应的接受字段大小。然而，这种方法会导致更多的参数和更高的模型复杂性，从而导致潜在的训练和泛化问题。  \n",
    "\n",
    "在控制模型容量的同时扩展更长的数据依赖性的另一种方法是如[35,7,29]中所建议的那样使 Base CNN 网络递归或复发。通过使用递归或递归卷积层，网络体系结构可以是任意深的，而不必在每一层重复使用相同的卷积加权矩阵多次来显着增加参数总数。  \n",
    "\n",
    "我们现在描述在我们的方法中使用的递归 CNN：在时间步 t（其中t≥0）馈送到递归卷积层的实例是输入图像/特征响应，如下：  \n",
    "\n",
    "$h_{i,j,k}(t) =\n",
    "        \\begin{cases}\n",
    "        \\sigma ((w_k^{hh})^T x_{i,j} + b_k),  & \\text{at t = 0} \\\\\n",
    "        \\sigma ((w_k^{hh})^T h_{i,j}(t-1) + b_k), & \\text{at t > 0}\n",
    "        \\end{cases} \\tag{1}$\n",
    "\n",
    "其中 $h_{i,j}(t-1)$ 和 $x_{i,j}$ 分别表示以特征地图 (i,j) 为中心的向量化前馈和输入 patchaes。$w_k^{hh}$ 是输出信道 k 的矢量化前馈加权。 $b_k$ 是输出通道 k 的偏差。$\\sigma$ 是确定性的非线性转换函数。  \n",
    "\n",
    "recursive CNNs 在相同参数容量的情况下增加了传统 CNNs 的深度，同时也比 CNNs 产生更加紧凑的特征响应。recursive 相互作用也可以视为 feature map 中的一种“横向连接性”，使得给定层的表示更好捕捉到高层依赖。更多对递归 CNN 层的讨论可以在[29]中看到。  \n",
    "\n",
    "#### 2.2.2 Untying in recursive convolutional layers  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/2/1.jpg?raw=true) \n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/2/2.jpg?raw=true) \n",
    "\n",
    "我们已经在前面的章节中看到了递归卷积图层的定义和潜在收益。但是，公式 1 限制所有权重以共享相同的内部值-它们被“捆绑”在一起。该捆绑的一个结果是，由于共享权重总是将输入特征图投影到输出的相同尺寸（宽×高×通道数），所以在所有层中通道的数量将是相同的 。这与 CNN 中通常的做法形成了鲜明的对比，即改变控制所执行计算量的通道数量以及不同特征类型的频谱。（例如流行和成功的 VGGNet，其中随着卷积层的空间范围根据{224,112,56,28}减小，信道数量增加如{64,128,256,512}。）  \n",
    "\n",
    "在这项工作中，我们建议使用递归卷积层的“untied”变体，区别在于层间（inter-layer）前馈权值 $w_{untied,k}^hh$ 和后面的层内（intra-layer）recursive权值 $w_{tied,k}^hh$。 这允许网络在不同层上具有不同的信道数量，并且还允许递归权重更加自由地专门化。  \n",
    "\n",
    "通过在时间 t=0 的时候 untying前馈权值，式（1）变为：  \n",
    "\n",
    "$h_{i,j,k}(t) =\n",
    "        \\begin{cases}\n",
    "        \\sigma ((w_{untied, k}^{hh})^T x_{i,j} + b_k),  & \\text{at t = 0} \\\\\n",
    "        \\sigma ((w_{tied, k}^{hh})^T h_{i,j}(t-1) + b_k), & \\text{at t > 0}\n",
    "        \\end{cases} \\tag{2}$\n",
    "\n",
    "通过这种方法，任意 recursive 卷积层的 channels 数目可以由 untied 权值来进行调整，控制整体的计算代价。可以使用相同的逻辑来 untie recurrent 卷积层，如图 2 所示。  \n",
    "\n",
    "在实验部分，我们观察到 Base CNN 模型的递归和循环版本都显着提高了许多最新标准基准数据集（如 Synth90k，SVT 和 ICDAR13）的性能。请参阅第 3.3 节中的详细信息。我们进一步发现，递归版本在我们探索的所有任务中始终优于循环版本，这与最近其他文献的发现一致，即递归结构可以有效地学习组合特征和部分交互，从而，为了获得高性能，在每个时间步不需要多次输入 $x_{i,j}$。出于这个原因，我们为整个系统流程选择 Base CNN 模型的递归版本，如图 1 的底部所示。  \n",
    "\n",
    "### 2.3. RNNs for character-level language modeling  \n",
    "\n",
    "2.2 节中提出的无约束递归字符序列模型已经可以作为一个端到端的可训练照片 OCR 系统，它明显优于[17]中的复杂结构化学习方法（分别提高，SVT 为7.2%，ICDAR13 为6.7%）。 尽管如此，我们观察到 Base CNN 模型（plain CNN，递归 CNN 或 recurrent CNN）通过使用多个损失函数独立地训练每个字符位置。然后，有动机询问我们是否可以允许每个字符位置之间进行某种交互，并利用底层的字符级语言统计信息。  \n",
    "\n",
    "最常见的方法在每个字符位置的输出预测之上添加某种图模型（例如CRF）。然而，这些方法需要为所有候选字符计算一元和更高阶项，并且在推理阶段可能需要昂贵的计算。访问这种字符级语言信息的另一种方式是直接使用 CNN 对所有可能性进行建模-如[17]中的 N-grams 组件。这种 CNN 模型需要来自字典的预定义 Ngram，并且使用大量的输出节点，其中每个节点表示 N-gram 组合中的元素（例如[17]中的 N = 4 的 10k 个输出节点）。为了在该 CNN 模型中使用反向传播联合训练整个 N-gram 范围，[17]中的方法通过其在训练词语料库中出现的频率的逆频率来重新缩放每个 N-gram 类的梯度，因为一些训练数据集中几乎不会出现 N-gram 类，即使最近发布的 Synth90k 数据集有 700 多万个训练样本。  \n",
    "\n",
    "与上述方法相反，我们建议使用递归神经网络（RNNs）来对文本的字符级统计进行建模。递归神经网络及其变种长短时记忆网络（LSTM）最近经历了复兴，并且在处理顺序数据（例如手写识别，机器翻译，语音识别和图像字幕）时是非常有效的模型。识别图像中的字符可以基本上被认为是解决从像素强度到自然字符级矢量的顺序动态和学习映射的任务。具体来说，我们的模型只需要一个图像并生成 1-of-K 的字符编码序列。  \n",
    "\n",
    "$y = \\{y_1,y_2,...,y_N\\}, y_t \\in R^K \\tag{3}$\n",
    "\n",
    "其中 K 是可能字符的大小，N 是单词的长度。  \n",
    "\n",
    "我们使用 RNN 通过在图像特征 $I$ 上每个时间步产生一个单词来产生一个 word 串，先前的隐层状态 $h_{t-1}$ 和输入 $x_t$ 使用下面的递归公式：  \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "h_t = \\sigma (W_{xh} x_t + W_{hh} h_{t-1} + b_h) \\\\\n",
    "y_t = \\sigma(W_{hy} h_t + b_y) \\tag{4}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "其中 $\\sigma$ 是元素级非线性转换函数，$h_t \\in R^M$ 为带有 M 个单元的隐层状态，输入 $x_t$ 可以为图片特征 $I$ 或先前生成的字符 $y_{t-1}$，具体取决于所使用的 RNN 结构。编码图像特征 $I$ 是从 CNN 模型的最后全连接层提取的。这个 CNN 模型既可以是普通的 CNN，也可以是 recurrent CNN，也可以是递归 CNN。我们将展示不同 CNN 模型在实验部分中的表现。  \n",
    "\n",
    "将图像特征 $I$ 提供给 RNN 可能有很多种方法，并且 RNN 本身也可以具有许多不同的结构。在本文中，我们通过实证探索了一系列设置。图 3 演示了我们探索的 base CNN 模型和五个 RNN 变体。我们在本节详细介绍了四个变体，并将在下一节中解释最后一个变体（包括注意力建模）：  \n",
    "\n",
    "**Base CNN:** 基线字符序列 CNN，使用多种损失函数进行训练，每个损失函数针对于字符的位置（如 2.1 所述）。  \n",
    "\n",
    "**Base CNN + RNN1c:** 一种来自图像字幕作品的单层 RNN。仅在第一时间步骤将提取的图像特征 $I$ 发送到 RNN。在时刻 t-1，RNN 的预测字符 $y_{t-1}$ 在时刻 t 被馈送到 RNN，直到我们获得词尾（EOW）标签。这个变体作为一个良好的完整性检查，帮助我们验证了我们的 RNN 在初始 CNN 表示的情况下执行字符级语言建模的能力。  \n",
    "\n",
    "**Base CNN + RNN1u:** 一个非因式分解的单层 RNN，图像特征输入到每个时间步。因此，字符预测始终以图像特征和前面的隐层状态为条件。  \n",
    "\n",
    "**Base CNN + RNN2u:** 一个非因式分解的双层 RNN，使用了两个 RNN 进行堆叠。这个模型在每个时间步上具有更深的结构。这个结构在每个时间步上都可以获得字符特征。  \n",
    "\n",
    "**Base CNN + RNN2f:** 一个因式分解的双层 RNN，使用了两个 RNN 进行堆叠。这个变体只有第二层 RNN 才能获得图像特征。通过这种方法，使第一层RNN 专注于字符级语言建模，使第二层 RNN 专注于语言统计与图像特征的结合。  \n",
    "\n",
    "如前所述，我们探索的所有 RNN 变体隐式执行字符级语言建模，并受益于不受限于预定义的 Ngram 序列。例如，Karpathy 等人证明了基于 RNN 的方法在字符级文本预测中始终优于 Ngram 模型，其中 N 大到20。在实验部分中，我们将显示具有和不具有所提出的 RNN 的误差分析。  \n",
    "\n",
    "### 2.4. Attention modeling  \n",
    "\n",
    "Attention-based 机制使得模型专注于输入特征的最重要部分，并且可能添加一定的可解释性。一般有两类 attention-based 图像理解：hard-attention和 soft-attention。Hard-attention 模型学习选择一系列离散的 glimpse location，但是很难训练，因为损失梯度（loss gradients）很难处理。本文使用的是一个 soft-attention 模型，可以使用标准的反向传播来进行训练。  \n",
    "\n",
    "我们现在将图 3 中所示的 attention modeling function 描述为 Base CNN + RNNAtten。在每个输出步骤 t，注意函数（在图中表示为字母 A）以图像特征 $I$ 以及第一层 RNN 的输出 $s_t$ 为条件计算一个能量向量（energy vector）$\\tau_t$：  \n",
    "\n",
    "$\\tau_t = f_{attention}(I,s_t) = tanh(\\phi(I) + \\psi(s_t)) \\tag{5}$\n",
    "\n",
    "其中 $\\phi$ 和 $\\psi$ 可以是多层感知器或简单的权重矩阵，它们将 $I$ 和 $s_t$ 投影到相同的空间。然后，在时间步 t，基于能量系数 $\\alpha_t$ 将上下文向量 $c_t$ 计算为加权图像特征：  \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\alpha_{td} = \\frac {\\exp(\\tau_{td})}{\\sum_{d=1}{D}\\exp(\\tau_{td})} \\\\\n",
    "c_t = \\alpha_t o I \\tag{6}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "其中 o 是 Hadamard 积。该机制生成一组正向权重 $\\alpha_{td}$，其可被理解为在对图像特征 $I$ 进行融合时赋予位置 d 的相对重要性，并且然后将所计算的上下文向量 $c_t$ 发送到 RNN 的第二层以用于最终输出预测。  \n",
    "\n",
    "## 3. Experiments  \n",
    "\n",
    "### 3.1. Datasets  \n",
    "\n",
    "我们用五个标准基准数据集：ICDAR 2003，ICDAR 2013，Street View Text，IIIT5k 和 Synth90k 评估提出的 Recursive Recurrent Nets with Attention Modeling 框架。  \n",
    "\n",
    "**ICDAR 2003**包含 251 个全景图像和 860 个裁剪图像。尽管本文的重点是无约束的文本识别，但为了易于比较，我们仍然提供了受限的文本识别结果。Wang 等人定义的每图 50 个单词词典被称为 IC03-50，所有测试词（563字）的词典被称为 IC03-Full。  \n",
    "\n",
    "**ICDAR 2013**包含 1015 幅来自自然场景图像的裁剪字图像，被称为 IC13。  \n",
    "\n",
    "**Street View Text**包含来自 Google 街景视图的 647 个裁剪后的单词图像。Wang 等人定义的每图 50 个单词词典被称为 SVT-50，所有测试词的词典（4282个词）被称为 SVT-Full。  \n",
    "\n",
    "**IIIT5k**包含从谷歌图像搜索引擎下载的 3000 张裁剪字图像。每个图像都有一个 50 字的词典（IIIT5k-50）和一个 1k 字的词典（IIIT5k-1k）。  \n",
    "\n",
    "**Synth90k**包含合成生成的单词图像。该数据集包含约 700 万训练图像，900k 验证图像和 900k 测试图像。  \n",
    "\n",
    "我们遵循 Jaderberg 等人的设置来准备训练和测试集，我们的方法纯粹在 Synth90k 训练集上进行训练，所有参数都通过验证集进行选择。我们不使用验证数据来重新训练我们的模型。我们也遵循 Wang 等人的评估协议。对仅包含字母数字字符（0-9和A-Z）和至少三个字符的图像进行识别。  \n",
    "\n",
    "### 3.2. Implementation details  \n",
    "\n",
    "表 A1 列出了我们的 Base CNN 模型的网络架构。它具有 8,64,64,128,128,256,256,512 和 512 个通道的卷积层，每个卷积层使用具有 3×3 空间范围的核。卷积步幅为 1，零填充和 ReLU 激活函数执行的。第二，第四和第六卷积层之后的 2×2 最大池化。两个全连接的层有 4096 个单元。输入是一个调整大小的 32×100 灰度图像。  \n",
    "\n",
    "我们现在提供表 A1 中提出的无约束递归 CNN 的网络结构的细节。请注意，每个偶数卷积层（conv2，conv4，conv6 或 conv8）都使用其自己的共享权重矩阵，该矩阵具有完全相同的输入和输出维度，因此可以在一个递归卷积层内多次将特征映射到同一空间 与 Base CNN 模型具有相同的参数容量。  \n",
    "\n",
    "对于字符级语言模型，我们使用配备了双曲线正切激活函数的具有 1024 个隐藏单元的 RNN。 我们的整个系统流程如图 1 所示。  \n",
    "\n",
    "我们使用反向传播时间（BPTT）算法来训练模型，256 batch size SGD 和 0.5 的 dropout rate。初始学习率为 0.002。所有变体都使用相同的方案，并根据验证集合确定 30 个总周期。我们应用 10 级的梯度裁剪，并在适当的位置找到 weight decay 不会增加额外的性能收益。所有初始权重从 0.01 的标准偏差的高斯分布中采样。我们在开源的深度学习框架 Caffe 中实现了这个系统。对于整个系统框架，单个 Nvidia Titan X GPU 上每张图像的平均推理时间为 2.2 毫秒。  \n",
    "\n",
    "### 3.3. Ablation study  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/2/4.jpg?raw=true) \n",
    "\n",
    "在本节中，我们通过实证研究了所提出方法中三个关键组件的贡献，即：用于图像编码的递归 CNN，用于字符级语言建模的 RNN 和用于更好图像特征使用的 attention-based 机制。  \n",
    "\n",
    "为了将由于架构变化而导致的性能改善与可能仅来自具有更多参数的性能改进分离，我们首先逐渐增加[5]中的基线 CHAR 模型的深度，从 5 个 conv 层开始，直到达到性能瓶颈，在表 A1 中显示为 Base CNN 的 8 个 conv 层。观察这个极值点后，我们没有探索更深的网络，例如（16 或 19 个 conv 层）VGGNet。表 A1 中的条形图显示了 Synth90k 数据集上不同深度网络的相应性能。  \n",
    "\n",
    "#### 3.3.1 Recursive and recurrent convolutional layers  \n",
    "\n",
    "表 1 显示了提出的无约束的文本识别任务中的基于 CNN 模型的无约束递归和循环 CNN 的有效性。我们观察到两种方法中的迭代次数越多，所有数据集的评估精度越高，并且改进基本上来自于相同的参数容量，因为卷积层的权重是共享的，如图 2 和表 A1 所示。这些共享权重之间的横向相互作用允许更广泛的接受领域和同一“层”中的表征单元之间的竞争。另外，我们发现递归版本的性能一直优于循环版本。这可能是因为递归卷积层可以防止误差信号通过 recurrent 连接在回归卷积层中直接反向传回，这对于卷积运算尤其如此，因为输入信号保持不变。因此，我们为最终的系统架构选择递归 CNN。  \n",
    "\n",
    "#### 3.3.2 Character-level language modeling  \n",
    "\n",
    "在表 2 中，我们报告了图 3 中每个 RNN 架构变体的无约束文本识别结果。我们观察到在已经达到其性能极大值的 Base CNN 网络上，使用所提议的的任何种类的 RNN 变体立即提高了性能。RNN1c 作为一个良好的健全性检查模块，因为来自 Base CNN 的图像特征仅在第一时间步骤被馈送到 RNN，然后 RNN1c 能够基于先前预测的字符正确地预测第一个和后续字符和隐藏状态信息。  \n",
    "\n",
    "RNN1u 和 RNN1c 结果的比较表明，在每个时间步从 Base CNN 向 RNN 馈送图像特征可以进一步提高性能，因为在推断过程中 RNN1u 不仅访问先前预测的字符和隐藏状态信息，而且还访问原始图像特征。（这种架构大概也允许 RNN 在序列建模上消耗更多容量，因为它不再需要将图像特征信息保留在其隐藏状态中。）此外，基于 RNN2f 优于 RNN2u 的观察，我们注意到因式分解似乎成为比无结构模型更有效的架构。这可能是因为编码图像特征只能被第二层 RNN 访问，并且这允许/强制第一层 RNN 专注于建模字符级别统计。  \n",
    "\n",
    "所提出的 RNNAtten 使用基于注意力的机制，该机制学习一组权重矩阵来重新调整图像特征并在将图像特征馈送到顶层 RNN 之前执行软注意力建模。观察到这种体系结构在探索的所有五种 RNN 变体中都能获得最佳性能。  \n",
    "\n",
    "因此，我们的最终网络体系结构包含上述用于图像特征提取的递归 CNN 和针对字符级语言建模的 RNNAtten，如图 1 所示。  \n",
    "\n",
    "在这一点上，值得一提的是，我们已经探索了用于字符级语言建模的后向 RNN 和双向 RNN，但是这些扩展都没有提供进一步的改进。这与格雷夫斯等人的观察形成对比。也许这是因为我们的工作重点是预测场景图像中的字符，平均包含大约 8 个字符，而[11]中的方法则着重于较长的手写脚本序列。出于这个原因，我们没有探索 LSTM 存储单元，它经常用来帮助提高 RNN 在较长时间尺度上保留信息的能力（通常以模型复杂度和计算运行时间为代价）。  \n",
    "\n",
    "#### 3.3.3 Constrained and unconstrained text recognition  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/2/5.jpg?raw=true) \n",
    "\n",
    "在没有词典或词典的情况下识别野外文本是一项具有挑战性的任务。Bissacco 首先报道了 lexicon-free（但强烈的词典加权）结果，而 Jaderberg 等人最近提出了在词汇/字典无约束的场景文本识别中的显着进步。表 3 比较了我们提出的方法与完全无约束文本识别任务中的先前最佳结果的准确性。可以看出，我们的方法比[17]中的联合模型在 SVT 和 ICDAR 2013分别提高了的 9% 和 8.2%。  \n",
    "\n",
    "尽管我们提出的方法是针对无约束的情景，但我们也将结果与受约束的设置进行比较，在该设置中，预测字符序列与预定义词典中的词之间的编辑距离最小时选择输出。表 4 显示了这些比较。我们的方法在几个基准测试中获得了无限制识别的最佳结果，特别是最近发布的 ICDAR 2013（IC13）数据集。 我们还报告了在以前的文献中没有记录的 IIIT5k 上的无约束文本识别结果。我们也在竞争激烈的环境中取得最好的成绩。（请注意，[18]的 DICT 模型是在包含测试集的真实词的特定字典上训练的；它不能处理先前未见过的字符串。）  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/2/6.jpg?raw=true) \n",
    "\n",
    "图 4 演示了提供的 R2AM 框架在 SVT, ICDAR, and IIIT5k 数据集上的无词典场景文本识别结果。R2AM 方法能够识别低对比度，大的变换和杂波背景的文字。它也能够通过隐式学习的语言模型恢复丢失/隐藏的字符。我们通过在非文本图像上执行文本预测来展示我们的语言模型，如图 A1 所示，看到我们的方法可以利用底层的字符级统计信息并生成类似字的字符串，即使图像没有字母数字字符。  \n",
    "\n",
    "## 4. Conclusion and future directions  \n",
    "\n",
    "我们已经提出了一个新的无词典照片 OCR 框架，该框架结合了用于图像编码的递归 CNN，用于语言建模的 RNN 以及用于更好的图像特征使用的基于注意力的机制。大量分析显示了每个提议组件的有效性，以相同的方法对约束和未受约束的场景进行了概括性，以及该方法利用最新结果识别真实场景文本的实际能力。  \n",
    "\n",
    "未来，我们将探索递归全卷积网络，以更好地连接提取的图像特征和输入图像上的相应位置，并在输入域上可视化注意力系数。此外，我们将通过门控单元允许传入信号改变基于注意的机制的状态并注入深度监督。  \n",
    "\n",
    "未来的进一步发展方向是将这项工作扩展到除阅读外的文本检测，从而形成一个端到端的全图像到文本阅读流水线。\n",
    "\n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/2/7.jpg?raw=true) \n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/2/8.jpg?raw=true) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考  \n",
    "\n",
    "- 1 [An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition](https://arxiv.org/abs/1507.05717) \n",
    "- 2 [CRNN论文翻译——中英文对照](https://blog.csdn.net/Quincuntial/article/details/77679419)  \n",
    "- 3 [Recursive Recurrent Nets with Attention Modeling for OCR in the Wild](https://arxiv.org/abs/1603.03101) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
