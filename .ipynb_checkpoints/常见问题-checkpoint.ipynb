{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 SVM  \n",
    "\n",
    "最小化经验风险和结构风险的算法，分类超平面有很多，但最优超平面是距离两侧最近样本点(支持向量)之间的间隔最大化。  \n",
    "\n",
    "最优超平面：$W_0^TX + b_0 = 0$\n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/issues/SVM.jpg?raw=true)\n",
    "<center> **最优超平面** </center >\n",
    "\n",
    "点到直线的距离：$r = \\frac{W_0^TX + b_0}{||W_0||}$，从而有判别函数$g(x) = r*||W|| = W_0^TX + b_0$,将判别函数归一化，令  \n",
    "\n",
    "$\n",
    "\\begin{cases}\n",
    "W_0^TX^p + b_0,  & \\text{if $y^p = +1$} \\\\\n",
    "W_0^TX_p + b_0, & \\text{if $y^p = -1$}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "距离超平面最近的样本$X^s$ 满足|g(X^s)| = 1，这些向量称为支持向量。  \n",
    "\n",
    "上式结合起来就是：$y^p(W_0^TX^p + b_0) >= 1$\n",
    "\n",
    "又支持向量到超平面之间的距离就是：$r = \\frac{g(X^s)}{||W_0||} = \\begin{cases}\\frac{1}{||W_0||},  & \\text{if $y^p = +1$} \\\\ \\frac{-1}{||W_0||}, & \\text{if $y^p = -1$}\\end{cases}$  \n",
    "\n",
    "两类支持向量之间的距离就是：$2r = \\frac{2}{||W_0||}$，类间距离最大等价于使权值向量的范数$||W||$最小化，因此满足$y^p(W_0^TX^p + b_0) >= 1$且使$||W||$最小化的平面就是最优分类超平面。  \n",
    "\n",
    "**立最优超平面的问题可转换为约束优化问题，即在$y^p(W_0^TX^p + b_0) >= 1$约束条件下，最小化代价函数$\\frac{1}{2}W^TW$。**  \n",
    "\n",
    "使用拉格朗日系数法解约束最优化问题：$L(W,b,\\alpha) = \\frac{1}{2}W^TW - \\sum_{p=1}^P {\\alpha_p[y^p(W_0^TX^p + b_0)-1]}$  \n",
    "\n",
    "最小化拉格朗日函数，就应该最小化第一项，最大化第二项  \n",
    "\n",
    "若将上述公式应用于非线性可分的数据时，会有一些样本不能满足$y^p(W_0^TX^p + b_0) >= 1$的约束条件，从而出现分类误差，所以适当放宽约束条件，$y^p(W_0^TX^p + b_0) >= 1-\\xi_p$，引入松弛变量$\\xi_p >= 0$，若$0 < \\xi_p < 1$,数据点落入分类正确一侧，若$\\xi_p > 1$，数据点分类错误。  \n",
    "\n",
    "当分类变得不线性，线性分类向量机就会失效，就需要新的方法去解决，那就是非线性向量机，而在非线性向量机中，一种非常重要的方法就必须要知道，那就是核函数。  \n",
    "\n",
    "核函数：如果存在一个从$X \\to H$ 的映射 $\\phi(x)$，使得对所有的 $x,z \\in X$，函数 $K(x,z) = \\phi(x) \\dot \\phi(z)$，则称$K(x,z)$为核函数，$\\phi(x)$为映射函数。  \n",
    "\n",
    "核函数一定，映射函数是不唯一的，而且当维度是无限大的时候，几乎无法求得映射函数，那么核函数的作用就在于此，核函数避免了映射函数的求解。  \n",
    "\n",
    "常见核函数： \n",
    "\n",
    "1、线性核函数：$k(x,y) = x^Ty + c$\n",
    "\n",
    "2、多项式核函数: $k(x,y) = {(ax^Ty + c)}^p$  \n",
    "\n",
    "3、径向基核函数: $k(x,y) = exp(-\\gamma{||x-y||}^2)$，也叫高斯核函数，可看做是$k(x,y) = exp(\\frac{{||x-y||}^2}{2\\delta^2})$的另一种形式。  \n",
    "\n",
    "二分类的 Hinge Loss：$max(0,1−y_i(w⋅x_i+b))$\n",
    "\n",
    "所以 SVM 可以通过直接最小化 Hinge Loss 和 $l_2$正则化参数求得最优分类平面：$\\sum_{i=1}^N max(0, 1−y_i(w⋅x_i+b)) + \\lambda{||w||}^2$  \n",
    "\n",
    "多分类的 Hinge Loss: $\\sum_{j \\neq y_i} max(0,s_j−s_{y_i}+1)$，用 $s_j$ 表示得分向量 $s$ 中的第 $j$ 个分量 ， $s_{y_i}$ 表示对应 $y_i=1$ 的分量。  \n",
    "\n",
    "## 2 欧式距离与曼哈顿距离\n",
    "\n",
    "空间两点：$x = (x_1,...,x_n)$ 和 $y = (y_1,...,y_n)$\n",
    "\n",
    "欧式距离：$d(x,y) = \\sqrt{\\sum_{i=1}^n {(x_i - y_i)}^2}$，相当于两点之间的直线距离，即三角形斜边距离。  \n",
    "\n",
    "曼哈顿距离：$d(x,y) = \\sum_{i=1}^n|x_i-y_i|$，相当于两点之间的折线距离，即三角形直角边距离的和。  \n",
    "\n",
    "## 3 LR和SVM的联系与区别\n",
    "\n",
    "1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）\n",
    "\n",
    "2、两个方法都可以增加不同的正则化项，如l1、l2等。所以在很多实验中，两种算法的结果是很接近的。 \n",
    "\n",
    "区别： \n",
    "\n",
    "1、LR是参数模型，SVM是非参数模型。 \n",
    "\n",
    "2、从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM 采用的是 hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。 \n",
    "\n",
    "3、SVM的处理方法是只考虑 support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。 \n",
    "\n",
    "4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。 \n",
    "\n",
    "5、logic 能做的 svm 能做，但可能在准确率上有问题，svm能做的logic有的做不了。\n",
    "\n",
    "## 4 L1和L2的区别  \n",
    "\n",
    "L2得到平滑的权值， L1得到稀疏的权值，在实践中，如果不是特别关注某些明确的特征选择，一般说来 L2 正则化都会比 L1 正则化效果好。  \n",
    "\n",
    "当几个特征的共线性很高，并且对分类都很重要，L2 倾向于将权值平分给这些特征，从而使得这些有用的特征都得以留下；  \n",
    "\n",
    "L1 则会随机选择其中一个特征，而扔掉其它的特征，所以说L2平滑，L1稀疏。\n",
    "\n",
    "首先来看 L2 和 L1 的梯度的更新方式：\n",
    "\n",
    "$L2 = \\frac {1}{2}(w_1^2 + w_2^2 + ... + w_n^2), \\ \\ \\ \\ \\frac {\\partial L_2}{w_i} = w_i$  \n",
    "\n",
    "$L1 = |w_1| + |w_2| + ... + |w_n| , \\ \\ \\ \\ \\frac {\\partial L_1}{w_i} = sign(w_i) = 1 \\ or \\  -1$  \n",
    "\n",
    "L2 的权值更新公式为 $w_i = w_i - \\eta * w_i$，可见权值不断变小，但是因为每次都减少上一次的 $\\eta$ 倍，在小数值时候，权值下降的较慢，很难下降到 0，会收敛到较小但不为 0 的值，所以训练好的权值比较平滑。\n",
    "\n",
    "L1 的权值更新公式为 $w_i = w_i - \\eta * sign(w_i)$，也就是说权值每次更新都固定减少一个特定的值 $\\eta$，在小数值时，权值下降较快，经过若干次迭代之后，权值有可能减少到 0。  \n",
    "\n",
    "## 5 RNN、LSTM、GRU比较\n",
    "\n",
    "LSTM 有forget gate，input gate，cell state， hidden information等的变化；因为LSTM有进有出且当前的 cell informaton 是通过 input gate 控制之后叠加的，RNN 是叠乘，因此 LSTM 可以防止梯度消失或者爆炸  \n",
    "\n",
    "RNN引入了循环的概念，但是在实际过程中却出现了初始信息随时间消失的问题，即长期依赖（Long-Term Dependencies）问题，所以引入了LSTM。  \n",
    "\n",
    "LSTM：因为LSTM有进有出且当前的cell informaton是通过input gate控制之后叠加的，RNN是叠乘，因此LSTM可以防止梯度消失或者爆炸。  \n",
    "\n",
    "GRU是LSTM的变体，将忘记门和输入们合成了一个单一的更新门。  \n",
    "\n",
    "## 6 贝叶斯公式  \n",
    "\n",
    "朴素贝叶斯中的朴素一词的来源就是假设各特征之间相互独立。这一假设使得朴素贝叶斯算法变得简单，但有时会牺牲一定的分类准确率。  \n",
    "\n",
    "$P(B|A) = \\frac{P(A|B)P(B)}{P(A)}$\n",
    "\n",
    "换成分类任务表达式：$P(类别|特征) = \\frac{P(特征|类别)P(类别)}{P(特征)}$\n",
    "\n",
    "优点：简单\n",
    "缺点：假设属性之间相互独立，这种假设在实际过程中往往是不成立的，在属性之间相关性越大，分类误差也就越大。  \n",
    "\n",
    "## 7 主题模型  \n",
    "\n",
    "LDA由Blei, David M.、Ng, Andrew Y.、Jordan于2003年提出，是一种主题模型，它可以将文档集中每篇文档的主题以概率分布的形式给出，从而通过分析一些文档抽取出它们的主题（分布）出来后，便可以根据主题（分布）进行主题聚类或文本分类。同时，它是一种典型的词袋模型，即一篇文档是由一组词构成，词与词之间没有先后顺序的关系。\n",
    "\n",
    "此外，一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。\n",
    "\n",
    "人类生成文档的过程：比如假设事先给定了这几个主题：Arts、Budgets、Children、Education，然后通过学习训练，获取每个主题 Topic 对应的词语，然后以一定的概率选取上述某个主题，再以一定的概率选取那个主题下的某个单词，不断的重复这两步，最终生成文章。\n",
    "\n",
    "\n",
    "LDA就是要根据给定的一篇文档，反推其主题分布。\n",
    "\n",
    "通俗来说，可以假定认为人类是根据上述文档生成过程写成了各种各样的文章，现在想让计算机利用LDA干一件事：计算机给我推测分析网络上各篇文章分别都写了些啥主题，且各篇文章中各个主题出现的概率大小（主题分布）是啥。\n",
    "\n",
    "在LDA模型中，一篇文档生成的方式如下：\n",
    "\n",
    "- 从狄利克雷分布中取样生成文档 i 的主题分布\n",
    "- 从主题的多项式分布中取样生成文档i第 j 个词的主题\n",
    "- 从狄利克雷分布中取样生成主题对应的词语分布\n",
    "- 从词语的多项式分布中采样最终生成词语\n",
    "\n",
    "其中，类似Beta分布是二项式分布的共轭先验概率分布，而狄利克雷分布（Dirichlet分布）是多项式分布的共轭先验概率分布。\n",
    "\n",
    "LDA只是pLSA的贝叶斯版本，文档生成后，两者都要根据文档去推断其主题分布和词语分布，只是用的参数推断方法不同，在pLSA中用极大似然估计的思想去推断两未知的固定参数，而LDA则把这两参数弄成随机变量，且加入dirichlet先验。  \n",
    "\n",
    "## 8 KNN中的K如何选取的  \n",
    "\n",
    "如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，K值的减小就意味着整体模型变得复杂，容易发生过拟合；\n",
    "\n",
    "如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，且K值的增大就意味着整体的模型变得简单。\n",
    "\n",
    "在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。  \n",
    "\n",
    "## 9 防止过拟合的方法  \n",
    "\n",
    "1、早停止：如在训练中多次迭代后发现模型性能没有显著提高就停止训练  \n",
    "2、数据集扩增：原有数据增加、原有数据加随机噪声、重采样  \n",
    "3、正则化  \n",
    "4、交叉验证  \n",
    "5、特征选择/特征降维  \n",
    "\n",
    "## 10 为何要对数据做归一化  \n",
    "\n",
    "1）归一化后加快了梯度下降求最优解的速度  \n",
    "\n",
    "机器学习模型使用梯度下降法求最优解时，归一化能加快梯度寻优的速度。\n",
    "\n",
    "2）归一化有可能提高精度  \n",
    "\n",
    "一些分类器需要计算样本之间的距离（如欧氏距离），如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）。  \n",
    "\n",
    "要强调：能不归一化最好不归一化，之所以进行数据归一化是因为各维度的量纲不相同。而且需要看情况进行归一化。\n",
    "\n",
    "## 11 哪些机器学习算法不需要做归一化处理  \n",
    "\n",
    "概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。  \n",
    "\n",
    "树模型不需要归一化原因：数值缩放，不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。另外，注意树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点事通过寻找最优分裂点完成的。  \n",
    "\n",
    "## 12 HMM 和 CRF\n",
    "\n",
    "CRF，HMM 都常用来做序列标注任务  \n",
    "\n",
    "HMM，CRF 根据观测状态预测隐藏状态，所以都需要计算发射概率和转移概率，HMM的两个概率是通过统计得到的，CRF的两个概率是根据学习得到的，都使用维特比算法进行解码。\n",
    "\n",
    "- HMM是生成式模型：\n",
    "\n",
    "1、对P(Y,X)建模  \n",
    "2、要对每个 label 都需要建模，最终选择最优概率的 label 为结果，所以没有什么判别边界  \n",
    "3、生成式模型的优点在于，所包含的信息非常齐全，所以不仅可以用来输出 label，还可以干其他的事情  \n",
    "4、生成式模型关注结果是如何产生的。但是生成式模型需要非常充足的数据量以保证采样到了数据本来的面目，所以速度相比之下较慢。\n",
    "\n",
    "\n",
    "- CRF是判别式模型：\n",
    "\n",
    "1、对 P(Y|X) 建模  \n",
    "2、对所有的样本只构建一个模型，确认总体判别边界  \n",
    "3、观测到输入什么特征，就预测最可能的label  \n",
    "4、判别式的优点：对数据量要求没生成式的严格，速度也会快，小数据量下准确率也会好些。  \n",
    "\n",
    "## 13 熵  \n",
    "\n",
    "熵在信息论中代表随机变量不确定度的度量，一个离散型随机变量 X 的熵 H(X) 定义为：  \n",
    "\n",
    "$H(X) = \\sum_{i=1}^n p(x) \\log{p(x)}$  \n",
    "\n",
    "## 14 "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
