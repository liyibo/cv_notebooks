{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尽管卷积神经网络在单幅图像超分辨率的准确性和速度方面有很大的突破，但是在大尺度缩放图像上进行超分辨率重建时，如何恢复更精细的纹理特征的问题还没有解决。最近的工作主要集中在最小化均方重建误差，由此产生的估计具有较高的峰值信噪比，但是它们通常缺乏高频细节，并没有达到较高分辨率所期望的逼真度，感觉上不能令人满意。  \n",
    "\n",
    "这篇文章提出SRGAN，图像超分辨率（SR）的生成对抗网络（GAN）。它是第一个能推出4倍放大因子的照片般逼真的自然图像的框架。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 摘要  \n",
    "1 输入数据和对应 label：原始图像作为下采样 4 倍的图像作为输入 x，原始图像作为对应的 label  \n",
    "\n",
    "2 用一个残差网络作为 Generator，输入 x 经过该网络之后，输出就是对应的高分辨率图像  \n",
    "\n",
    "3 用一个分类网络作为 Discriminator，用来对原始图像和生成的高分辨率图像进行分类  \n",
    "\n",
    "4 用 VGG 网络分别提取原始图像和生成的高分辨率图像的高层特征，引入一个高层感知损失函数来优化 Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 论文介绍  \n",
    "\n",
    "### Abstract  \n",
    "\n",
    "尽管使用更快和更深的卷积神经网络在单幅图像超分辨率的准确性和速度方面取得了突破，但是一个中心问题仍然没有解决：在大尺度缩放的图像上进行超分辨率重建时，如何恢复更精细的纹理特征？基于优化的超分辨率方法的行为主要由目标函数的选择驱动。最近的工作主要集中在最小化均方重建误差。由此产生的估计具有较高的峰值信噪比，但是它们通常缺乏高频细节，并没有达到较高分辨率期望的逼真度，感觉上不能令人满意。在本文中，我们提出SRGAN，图像超分辨率（SR）的生成对抗网络（GAN）。据我们所知，它是第一个能推出4倍放大因子的照片般逼真的自然图像的框架。为了达到这个目的，我们提出了一个由对抗性损失和内容损失构成的感知损失函数。对抗损使用一个判别网络促使我们的结果与真实图像更相似。此外，我们使用感知相似性驱动的内容损失，而不是像素空间中的相似性。我们的深度残差网络能够从公共基准上严重下采样的图像上恢复真实照片般逼真的纹理。平均评分（MOS）测试显示使用 SRGAN 的感知质量的巨大显着增益。用 SRGAN 获得的 MOS 分数比用最先进的方法获得的分数更接近于原来的高分辨率图像。  \n",
    "\n",
    "### Introduction  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/SRGAN_1.jpg?raw=true)\n",
    "<center> ** Figure 1 ** </center >\n",
    "\n",
    "从低分辨率（LR）图像估算高分辨率（HR）图像这样具有高度挑战性的任务被称为超分辨率（SR）。 SR 在计算机视觉研究领域得到了广泛的关注，并具有广泛的应用。  \n",
    "\n",
    "欠定 SR 问题的不适定性质对于高比例因子特别明显，因为重构 SR 图像中的纹理细节通常是不存在的。监督 SR 算法的优化目标通常是恢复的 HR 图像与真实之间的均方误差（MSE）的最小化。这样做很方便，因为最小化 MSE 也可以最大化峰值信噪比（PSNR），这是用于评估和比较 SR 算法的常用度量。然而，MSE（和PSNR）捕捉感知相关差异（如高纹理细节）的能力非常有限，因为它们是基于按像素的图像差异定义的。如 Figure 2 所示，其中最高的 PSNR 不一定反映感知上较好的 SR 结果。超分辨率和原始图像之间的感知差异意味着恢复的图像不像 Ferwerda 所定义的真实照片。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/SRGAN_2.jpg?raw=true)\n",
    "<center> ** Figure 2 ** </center >\n",
    "\n",
    "在这项工作中，我们提出了一个超分辨率的对抗生成网络（SRGAN），采用了一个深度残差网络（ResNet）和 MSE 作为唯一的优化目标。与以前的研究不同，我们使用 VGG 网络的高层次特征映射和鉴别器来定义一种新的感知损失，鉴别器在感知上很难区分 HR 参考图像。Figure 1 显示了一个 4 倍放大因子超分辨率重建后的效果图。  \n",
    "\n",
    "### Related work  \n",
    "\n",
    "#### 图像超分辨率  \n",
    "\n",
    "最近关于图像的综述文章包括 Nasrollahi 和 Moeslund 或 Yang 等人。在这里，我们将重点关注单幅图像超分辨率（SISR），而不会进一步讨论从多幅图像恢复HR图像的方法。  \n",
    "\n",
    "基于预测的方法是处理SISR的方法之一。尽管这些过滤方法，例如线性，双三次或 Lanczos 滤波可以非常快速，它们过分简化了 SISR 问题，并且通常会产生具有过度平滑纹理的解决方案。目前已经有人已经提出了特别关注边缘保存的方法。  \n",
    "\n",
    "更强大的方法旨在建立低分辨率和高分辨率图像信息之间的复杂映射，并且通常依赖于训练数据。许多基于示例对的方法都依赖于 LR 训练补丁，而相应的HR 标签是已知的。早期的工作由 Freeman 等人提出。SR 问题的相关方法来源于压缩感知。在 Glasner 等人作者利用图像内的各个尺度的补丁冗余来驱动SR。 Huang 等人也采用了这种自相似的范式。通过进一步允许小的转换和形状变化来扩展自我字典。顾等人提出了一种卷积稀疏编码方法，通过处理整个图像而不是重叠补丁来提高一致性。  \n",
    "\n",
    "Tai等人为了重建逼真的纹理细节，同时避免了边缘伪影，在之前，将基于梯度曲线的边缘定向的SR算法与基于学习的细节合成相结合。张等人提出了一个多尺度词典来捕捉不同尺度的相似图像块的冗余。为了超级解决地标图像，岳等人检索与网络中具有相似内容的 HR 图像的相关性，并提出用于对齐的结构感知匹配标准。    \n",
    "\n",
    "最近，基于卷积神经网络（CNN）的 SR 算法表现出优异的性能。 Wang et al 在基于学习迭代收缩和阈值算法（LISTA）将稀疏表示先编码成其前馈网络结构。董等人使用双三次插值来提高输入图像，并端到端地训练三层深全卷积网络，以实现最先进的 SR 性能。随后显示，使网络能够直接学习升级滤波器可以进一步提高精度和速度方面的性能。 Kim 等人用深度递归卷积网络（DRCN）提出了一个高性能的架构，允许远距离的像素依赖性，同时保持模型参数的数量小。与我们的论文特别相关的是Johnson等人和布鲁纳等的著作，他们依靠可感知损失相似的损失函数来恢复视觉上更令人信服的 HR 图像。  \n",
    "\n",
    "#### 卷积神经网络的设计  \n",
    "\n",
    "在 Krizhevsky 等人的工作成功后，在许多视觉任务上达到 state of the art 效果的方法都是通过设计特定的 CNN 网络结构达到的。  \n",
    "\n",
    "结果表明，较深的网络架构可能难以训练，但有可能显著提高网络的准确性，因为它们允许非常高的复杂性的建模映射。为了有效地训练这些更深层次的网络架构，常常使用 batch normalization 来抵消内部的协变量转换。更深入的网络体系结构也被证明可以提高 SISR 的性能，Kim 等人制定一个递归的 CNN 达到 state of the art 的效果。最近引入了残留块和跳跃连接的概念，这是减轻深度 CNN 训练的另一个强大的设计选择。  \n",
    "\n",
    "#### 损失函数  \n",
    "\n",
    "诸如 MSE 之类的逐像素损失函数努力处理恢复丢失的高频细节（例如纹理）中固有的不确定性：最小化 MSE 鼓励寻找通常过度平滑且感知质量较差的似然解的像素平均值。不同感知质量的重构以 Figure 2 中相应的 PSNR 为例。我们举例说明了 Figure 3 中最小化 MSE 的问题，其中多个具有高纹理细节的潜在解被平均以创建平滑重建。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/SRGAN_3.jpg?raw=true)\n",
    "<center> ** Figure 3 ** </center >\n",
    "\n",
    "在 Mathieu 等人和 Denton 等作者通过使用生成对抗网络（GANs）应用图像生成来解决这个问题。Yu 和 Porikli 用鉴别器损失增加了像素方向的 MSE 损失，以训练一个网络，该网络能够以大的放大倍数（8×）超解析人脸图像。GANS 也被用于 Radford 等人的无监督表示学习。Li 和 Wand 描述了使用 GANs 来学习从一个流形到另一个流形的映射，而 Yeh 等人进行修补。Bruna 等人最小化 VGG19 和散射网络的特征空间的平方误差。  \n",
    "\n",
    "Dosovitskiy 和 Brox 基于神经网络特征空间中计算的欧几里得距离的损失函数结合对抗训练。结果表明，所提出的损失允许在视觉上更优的图像生成，并且可以用于解决对非线性特征表示进行解码的不适定的逆问题。类似于这项工作，约翰逊等人和布鲁纳等提出使用从预训练的 VGG 网络中提取的特征，而不是低级像素方式的误差测量。具体而言，作者基于从 VGG19 网络提取的特征图之间的欧氏距离制定损失函数。在超分辨率和艺术风格转移方面，获得了更令人信服的结果。最近，Li 和 Wand 也研究了在像素或 VGG 特征空间中比较和混合色块的效果。  \n",
    "\n",
    "### 贡献  \n",
    "\n",
    "GANs 提供了一个强大的框架，以产生具有高感知质量的看起来更自然的图像。GAN 程序鼓励朝着包含相片逼真图像的高概率的搜索空间的区域移动的方向重建，因此更接近自然图像，如 Figure 3 所示。  \n",
    "\n",
    "在本文中，我们描述了一个非常深的 ResNet 架构，主要贡献是：  \n",
    "\n",
    "- 我们利用 PSNR 测量的高放大倍数（4倍）和与我们为 MSE 优化的 16 块深度ResNet（SRResNet）的结构相似度（SSIM）为在 SR 上达到了 state of the art 的效果。   \n",
    "\n",
    "- 我们提出 SRGAN，这是一个基于 GAN 的网络，针对新的感知损失进行了优化。在这里，我们用在 VGG 网络的特征映射上计算的损失代替基于 MSE 的内容损失，这对于像素空间的变化是更鲁棒的。  \n",
    "\n",
    "- 我们通过对来自三个公共基准数据集的图像进行广泛的平均评分（MOS）测试来证实，SRGAN 具有 state of the art 的效果。  \n",
    "  \n",
    "### Method  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/SRGAN_6.jpg?raw=true)\n",
    "<center> ** Figure 4 ** </center >\n",
    "\n",
    "在 SISR 中，目标是从低分辨率 $I^{LR}$ 输入图像估计高分辨率，超分辨率的图像 $I^{SR}$。在这里，$I^{LR}$ 是其高分辨率对应 $I^{SR}$ 的低分辨率版本。高分辨率图像仅在训练期间可用。在训练中，$I^{LR}$ 通过对 $I^{SR}$ 应用高斯滤波器，然后使用下采样因子 r 进行下采样操作来获得。对于具有 C 色通道的图像，我们通过尺寸为 W×H×C 的实数张量来描述 $I^{LR}$，并且分别用 rW×rH×C 来描述 $I^{HR}$，$I^{SR}$。  \n",
    "\n",
    "我们的最终目标是训练一个生成函数 G，为给定的 LR 输入图像估计其对应的 HR 图像。为了达到这个目的，我们训练一个生成网络作为一个由 $\\theta_G$ 参数化的前馈 CNN $G_{\\theta_G}$。这里 $\\theta_G = {W_{1:L};b_{1:L}}$ 表示 L 层深度网络的权重和偏差，并通过优化 SR 特定损失函数 $l^{SR}$ 来获得。对于训练图像 $I_n^{HR},n = 1，...。。。,N $ 与相应的 $I_n^{LR},n = 1，...。。。,N $，我们解决：  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/SRGAN_4.jpg?raw=true)\n",
    "\n",
    "在这项工作中，我们将特别设计一个感知损失 $l^{SR}$ 作为几个损失组件的加权组合，模拟恢复的 SR 图像的不同的期望特征。 \n",
    "\n",
    "#### 对抗网络结构  \n",
    "\n",
    "根据 Goodfellow 等，我们进一步定义了一个鉴别器网络 $D_{\\theta_D}$，我们与 $G_{\\theta_G}$ 一起以交替的方式进行优化，以解决对抗最小最大问题：  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/SRGAN_5.jpg?raw=true)\n",
    "\n",
    "这个公式背后的一般思想是，它允许人们训练一个生成模型 G，目的是欺骗一个可区分的鉴别器 D，该鉴别器被训练成将超分辨图像与真实图像区分开来。 利用这种方法，我们的生成器可以学习创建与真实图像高度相似的解决方案，因此难以用 D 进行分类。这鼓励了存在于自然图像的子空间，流形中的感知上的解决方案。这与通过最小化逐像素误差测量（例如MSE）获得的 SR 解决方案相反。  \n",
    "\n",
    "在我们非常深的生成器网络 G 的核心，如 Figure 4 所示，是具有相同布局的 B 残余块。受 Johnson 等人的启发我们采用了 Gross 和 Wilber 提出的区块布局。具体而言，我们使用了两个小的 3×3 内核和 64 个特征映射的卷积层，其次是批量归一化层和 ParametricReLU 作为激活函数。Shi 等人提出的利用两个训练的子像素卷积层来增加输入图像的分辨率。  \n",
    "\n",
    "为了从生成的 SR 样本中辨别真实的 HR 图像，我们训练鉴别器网络。架构如 Figure 4 所示。我们遵循 Radford 等人总结的架构指南。并使用 LeakyReLU 激活（α= 0.2），并避免整个网络的最大池化。鉴别器网络被训练以解决公式2中的最大化问题。它包含 8 个卷积层，其中 3×3 滤波器内核的数量增加，从 VGG 网络中的 64 个到 512 个内核增加 2 倍。每次功能数量增加一倍时，使用分段卷积来降低图像分辨率。得到的 512 个特征图之后是两个密集层和一个最终的 S 形激活函数，以获得样本分类的概率。  \n",
    "\n",
    "#### 感知损失函数  \n",
    "\n",
    "我们的感知损失函数的定义对于我们的生成网络的性能是至关重要的。虽然 $l^{SR}$ 通常基于 MSE 进行建模，但我们改进了Johnson 等人和布鲁纳等人的方法。并设计一个损失函数，就感知相关的特性进行评估。我们将知觉损失制定为内容损失 $l_X^{SR}$ 和对抗损失部分的加权和：  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/SRGAN_7.jpg?raw=true)\n",
    "\n",
    "在下面我们描述可能的选择内容损失 $l_X^{SR}$ 和对抗损失 $l_{Gen}^{SR}$  \n",
    "\n",
    "##### 内容损失  \n",
    "\n",
    "以像素为单位的 MSE 损失计算如下：  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/SRGAN_8.jpg?raw=true)\n",
    "\n",
    "这是最广泛使用的图像 SR 的优化目标。然而，在实现特别高的 PSNR 的同时，MSE 优化问题的解决方案常常缺乏导致感知不足的高频率内容。  \n",
    "\n",
    "区别于依靠像素方面的损失，我们建立在 Gatys 等人的想法上。并使用更接近知觉相似性的损失函数。我们基于 Simonyan 和 Zisserman 中描述的预训练的 19 层 VGG 网络的 ReLU 激活层来定义 VGG 损失。用 $\\phi_{i,j}$ 表示在 VGG19 网络中的第 i 个最大层数之前通过第 j 个卷积（在激活之后）获得的特征图，我们考虑给出。然后我们将 VGG 损失定义为重构图像 $G_{\\theta_G}(I^{LR})$ 和参考图像 $I^{HR}$ 的特征表示之间的欧氏距离：  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/SRGAN_9.jpg?raw=true)  \n",
    "\n",
    "这里 $I_{i,j}$ 和 $H_{i,j}$ 描述了 VGG 网络中各个特征映射的尺寸。  \n",
    "\n",
    "##### 对抗损失  \n",
    "\n",
    "除了迄今为止描述的内容损失之外，我们还将 GAN 的生成元素添加到感知损失中。这鼓励我们的网络倾向于通过尝试欺骗鉴别器网络来支持驻留在自然图像多方面的解决方案。基于所有训练样本上的鉴别器 $D_{\\theta_D}(G_{\\theta_G}(I^{LR}))$ 的概率来定义生成损失 $l_{Gen}^{SR}$：  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/SRGAN_10.jpg?raw=true)\n",
    "\n",
    "这里 $D_{\\theta_D}(G_{\\theta_G}(I^{LR}))$ 是重建图像 $G_{\\theta_G}(I^{LR})$ 是自然 HR 图像的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 数据准备  \n",
    "\n",
    "1 t_target_image：原始图像随机裁剪成统一大小之后的图像  \n",
    "\n",
    "2 t_image：t_target_image下采样四倍的输入图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_image = tf.placeholder('float32', [batch_size, 96, 96, 3], name='t_image_input_to_SRGAN_generator')\n",
    "t_target_image = tf.placeholder('float32', [batch_size, 384, 384, 3], name='t_target_image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 应用 Generator 和 Discriminator  \n",
    "\n",
    "1 SRGAN_g：根据输入的低分辨率图像生成对应的高分辨率图像  \n",
    "\n",
    "2 SRGAN_d：输出生成的图像和原始高分辨率图像的分类结果  \n",
    "\n",
    "3 Vgg19_simple_api：图像缩放以适应VGG网络的输入格式，分别提取生成的图像和原始高分辨率图像的高层特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net_g = SRGAN_g(t_image, is_train=True, reuse=False)\n",
    "net_d, logits_real = SRGAN_d(t_target_image, is_train=True, reuse=False)\n",
    "_,     logits_fake = SRGAN_d(net_g.outputs, is_train=True, reuse=True)\n",
    "t_target_image_224 = tf.image.resize_images(t_target_image, size=[224, 224], method=0, align_corners=False) # resize_target_image_for_vgg # http://tensorlayer.readthedocs.io/en/latest/_modules/tensorlayer/layers.html#UpSampling2dLayer\n",
    "t_predict_image_224 = tf.image.resize_images(net_g.outputs, size=[224, 224], method=0, align_corners=False) # resize_generate_image_for_vgg\n",
    "\n",
    "net_vgg, vgg_target_emb = Vgg19_simple_api((t_target_image_224+1)/2, reuse=False)\n",
    "_, vgg_predict_emb = Vgg19_simple_api((t_predict_image_224+1)/2, reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 损失函数构建  \n",
    "\n",
    "1 d_loss：优化Discriminator网络的损失函数，logits_real是真实高分辨率图像，logits_fake是生成的图像，Discriminator的作用就是尽可能的区分出这两种图像  \n",
    "\n",
    "2 g_loss：优化GAN的损失函数，g_gan_loss将生成的图像作为真实的图像输入Discriminator，用来欺骗Discriminator，mse_loss是生成图像与真实图像的均方差损失，用来优化Generator网络，vgg_loss是引入的感知损失函数，用来优化Generator网络，使生成的图像更加自然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_loss1 = tl.cost.sigmoid_cross_entropy(logits_real, tf.ones_like(logits_real), name='d1')\n",
    "d_loss2 = tl.cost.sigmoid_cross_entropy(logits_fake, tf.zeros_like(logits_fake), name='d2')\n",
    "d_loss = d_loss1 + d_loss2\n",
    "\n",
    "g_gan_loss = 1e-3 * tl.cost.sigmoid_cross_entropy(logits_fake, tf.ones_like(logits_fake), name='g')\n",
    "mse_loss = tl.cost.mean_squared_error(net_g.outputs , t_target_image, is_mean=True)\n",
    "vgg_loss = 2e-6 * tl.cost.mean_squared_error(vgg_predict_emb.outputs, vgg_target_emb.outputs, is_mean=True)\n",
    "\n",
    "g_loss = mse_loss + vgg_loss + g_gan_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 训练网络  \n",
    "\n",
    "1 g_optim_init：用来初始化Generator网络  \n",
    "\n",
    "2 g_optim，d_optim：训练GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Pretrain\n",
    "g_optim_init = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(mse_loss, var_list=g_vars)\n",
    "## SRGAN\n",
    "g_optim = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "d_optim = tf.train.AdamOptimizer(lr_v, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "###========================= initialize G ====================###\n",
    "sess.run(tf.assign(lr_v, lr_init))\n",
    "print(\" ** fixed learning rate: %f (for init G)\" % lr_init)\n",
    "for epoch in range(0, n_epoch_init+1):\n",
    "    epoch_time = time.time()\n",
    "    total_mse_loss, n_iter = 0, 0\n",
    "    ## If your machine have enough memory, please pre-load the whole train set.\n",
    "    for idx in range(0, len(train_hr_imgs), batch_size):\n",
    "        step_time = time.time()\n",
    "        b_imgs_384 = tl.prepro.threading_data(\n",
    "                train_hr_imgs[idx : idx + batch_size],\n",
    "                fn=crop_sub_imgs_fn, is_random=True)\n",
    "        b_imgs_96 = tl.prepro.threading_data(b_imgs_384, fn=downsample_fn)\n",
    "        ## update G\n",
    "        errM, _ = sess.run([mse_loss, g_optim_init], {t_image: b_imgs_96, t_target_image: b_imgs_384})\n",
    "\n",
    "        \n",
    "###========================= train GAN (SRGAN) =========================###\n",
    "for epoch in range(0, n_epoch+1):    \n",
    "    epoch_time = time.time()\n",
    "    total_d_loss, total_g_loss, n_iter = 0, 0, 0\n",
    "\n",
    "    ## If your machine have enough memory, please pre-load the whole train set.\n",
    "    for idx in range(0, len(train_hr_imgs), batch_size):\n",
    "        step_time = time.time()\n",
    "        b_imgs_384 = tl.prepro.threading_data(\n",
    "                train_hr_imgs[idx : idx + batch_size],\n",
    "                fn=crop_sub_imgs_fn, is_random=True)\n",
    "        b_imgs_96 = tl.prepro.threading_data(b_imgs_384, fn=downsample_fn)\n",
    "        ## update D\n",
    "        errD, _ = sess.run([d_loss, d_optim], {t_image: b_imgs_96, t_target_image: b_imgs_384})\n",
    "        ## update G\n",
    "        errG, errM, errV, errA, _ = sess.run([g_loss, mse_loss, vgg_loss, g_gan_loss, g_optim], {t_image: b_imgs_96, t_target_image: b_imgs_384})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考  \n",
    "\n",
    "1 [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802)  \n",
    "2 [SRGAN 源码](https://github.com/zsdonghao/SRGAN)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
