{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract  \n",
    "\n",
    "因为神经网络训练过程中网络参数会不断的变化，导致每层输入数据的数据分布都会发生变化，从而增加训练模型的复杂度。一般通过降低学习率和小心的初始化等方法来缓解这个问题，但是这样会减慢训练速度，并且使得训练模型变得非常困难。我们将这种现象称为 internal covariate\n",
    "shift，并通过归一化层输入数据来解决这个问题问题。我们将数据标准化作为模型架构的一部分，并对每个 mini-batch 进行标准化处理。Batch Normalization 使我们能够使用更高的学习速率，并且不用太在意初始化，同时也可以用来作为 regularizer，在某些情况下替换掉 Dropout。应用到state-of-the-art 的图像分类模型中，Batch Normalization 在达到相同准确度的同时，训练速度加快了 14 倍。使用加了 Batch Normalization 的网络可以提高原始网络在 ImageNet 分类任务中的准确率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction  \n",
    "\n",
    "深度学习极大地推动了视觉，语音和其他领域的发展。随机梯度下降（SGD）已被证明是训练深度网络的一种有效方式，而动量和 Adagrad 等 SGD 改进方法也被用于在不同任务中去达到 state of the art 的性能。SGD 优化了网络的参数 $\\Theta$，从而最小化损失函数  \n",
    "\n",
    "$ \\Theta = arg \\ min_{\\theta} \\frac{1}{N} \\sum_{i=1}^N l(x_i,\\Theta) $  \n",
    "\n",
    "其中 $ x_{1...N} $ 是训练数据。使用 SGD 时，分步进行训练，每一步都会使用大小为 $m$ 的 minibatch $ x_{1...N} $。通过计算 minibatch 的 $\\frac{1}{m} \\frac {\\partial l(x_i,\\Theta)}{\\partial \\Theta}$ 来近似计算损失函数对于网络参数的梯度。  \n",
    "\n",
    "使用 minibatch 进行训练相比单个样本进行训练有几个好处。首先，minibatch 上损失函数的梯度是训练集上梯度估计值，其质量随着 minibatch 大小的增加而提高。其次，由于现代计算平台提供的并行性，对于一个 batch 的计算可以比单个示例的 m 个计算效率高得多。  \n",
    "\n",
    "虽然随机梯度简单而有效，但它需要仔细调整模型超参数，特别是优化中使用的学习速率以及模型参数的初始值。由于每层的输入都受到前面所有层的参数的影响，并且随着网络的加深，网络参数的微小变化带来的影响就会放大，从而导致训练变得更加复杂。  \n",
    "\n",
    "层输入数据分布的变化是一个问题，因为层需要不断适应新的数据分布。当学习系统的输入分布发生变化时，就遇到了 internal covariate shift 问题。这通常通过作用域调整来处理。然而，covariate shift 的概念可以扩展到整个学习系统之外，如子网络或层等。考虑一个网络计算  \n",
    "\n",
    "$l = F_2(F_1(u,\\Theta_1),\\Theta_2)$  \n",
    "\n",
    "其中 $F_1$ 和 $F_2$ 是任意变换，并且 $ \\Theta_1,\\Theta_2 $ 是为了最小化损失函数而优化的参数。学习 $ \\Theta_2 $ 可以看作子网络的输入是 $ x = F_1(u,\\Theta_1)$,损失函数是 $ l = F_2(x,\\Theta_2) $。  \n",
    "\n",
    "对于 $ \\Theta_2 $ 参数的学习就如下所示：  \n",
    "\n",
    "$ \\Theta_2 \\leftarrow \\Theta_2 - \\frac{\\alpha}{m} \\sum_{i=1}^m \\frac{\\partial F_2(x,\\Theta_2)}{\\partial \\Theta_2} $\n",
    " \n",
    "（$ m 和 \\alpha$ 分别为 batch 大小和学习率）与输入 x 的独立网络 $F_2$ 完全等价。因此，好的数据分布特征可以使训练更加有效的性质（例如训练和测试数据之间具有相同的分布）也适用于训练子网络。因此，x 的分布随时间保持固定是有利的，因为，$ \\Theta_2 $不必重新调整以补偿 x 的分布变化。  \n",
    "\n",
    "固定传给子网络的输入分布也会对子网外的层产生积极影响。考虑具有 S 形激活函数 $ z = g(Wu + b) $ 的层，其中 u 是层输入，权重矩阵 W 和偏差矢量 b 是要学习的层参数，并且 $ g(x) = \\frac {1}{1 + {exp}(-x)}$。随着 $|x|$ 的增加，$ g'(x) $ 趋向于零。这意味着对于 $ x = Wu + b $ 的所有维度，除了值绝对小的那些维度以外，流向下层的梯度将消失，并且模型训练速度将变慢。然而，由于 x 受 W，b 和下面所有层参数的影响，在训练期间对这些参数的改变可能将 x 的许多维度移动到非线性的饱和状态并且减慢收敛。随着网络深度的增加，这种影响会被放大。在实践中，通常通过使用$ ReLU(x)= max(x,0) $，仔细的初始化和使用小的学习率来解决饱和问题和饱和问题带来的梯度消失问题。但是，如果我们可以确保非线性输入的分布在网络训练时保持更稳定，那么优化器将不太可能陷入饱和状态，并且加快训练速度。  \n",
    "\n",
    "我们将训练过程中网络内各层节点之间数据分布的变化称为 internal covariate shift，消除这个影响就会加快训练速度。我们提出了一种称为 Batch Normalization 的机制，它采取了一个减少 internal covariate shift 的步骤，这样做显著加快了深度神经网络训练的速度。它通过固定层输入的均值和方差进行标准化来减少输入数据数据分布的差异。批量标准化还可以通过降低对参数的大小或初始化方法的依赖性来对网络的梯度产生有利影响。这使我们能够使用更高的学习率。此外，批量标准化正则化了模型并减少了对 Dropout 的需求。最后，批量标准化可以防止网络使用饱和非线性激活函数时陷入饱区域中。\n",
    "\n",
    "在 4.2 中，我们将 Batch Normalization 应用于最佳的 ImageNet 分类网络中，并表明我们只需使用 7％ 的训练时间即可匹配其性能，并且可以进一步超过其精度。使用 Batch Normalization 训练的这种网络集合，提高了当前在 ImageNet 分类任务中 top-5 的准确率。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Towards Reducing Internal Covariate Shift  \n",
    "\n",
    "我们将训练期间网络参数变化而引起的网络激活分布的变化定义为 Internal Covariate Shift。为了改善训练过程，我们试图减少 Internal Covariate Shift。通过在训练过程中固定层输入 x 的分布来提高训练速度。有文献指出，如果将网络输入数据进行白化处理（零均值和单位方差）并且去相关，网络训练收敛会更快。由于网络中每层的输入都是由前面一层输出生成的，所以对每层输入数据进行相同的白化处理是有利的。通过白化每层的输入数据，就能固定输入数据的数据分布，从而可以消除 Internal Covariate Shift 问题。  \n",
    "\n",
    "我们可以在每一步的训练中或某个时间间隔白化激活值，也可以通过直接调整模型的结构或者依照激活值来修改优化函数的参数进行白化。但是，如果这些方法在优化步骤中执行，那么在更新参数的时候，必须先计算归一化后的值的误差，这降低了梯度下降的效果。例如，考虑添加了学习偏差 b 的输入为 u的图层，并通过减去在训练数据上计算的激活平均值来对结果进行归一化：$\\hat x  = x - E[x]$ 其中 $ x = u + b $, $ X = {x_{1...N}} $ 是训练集上 x 的集合，$E[x] = \\frac{1}{N} \\sum_i^N x_i$。如果梯度下降步骤忽略了 $E[x]$ 对 b 的依赖性，那么它将更新 $b \\leftarrow b + \\Delta b $，其中 $ \\Delta b \\propto - \\frac {\\partial l}{\\partial \\hat x}$。那么 $ u + (b + \\Delta b) - E[u + (b + \\Delta b)] = u + b + E[u + b]$。因此，当归一化和参数 b 的更新同时进行的时候，会使该神经网络层的输出没有变化，进一步的导致损失函数的值没有变化。随着训练的继续，b 一直在增长而损失函数的值却一直不变。当归一化操作不仅中心化激活值，同时对其进行缩放，这个现象会更严重。在初步的实验中我们观察到，如果归一化参数不在梯度下降中计算的话，会使模型参数膨胀。  \n",
    "\n",
    "上述方法的问题在于梯度下降优化没有考虑标准化的情况。为了解决这个问题，我们希望确保对于任何参数值，网络始终以期望的分布产生激活值。这样就能使模型参数的损失函数梯度都把标准化的情况考虑进去，以及它对模型参数 $\\Theta$ 的依赖性。再次将 x 设为层输入，将其视为向量，X 是训练数据集上的这些输入的集合。归一化可以写成一个变换 $\\hat x  = Norm(x,X)$，它不仅取决于给定的训练样例 x，而且取决于所有示例 X - 如果 x 由另一个层生成，则每个示例取决于Θ。对于反向传播，我们需要计算雅可比行列式  \n",
    "\n",
    "$\\frac {\\partial Norm(x,X)}{\\partial x}$ 和 $\\frac {\\partial Norm(x,X)}{\\partial X}$。  \n",
    "\n",
    "忽略后者将导致上述爆炸。在该框架内，白化层输入是昂贵的，因为它需要计算协方差矩阵 $Cov[x] = E_{x \\in X}[xx^T] - E[x]E[x]^T$ 及其负平方根，来产生白化激活值 $ Cov[x]^{-1/2}(x - E[x]) $，以及这些变换的反向传播的导数。这激励我们寻求一种替代方案，可微并且在参数更新的时候不需要对整个训练数据集进行分析的神经网络层输入归一化方法。  \n",
    "\n",
    "一些先前的方法使用在单个训练示例上计算的统计量，或者在图像处理任务中，在不同特征图的相同位置上的数据的统计量。但是，由于去掉了激活值的绝对值缩放，而改变了网络的表征能力。我们希望通过同时考虑单个训练样本和整个训练样本集的统计信息来归一化激活值，从而在网络中保留更多的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Normalization via Mini-Batch Statistics  \n",
    "\n",
    "由于每层输入的完全白化成本很高，而且不是处处可微的，所以我们做了两个必要的简化。首先，不是将层输入和输出中的特征同时进行归一化，我们对标量特征进行单独的 0 均值 1 方差归一化。对于具有 d 维输入 $x = (x^{{1}}...x^{{d}})$，我们将对每个维度进行归一化  \n",
    "\n",
    "$\\hat x^{(k)} = \\frac {x^{(k)} - E[x^{(k)}]}{\\sqrt {Var[x^{(k)}]}}$\n",
    "\n",
    "其中期望值和方差通过训练数据集计算得到。即使特征没有去相关，这种归一化也能加速收敛。  \n",
    "\n",
    "请注意，简单地标准化层的每个输入可能会改变该层可以表示的内容。例如，标准化 sigmoid 输入将会将它们限制在非线性函数的线性端。为了解决这个问题，我们确保穿插在网络内部的变换能够表示同样的变换。为了实现这一点，我们为每个激活 $x^{(k)}$ 引入一对参数 $\\gamma^{(k)}, \\beta^{(k)}$，它们对归一化值进行缩放和移位：  \n",
    "\n",
    "$y^{(k)} = \\gamma^{(k)} \\hat x^{(k)} + \\beta^{(k)}$\n",
    "\n",
    "这些参数与原始模型参数一起学习，并恢复网络的表示能力。事实上，如果可以的话，通过设定 $\\gamma^{(k)} = \\sqrt {Var[x^{(k)}]} 和 \\beta^{(k)} = E[x^{(k)}]$，我们可以恢复原始激活值。  \n",
    "\n",
    "在批处理设置中，每个次训练都基于整个训练集，我们将使用整个训练集来规范化激活值。但是，使用随机优化时这是不切实际的。因此，我们进行第二次简化：由于我们在随机梯度训练中使用 mini-batches，每个 mini-batches 产生每次激活的均值和方差的估计。这样，用于标准化的统计信息就可以完全参与梯度反向传播。请注意，通过计算每维度标量的方差而不是协方差矩阵来启用 minibatches; 在协方差矩阵中，正则化将是必需的，因为 batch 的数量可能比激活值向量的大小还要小，会导致奇异协方差矩阵。  \n",
    "\n",
    "考虑大小为 m 的 mini-batch B。由于是对每个激活值进行标准化，让我们专注于特定的激活 $x^{(k)}$ 并省略 k 以简化公式。在 mini-batch 中，我们有 m 个激活值  \n",
    "\n",
    "$ B = {x_{1...m}}$  \n",
    "\n",
    "设归一化值为 $ \\hat x_{1...m}$，其线性变换为 $y_{1...m}$。我们将变换  \n",
    "\n",
    "$BN_{\\gamma,\\beta}:x_{1...m} \\rightarrow y_{1...m}$\n",
    "\n",
    "称为批量归一化变换。 我们在算法 1 中给出了 BN 变换。在该算法中，$\\epsilon$ 是一个常数，添加到 mini-batch 方差以获得数值稳定性。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/train_model/BN/1.jpg?raw=true)  \n",
    "\n",
    "BN 转换可以加入到网络中，并对任何激活值执行 BN 转换。在 $y = BN_{\\gamma,\\beta}(x)$ 中，我们指出要学习参数 $\\gamma 和 \\beta$，BN 转换不仅仅与单个训练样本有关，也与 mini-batch 中的其他训练样本有关。即 $BN_{\\gamma,\\beta}(x)$ 与两者都有关。由缩放和平移变换而来的 y 作为下一层的输入被传递给其他网络层。虽然归一化激活值 $\\hat x$ 被包含在 BN 转换中，但确实非常重要的存在。由于 mini-batch 中的样本 x 都来自同一个分布，因此在忽略 $\\epsilon$ 的情况下，$\\hat x$ 中的每一个元素都为 0 均值 1 方差分布。每个 $\\hat x^{(k)}$ 可看做含有线性变换 $y^{(k)} = \\gamma^{(k)} \\hat x^{(k)} + \\beta^{(k)}$ 以及原始网络其他操作的子网络的输入。子网络的输入保持着相同的均值和方差，并允许归一化后不同的 $\\hat x^{(k)}$ 之间的联合概率分布在训练过程中可变。我们希望这一归一化能够加速子网的训练，并最终加速整个网络的训练。  \n",
    "\n",
    "在训练过程中我们需要计算流过 BN 变换的误差，同时也要计算 BN 变换中涉及到的参数的误差。利用链式规则： \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/train_model/BN/2.jpg?raw=true)  \n",
    "\n",
    "因此，BN 变换是将归一化激活值引入神经网络的一种可微变换。这确保了网络可训练性，以及神经网络层能够持续学习输入数据分布。从而能够减少Internal Covariate Shift发生，从而加速训练。此外，将学习好的仿射变换应用到归一化激活值中，能够允许 BN 变换能够保存网络的表征能力。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Training and Inference with BatchNormalized Networks  \n",
    "\n",
    "根据算法 1 中的定义，我们指定一个激活子集并为它们中的每一个插入 BN 变换，这样就可以对网络进行 BN 变换。先前以 x 为输入的层，现在都会以 $BN(x)$ 作为输入。使用批量标准化的模型可以使用批量梯度下降或随机梯度下降（mini-batch m> 1）或其任何变体进行训练。基于 mini-batch 对激活值进行标准化可以加快网络的训练速度，而前向运算时并不需要进行计算，我们希望输出仅仅依赖于输入。为此，一旦网络已经被训练，我们使用归一化  \n",
    "\n",
    "$\\hat x^{(k)} = \\frac {x^{(k)} - E[x^{(k)}]}{\\sqrt {Var[x^{(k)}]}}$  \n",
    "\n",
    "使用总体而不是 mini-batch 进行统计统计。我们使用无偏方差估计 $Var[x] = \\frac {m}{m-1} E_{B}[\\sigma_B^2]$ ，其中期望和方差是通过训练的 mini-batches 计算得到的。由于平均值和方差在推理过程中是固定的，因此规范化只是一种应用于每次激活的线性变换。它可以进一步由 γ 缩放和 β 移位组成，以产生代替 $BN(x)$ 的单个线性变换。算法 2 总结了训练批量归一化网络的过程。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/train_model/BN/3.jpg?raw=true)\n",
    "\n",
    "## 3.2 Batch-Normalized Convolutional Networks  \n",
    "\n",
    "批量标准化可应用于网络中的任何一组激活。在这里，我们专注于由仿射变换和随后的元素级非线性变换：  \n",
    "\n",
    "$z = g(Wu + b)$\n",
    "\n",
    "其中 W 和 b 是模型的学习参数，$g(\\cdot)$ 是非线性，如 sigmoid 或 ReLU。该公式涵盖全连接层和卷积层。我们通过对 $x = Wu + b$ 归一化来在非线性变换之前添加 BN 变换。我们也可以对层输入 u 进行归一化，但由于 u 很可能是另一种非线性的输出，因此它的分布形状在训练期间可能会改变，并且约束它的第一和第二时刻不会消除协变量移位。相比之下，$Wu + b$ 更可能具有对称的非稀疏分布，即“更高斯”。使其标准化可能会产生更稳定的激活分布。  \n",
    "\n",
    "请注意，由于我们对 $Wu + b$ 进行了归一化，所以偏置 b 可以忽略，因为它的影响将被随后的减均值所抵消。因此，将 $z = g(Wu + b)$ 替换为$z = g(BN(Wu))$，其中 BN 变换单独应用于 $x = Wu$ 的每个维度，每维具有单独的一对学习参数$ \\gamma^{(k)},\\beta^{(k)} $。  \n",
    "\n",
    "对于卷积层，我们还希望归一化服从卷积属性,以便以相同的方式对不同位置的相同特征图的不同元素进行归一化。为了实现这个目标，我们共同标准化minibatch 相同位置下的所有激活值。在算法 1 中，我们让 B 为一个特征图中的所有值的集合，因此对于大小为 m 的 mini-batch 和大小为 p×q 的特征图，我们使用有效大小为 $m' = |B| = m \\cdot pq $ 的 mini-batch 。我们为每个特征图学习一对参数$ \\gamma^{(k)},\\beta^{(k)} $，而不是每个激活值。算法 2 被类似地修改，使得在推断期间，BN 变换将相同的线性变换应用于给定特征图中的每个激活值。  \n",
    "\n",
    "## 3.3 Batch Normalization enables higher learning rates  \n",
    "\n",
    "在传统的深层网络中，学习速率过高可能会导致梯度爆炸或梯度消失，以及陷入局部最小值。批量标准化有助于解决这些问题。通过标准化整个网络中的激活值，它可以防止参数的细微变化会方法梯度的变化或计算出次优的梯度。例如，它可以防止训练陷入饱和的非线性状态。  \n",
    "\n",
    "批量标准化还使得训练对参数大小更鲁棒。通常，大的学习速率可能会增加层参数的尺度，然后在反向传播期间放大梯度并导致梯度爆炸。但是，对于批量标准化，通过层的反向传播不受其参数尺度的影响。事实上，对于一个标量 a  \n",
    "\n",
    "$BN(Wu) = BN((aW)u)$  \n",
    "\n",
    "我们可以证明  \n",
    "\n",
    "$ \\frac {\\partial BN((aW)u)}{\\partial u} = \\frac {\\partial BN((Wu))}{\\partial u} $  \n",
    "\n",
    "$ \\frac {\\partial BN((aW)u)}{\\partial W} = \\frac {1}{a} \\frac {\\partial BN((Wu))}{\\partial W} $ \n",
    "\n",
    "尺度不影响层的雅可比矩阵，也不影响梯度传播。此外，更大的权重会导致更小的梯度，批量标准化将稳定参数增长。    \n",
    "\n",
    "## 3.4 Batch Normalization regularizes the model  \n",
    "\n",
    "在使用批量标准化进行训练时，可以结合其他 mini-batch 中的示例查看训练示例，并且网络不再为给定的示例生成确定值。在实验中，我们发现这种效应对网络的泛化有利。虽然 Dropout 通常用于减少过拟合，但在批量标准化的网络中，我们发现可以移除或减少 Dropout。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Experiments  \n",
    "\n",
    "### 4.1 Activations over time  \n",
    "\n",
    "为了验证 internal covariate shift 对训练的影响以及批量标准化对抗它的能力，我们用MNIST数据集来进行实验。使用了一个非常简单的网络，以 28x28 的二进制图像作为输入，以及 3 个全连接的隐藏层，每层都有 100 个激活。每个隐含层用 sigmoid 计算 $ y = g(Wu + b)$，并且权重 W 初始化为小的随机高斯值。最后一个隐藏层之后是一个具有 10 个激活（每类一个）和交叉熵损失的全连接层。我们对网络进行了 50000 步训练，每个mini-batch 有 60 个示例。将 Batch Normalization 添加到网络的每个隐藏层。我们的目的是对比有无 batch-normalized 情况下网络的运行情况，而不是提高 MNIST 的分类性能。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/train_model/BN/4.jpg?raw=true)\n",
    "\n",
    "上图显示，随着训练的进行，两个网络在测试集上的运行情况。批量归一化网络具有较高的测试精度。为了研究为什么，我们在训练过程中研究了原始网络N 和 batch-normalized 网络 $N_{BN}^{tr}$ 中 sigmoid 函数的输入。在图 1(b,c)中，我们展示了每个网络最后一个隐藏层的一个激活值分布的变化情况。原始网络中的分布随着时间的推移发生显著变化，无论是均值还是方差，这都会使后续层次的训练变得复杂。相比之下，随着训练的进行，batch-normalized 网络中的分布更加稳定，这有助于训练。  \n",
    "\n",
    "### 4.2 ImageNet classification  \n",
    "\n",
    "我们将批量标准化应用于 Inception 网络的新变种，并对 ImageNet 分类任务进行了训练。该网络有大量的卷积层和池化层，并有一个 softmax 层来预测1000 种可能性的图像类。卷积层使用 ReLU 作为非线性激活函数。描述的网络的主要区别在于 5×5 卷积层被两个连续的 3×3 卷积层的128 个滤波器所取代。该网络包含 $13.6 \\cdot 10^6$ 个参数，除了顶层 softmax 层之外，没有全连接层。我们在本文的其余部分将此模型称为初始模型。该模型使用具有动量的随机梯度下降进行训练，使用 mini-batch 大小为 32。  \n",
    "\n",
    "实验中，我们用批量标准化评估了对 Inception 网络进行修改后的一些网络，其中都在非线性变换之前进行 Batch Normalization 操作。  \n",
    "\n",
    "#### 4.2.1 Accelerating BN Networks  \n",
    "\n",
    "简单地将批量标准化添加到网络并不能充分体现我们方法的优点。为此，我们进一步改变了网络及其训练参数，如下所示：  \n",
    "\n",
    "提高学习率：在批量归一化模型中，已经能够从更高的学习速度实现训练加速，没有不良副作用。  \n",
    "\n",
    "移除 Dropout：如第二节所述，批量标准化实现了与 Dropout 相同的一些功能。从修改的 BN-Inception 中移除 Dropout 加快了训练速度，而不会增加过拟合。    \n",
    "\n",
    "减少 L2 正则化权重：在初始阶段，模型参数的 L2 损失控制过拟合，在修正的 BN-Inception 中，这种损失的权重减少了5倍。我们发现这提高验证集上的准确率。  \n",
    "\n",
    "加快学习率衰退：在训练初期，学习率呈指数级衰减。由于我们的网络训练速度比 Inception 快，我们的学习速度降低了6倍。  \n",
    "\n",
    "移除局部响应标准化：虽然初始和其他网络从中受益，但我们发现这对使用批量标准化的网络并不是必需的。  \n",
    "\n",
    "更彻底地打乱训练数据：我们对训练数据采用了 within-shard shuffling，这可以防止相同的实例总是一起出现在一个 mini-batch 中。这样做会使验证准确性提高了约 1%。  \n",
    "\n",
    "减少光度失真：由于 batchnormalized 网络训练速度更快，观察每个训练示例的次数更少，因此我们减少了数据的扩充，使网络更多的关注真实的数据。  \n",
    "\n",
    "#### 4.2.2 Single-Network Classification  \n",
    "\n",
    "我们对以下网络进行了评估，所有网络都在 LSVRC2012 训练集上进行训练，并在验证集上进行测试：  \n",
    "\n",
    "Inception：4.2 节开始描述的网络，初始学习率为 0.0015。  \n",
    "\n",
    "BN-Baseline：Inception 在非线性变换之前加上 Batch Normalization。  \n",
    "\n",
    "BN-x5：BN-Baseline 相同的网络，初始学习率提高了 5 倍，达到 0.0075。 与原始 Inception 上相同的学习率会造成梯度爆炸。  \n",
    "\n",
    "BN-x30：与 BN-x5 一样，但初始学习率为 0.045（初始的30倍）。  \n",
    "\n",
    "BN-x5-Sigmoid：与 BN-x5一样，但使用的激活函数是 sigmoid 而不是RELU。我们也试图用 sigmoid 训练原始网络，但模型准确率不能保证。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/train_model/BN/5.jpg?raw=true)\n",
    "\n",
    "图 2 显示了训练过程中网络在验证集上的准确率。Inception 网络在训练了 $31 \\cdot 10^6$ 之后达到了 72.2% 的准确率。图 3 显示了其他网络达到相同 72.2% 的准确率所需的训练步骤的数量，以及网络达到的最大验证准确度和达到它的步骤数量。  \n",
    "\n",
    "通过仅使用批量标准化（BN-Baseline），我们仅用相比于原始的 Inception 网络一半的训练时间，就达到了相同的准确率。通过 4.2.1 中应用修改，我们显著提高了网络的训练速度。BN-x5 需要比 Inception 少 14 倍的步骤就达到 72.2% 的准确率。有趣的是，进一步提高学习速度（BN-x30）会使模型最初训练速度稍慢，但可以达到更高的最终准确率。经过 $6 \\cdot 10^6$ 步后达到 74.8% ，即比原始网络达到 72.2% 的步数少 5 倍。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/train_model/BN/6.jpg?raw=true)\n",
    "\n",
    "尽管大家都知道训练使用 sigmoid 作为非线性激活函数时的网络很困难。但是我们的 BN-x5-Sigmoid 达到了 69.8% 的准确率。如果没有批处理标准化，使用 sigmoid 非线性激活的 Inception 网络准确率达不到现在的 1/1000。    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 References  \n",
    "\n",
    "1 [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
