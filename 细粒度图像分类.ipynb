{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-based R-CNNs for Fine-grained Category Detection  \n",
    "\n",
    "\n",
    "**首先利用 Selective Search 等算法在细粒度图像中产生物体或物体部位可能出现的候选框（object proposal）。之后类似于 R-CNN 做物体检测的流程，借助细粒度图像中的 object bounding box 和 part annotation 可以训练出三个检测模型（detection model），一个对应细粒度物体级别检测，一个对应物体头部检测，另一个则对应躯干部位检测。然后，对三个检测模型得到的检测框加上位置几何约束，例如，头部和躯干的大体方位、以及位置偏移不能太离谱等。这样便可得到较理想的物体／部位检测结果。接下来将得到的图像块（image patch）作为输入，分别训练一个CNN，则该CNN可以学习到针对该物体／部位的特征。最终将三者的全连接层特征级联（concatenate）作为整张细粒度图像的特征表示。显然，这样的特征表示既包含全部特征（即物体级别特征），又包含具有更强判别性的局部特征（即部位特征：头部特征，躯干特征），因此分类精度较理想。但在Part-based R-CNN中，不仅在训练时需要借助bounding box和part annotation，为了取得满意的分类精度，在测试时甚至还要求测试图像提供bounding box。这便限制了Part-based R-CNN在实际场景中的应用。**\n",
    "\n",
    "## Abstract  \n",
    "\n",
    "通过特定对象局部区域细微外观差异得到的局部的语义可以进行细粒度分类。用于姿态标准化的表示的方法已经提出，但由于对象检测的困难，通常在测试时假定边界框标注。我们提出了一个细粒度分类模型，通过利用由下至上区域提议计算的深度卷积特征来克服这些限制。我们的方法学习整体和局部检测器，在它们之间强制学习几何约束，并从姿态标准化表示预测细粒度类别。对 Caltech UCSD 鸟类数据集的实验证实，我们的方法在端到端评估中胜过了最先进的细粒度分类方法，而无需在测试时间使用边界框。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "由于相关类别中特定区域外观的细微差异，视觉细粒度分类问题可能非常具有挑战性。与常见的识别相比，细粒度分类旨在区分不同品种或物种或产品模型，并且必须以物体姿态为条件进行可靠识别。面部识别是细粒度识别的经典案例，值得注意的是最好的面部识别方法都会去发现面部特征点并从这些位置提取特征。  \n",
    "\n",
    "因此，定位目标的局部区域对于建立目标之间的对应关系以及避免因为姿势和相机视角变化带来的影响是很重要的。已经有工作研究了基于部件的方法来解决这个问题。局部定位确实是许多姿态归一化表示方法的瓶颈。之前已经使用了 Poselet 和 DPM 方法再局部定位上取得了一定的成功;只有在测试时给出已知的边界框时，方法通常才能得到较好的局部定位。通过开发一种新颖的深度局部检测方案，我们提出了一种端到端的细粒度分类系统，该系统在测试时不需要给定对象边界框，并且可以实现与先前报道的方法相媲美的性能，这些方法需要测试时给出真实标注框来过滤误报的检测结果。  \n",
    "\n",
    "卷积网络最近在 ImageNet 挑战上的成功激发了将深度卷积特征应用于相关图像分类和检测任务的进一步工作。Girshick 等人通过将 CNN 应用于一组自下而上的候选区域提议，突破了对象检测的性能，与以前的最佳方法相比，PASCAL 检测性能提高了 30％ 以上。OverFeat 提出使用 CNN 进行目标位置的回来来进行检测。但是，利用深度卷积特征的进展并不局限于基本的目标检测。在许多应用中，如细粒度识别，属性识别，姿态估计等，合理的预测需要准确的区域定位。  \n",
    "\n",
    "特征学习已被用于细粒度识别和属性估计，但受限于局部区域的工程特征。DPD-DeCAF 使用 DeCAF 作为特征描述符，但依靠基于 HOG 的 DPM 进行局部定位。PANDA 学习了特定部分深度卷积网络，其位置取决于基于 HOG 的 poselet 模型。这些模型缺乏 R-CNN 检测的鲁棒性。在这项工作中，我们探索了一种统一的方法，它使用相同的深度卷积表示进行检测以及局部描述。  \n",
    "\n",
    "我们推测自下而上区域提议方法取得的进展，如 selective search，可以提高使目标及局部检测的性能。正如我们后面显示的那样，Caltech-UCSD 鸟类数据集使用 selective search proposals 的平均召回率为 95％。  \n",
    "\n",
    "在本文中，我们提出了一个局部定位模型，它通过利用自下而上 selective search proposals 上计算的深度卷积特征来克服先前的细粒度识别系统的局限性。我们的方法学习局部外观模型并强化局部之间的几何约束。我们的方法概述如图 1 所示。我们已经研究了不同的几何约束，包括在语义外观空间中以最近邻为条件的联合区域定位的非参数模型。用广泛使用的细粒度基准 Caltech-UCSD 鸟类数据集来评估我们的方法，并得到 state-of-the-art 结果。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/1/1.jpg?raw=true) \n",
    "\n",
    "## 2.  Related work  \n",
    "\n",
    "### 2.1 Part-based models for detection and pose localization  \n",
    "\n",
    "以前的工作已经提出了对对象局部外观和位置的显式建模，以便更精确地识别和定位。从图像结构开始，通过 poselets 和相关工作，许多方法已经联合定位了一系列的几何相关的区域。可变形零件模型（DPM），直到最近还是 PASCAL 物体检测最先进的方法，在与整个对象边界框相结合的位置上添加附加学习滤波器的模型部分，允许部件以已学会的变形成本对 anchor 进行偏移。DPM 将这种方法用于强监督训练，其中区域位置在训练时进行标注。这些方法的局限性在于它们使用弱特征（通常是 HOG）。  \n",
    "\n",
    "### 2.2 Fine-grained categorization  \n",
    "\n",
    "最近，大量的计算机视觉研究集中在许多领域的细粒度分类问题，如动物品种或物种，植物物种和人造物体。  \n",
    "\n",
    "有几种方法基于检测和提取物体某些部分的特征。Farrell 等人提出了一种使用 poselets 的姿态归一化表示法。可变形零件模型在也被用于局部定位。基于定位脸部基准特征点的工作，Liu等人提出了一种基于样本的几何方法来检测狗脸并从关键点提取高度局部化的图像特征以区分狗的品种。此外，Berg 等人通过学习每对关键点的描述符，学习了一组高度区分的中间特征。此外，还有文章通过强化姿态和子类一致性来扩展以前的非参数样本法。Yao 等人和Yang 等人研究了模板匹配方法以降低滑动窗口方法的成本。 Goring 等人最近的工作将局部标注从具有类似全局形状的对象转换为非参数局部检测。但是，所有这些 part-based 的方法在测试时都需要真实边界框来进行局部定位或关键点预测。  \n",
    "\n",
    "Human-in-the-loop 的方法要求人们命名对象的属性，点击某些部分或标记最具区分性的区域以提高分类的准确性。基于分割的方法对于细粒度识别也是非常有效的。有方法使用 region-level 线索来推断前景分割 mask 并丢弃背景中的噪声视觉信息。Chai 等表明联合学习局部定位和前景分割可以有益于细粒度分类。与大多数先前的 part-based 的方法类似，这些方法需要真实边界框来初始化分割种子。相比之下，我们的工作目标是在测试时不知道真实标注框的情况下进行端到端的细粒度分类。我们的局部检测器使用基于自底向上区域提议的卷积特征，以及学习的非参数几何约束来更精确地定位对象区域，从而实现强大的细粒度分类。  \n",
    "\n",
    "### 2.3  Convolutional networks  \n",
    "\n",
    "近年来，卷积神经网络（CNN）已被纳入各种领域的许多视觉识别系统中。这些模型的强度至少有一部分在于它们能够从原始数据输入（例如图像像素）中学习辨别特征，这与作为初始预处理步骤计算图像上的手工设计特征的更传统的物体识别流水线相反。LeCun 及其同事最初将这些模型应用于数字识别和OCR，后来应用于通用对象识别任务，CNN 得到了推广。随着大型标签图像数据库和 GPU 实现的引入，这些网络已经成为通用对象分类中最准确的方法。  \n",
    "\n",
    "最近，通用对象检测方法已经开始利用深度 CNN，并且胜过基于传统特性的任何竞争方法。 OverFeat 使用 CNN 在粗滑动窗口检测框架中回归对象位置。对我们工作特别有启发的是 R-CNN 方法，该方法利用区域提案框架中 CNN 的深度特征，在 PASCAL VOC 数据集上实现前所未有的目标检测结果。我们的方法通过将 R-CNN 应用于除了整个对象之外的局部检测来推广 R-CNN，我们的实证结果证明这对精确的细粒度识别是至关重要的。  \n",
    "\n",
    "## 3.  Part-based R-CNNs  \n",
    "\n",
    "虽然证明了 R-CNN 方法在通用物体检测任务（PASCAL VOC）上的有效性，但没有探索这种方法同时在定位和细粒度识别中的应用。因为我们的工作是在这个机制中运作的，所以我们扩展了 R-CNN 来检测物体并将它们的局部定位并对定位进行几何约束。关于感兴趣对象的单个语义部分位置的假设（例如，用于动物类的头部位置），对于倾向于出现在相对于这些粗略固定的位置中的微妙外观差异进行建模变得合理部分。  \n",
    "\n",
    "在 R-CNN 方法中，对于特定对象类别，具有 CNN 特征描述符 $\\phi (x)$ 的候选检测 x 被赋予 $w_0^T \\phi (x)$，其中 $w_0$ 是对象类别的 SVM 学习向量的权重。在我们的方法中，我们假设一个强有力的监督设置，在训练时间，我们不仅为整个对象设定了真实标注框，而且对一组固定的局部区域也进行了标注 ${p_1,p_2,...,p_n}$。  \n",
    "\n",
    "在给定这些局部标注的情况下，在训练时，所有对象及其每个区域都被视为独立的对象类别：我们在区域提议中提取的特征描述符上训练一对多线性 SVM，其中与标注框重叠超过 0.7 的 proposal 被标注为该区域的正例，并且与任何标注框区域重叠小于 0.3 的区域被标记为负样本。因此，对于单个对象类别，我们为整体和局部 ${p_1,p_2,...,p_n}$ 分别学习了 SVM 权重 $w_0$ 和 ${w_1,w_2,...,w_n}$。在测试时，对于每个 proposal，我们计算来自所有SVM 的分数。当然，这些分数并不包含任何有关物体及其局部几何约束的知识；例如，在没有任何额外限制的情况下，鸟头检测器可以在检测器检测区域之外被检测到。因此，我们的最终联合对象和局部假设是使用下面几节中详述的几何打分函数来计算的，它强化了姿势预测与训练时观察姿势的统计数据一致的直观理想属性。  \n",
    "\n",
    "### 3.1 Geometric constraints  \n",
    "\n",
    "设 $X = {x_0，x_1，...，x_n}$ 表示对象 $p_0$ 和 n 个局部区域 ${p_i}_{i=1}^n$ 的位置（边界框），它们在训练数据中被标注，但在测试时是未知的。我们的目标是在先前未见过的测试图像中推断出物体位置和局部区域位置。给定目标和局部区域的 R-CNN 权重 ${w_0，w_1，...，w_n}$，我们将有相应的检测器 ${d_0，d_1，...，d_n}$，其中每个检测器得分为 $d_i(x)= \\sigma (w_i^T \\phi (x))$，其中 $\\sigma (\\cdot)$ 是 sigmoid 函数，$\\phi (x)$ 是在位置 x 处提取的 CNN 特征描述符。我们通过解决以下优化问题来推断对象和局部区域的联合配置：  \n",
    "\n",
    "$X^* = arg \\ max_X \\ \\Delta(X) \\prod_{i=0}^n d_i(x_i)$\n",
    "\n",
    "其中 $\\Delta(X)$ 定义了边界框在联合配置上的评分函数。我们考虑并报告了几个配置评分函数 $\\Delta$ 的定量结果，在以下段落中详细介绍。  \n",
    "\n",
    "框约束。 对物体和零件进行局部化的一个直观的想法是考虑每个可能的物体窗口和物体内的所有窗口，并选择具有最高分数的窗口。 在这种情况下，我们定义评分函数  \n",
    "\n",
    "**边界框约束**。对物体和局部区域进行定位的一个直观的想法是考虑每个可能的物体窗口和物体内的所有窗口，并选择具有最高分数的窗口。在这种情况下，我们定义评分函数  \n",
    "\n",
    "$\\Delta_{box}(X) = \\prod_{i=0}^n c_{x_0}(x_i)$\n",
    "\n",
    "其中  \n",
    "\n",
    "$ c_x(y) = \\begin{cases} 1,&\\text{if region y falls outside region x by at most $\\epsilon$ pixels} \\\\ 0,&\\text{otherwise} \\end{cases} $\n",
    "\n",
    "在我们的实验中，$\\epsilon = 10$。  \n",
    "\n",
    "**几何约束**。由于单个的区域检测器不完善，单个区域检测器得分最高的窗口并不总是正确的，特别是在有遮挡的情况下。因此，我们考虑几个评分函数，以限制检测结果相对于目标位置的布局从而过滤掉不正确的结果。我们定义  \n",
    "\n",
    "$\\Delta_{geometric}(X) = \\Delta_{box}(X) \\prod_{i=0}^n \\delta_i (x_i)$\n",
    "\n",
    "其中 $\\delta_i$ 是给定训练数据的区域 $p_i$ 的位置的评分函数。在之前关于区域定位的工作中，我们试验了$\\delta$的三个定义：  \n",
    "\n",
    "- $\\delta_i^{MG}(x_i)$ 将训练数据中区域 $p_i$ 的 $N_g$ 个成分拟合一个高斯混合模型。实验中，我们设定 $N_g = 4$。  \n",
    "\n",
    "- $\\delta_i^{NP}(x_i)$ 找出 $\\tilde{x}_0$ 相似的 K 个最近邻，其中 $\\tilde{x}_0 = arg \\ max d_0(x_0)$ 是检测器检测结果评分最高的窗口，然后将这 K 个 窗口拟合一个高斯模型。实验中，设置 K = 20。图 2 展示了最近邻的一些例子。  \n",
    "\n",
    "DPM 用每个分量的高斯先验来模拟 deformation costs。 R-CNN 是一个单成分模型，需要定义$\\delta^{MG}$ 或 $\\delta^{NP}$。我们的 $\\delta^{NP}$ 定义受 Belhumeur 等人的启发，但不同之处在于我们在外观上而不是几何上定义最近邻。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/1/2.jpg?raw=true)\n",
    "\n",
    "### 3.2 Fine-grained categorization  \n",
    "\n",
    "我们从局部区域以及整个对象中提取语义特征。最终的特征表示是 $ [\\phi (x_0),...,\\phi (x_n)] $ 其中 $x_0$ 和 $x_{1...n}$ 是使用上一节中的模型推导的整体和局部检测结果，$ \\phi (x_i) $ 是区域 $x_i$ 的特征表示。  \n",
    "\n",
    "在一组实验中，我们从 ImageNet 预先训练的 CNN 中提取深卷积特征 $ \\phi (x_i) $，类似于 DeCAF。为了使 CNN 特征在细粒度鸟类分类的目标任务具有更强去区分性，我们还将从 CUB 数据集根基标注框切分的 200 类鸟类分类任务对 预训练的 CNN 网络进行 fune-tune。特别是，我们用一个新的 200 路 fc8 分层替换了原始的 1000 路 fc8 分类层，并随机初始化这层的权重。我们根据 R-CNN 的建议设置微调的学习率，将全局速率初始化为初始ImageNet 学习速率的十分之一，并在整个训练过程中将其降低 10 倍，但在新的 fc8 中学习速率这是全局学习率的 10 倍。对于整个对象边界框和每个区域边界框，我们独立地对 ImageNet 预先训练的 CNN 进行微调，以对每个区域进行分类，对每个区域进行调整为 227×227 网络输入大小。测试时，我们使用针对特定整个对象或局部区域进行微调的网络，为预测的整个对象或局部区域提取特征。  \n",
    "\n",
    "为了对分类器进行训练，我们采用了一个一对多的线性 SVM 来使用最终的特征表示。对于新的测试图像，我们将全部和局部检测器与几何打分函数一起应用，以获取检测到的局部区域位置并使用特征进行预测。如果在测试图像中没有检测到特定区域 i（由于所有提议低于局部检测器的阈值），我们设置其特征 $ \\phi (x_i) = 0$（零矢量）。  \n",
    "\n",
    "## 4. Evaluation  \n",
    "\n",
    "在本节中，我们对提出的方法进行了评估。具体而言，我们在广泛使用的细粒度基准 Caltech-UCSD 鸟类数据集上进行了实验。分类任务是区分 200 种鸟类，并且由于类别之间的高度相似性从而对计算机视觉系统具有挑战性。它包含 200 种鸟类的 11,788 幅图像。每个图像都用边界框和 15 个关键点的图像坐标标注：喙，背部，胸部，腹部，额头，冠，左眼，左腿，左翼，右眼，右腿，右翼，尾巴，颈背 和喉咙。我们对包含在数据集中的分组进行训练和测试，其中每个物种约 30 个训练样本。遵循之前的协议，我们使用鸟类数据集的两个语义部分：头部和身体。  \n",
    "\n",
    "我们使用开源软件包 Caffe 来提取深度特征并微调我们的 CNN。对于目标和局部区域检测，我们使用 Caffe 参考模型，它与 Krizhevsky 等人使用的模型几乎相同。我们将 CNN 第 n 层的深层特征称为 convn，pooln 或 fcn，分别是卷积层，汇聚层或完全连接层的输出。我们使用 fc6 来训练 R-CNN 目标和局部区域检测器以及用于分类的图像表示。对于 $\\delta^{NP}$，使用 pool5 和余弦距离度量来计算最近邻。  \n",
    "\n",
    "### 4.1 Fine-grained categorization  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/1/3.jpg?raw=true)\n",
    "\n",
    "我们首先展示 Caltech-UCSD 鸟类数据集的标准细粒度分类任务上的结果。表 1 中的第一组结果是在测试时整个鸟类的真实标注框已知的情况下实现的，因为大多数 state-of-art 方法假设这样会使分类任务更容易一些。在这种情况下，我们具有局部非参数几何约束 $\\delta^{NP}$ 的 part-based 方法在没有微调的情况下效果最好，无需微调即可实现 68.1％ 的分类精度。 Finetuning 大幅改善了这一结果，达到 76％ 以上。我们将结果与三种最先进的基准方法进行比较。我们使用深度卷积特征，但他们使用基于 HOG 的 DPM 作为其局部定位方法。性能的提高可能是由于更好的局部区域定位（见表4）。Oracle 方法在训练和测试时都使用真实的整体和局部标注。  \n",
    "\n",
    "第二组是在测试时未知鸟的标注框的结果。大多数的文章都没有展示在这个更真实、更复杂数据集上的效果。如表 1 所示，在此设置中，我们 part-based 的方法比基准 DPD 模型效果更好。我们在不进行微调的情况下获得 66.0％ 的分类精度，几乎与给出真实边界框时的精度一样好。这意味着在测试时不需要对任何目标进行标注就可以进行分类。通过微调 CNN 模型，我们的方法分类准确率达到 73.89％。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/1/4.jpg?raw=true)\n",
    "\n",
    "我们做的另一个有趣的实验是移除局部区域描述符仅使用预测框中的整体图像描述符。通过几何约束，我们的方法能够帮助局部区域定位。如表 2 所示，我们的方法优于使用 R-CNN 的单个对象检测器，这意味着几何约束有助于我们的方法更好地定位对象窗口。强 DPM 的检测不如我们的方法准确，这就解释了它性能下降的原因。oracle 方法使用真实边界框，并且达到了 57.94％ 的准确率，这仍然远低于表 1 中在对象和局部区域内使用两个图像描述符的方法。  \n",
    "\n",
    "### 4.2 Part localization  \n",
    "\n",
    "我们现在展示我们系统在定位局部区域上的效果。在表 4 中的结果是根据正确定位的区域分（PCP）度量的百分比给出的。对于第一组结果，给出了整个对象边界框，任务仅仅是定位边界框中局部区域的位置，与真实局部区域标注相交面积大于 0.5 的认为是正确的结果。  \n",
    "\n",
    "对于第二组结果，使用在第二部分中描述的目标函数在排名最高的局部区域预测上计算 PCP 度量。请注意，在这个更真实的设置中，我们不会假设在测试时就知道真实的目标标注框。  \n",
    "\n",
    "如表 4 所示，对于给定边界框和未知边界框的两种设置，我们的方法都优于强 DPM 方法。增加一个几何约束 $\\delta^{NP}$ 可以改善我们的结果。在全自动设置中，头部排名最高的检测和局部区域定位性能比基准方法好 65％。 $\\Delta_{null}= 1$是没有应用几何约束的仅外观情况。虽然细粒度的分类结果没有显示 Δgeometric 和 Δbox 之间的很大差距，但我们可以看到局部区域定位性能的差距。性能差距较小的原因可能在于深层卷积特征对于小的变化和旋转是不变的。  \n",
    "\n",
    "我们还评估了对边界框和局部区域 selective search region proposals 的 recall。表 3 列出了不同重叠阈值的召回结果。重叠要求为 0.5 时，鸟头部和身体部位的召回率很高，这为根据 region proposals 定位这些部件提供了基础。但是，我们还观察到，当重叠阈值为 0.7 时，头部召回率低于 40％，这表明自下而上的 region proposals 可能是精确局部定位的瓶颈。  \n",
    "\n",
    "其他可视化显示在图 4 中。我们显示了每个图像的三个检测和局部区域定位，第一列是强 DPM 的输出，第二列是我们的单个局部区域预测的方法，最后一列是我们使用局部优先的方法。我们使用预训练的模型来获得结果。我们还会在图 5 中显示我们方法的一些失败案例。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/1/5.jpg?raw=true)\n",
    "\n",
    "### 4.3 Component Analysis  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/1/6.jpg?raw=true)\n",
    "\n",
    "为了检验 $\\Delta_{geometric}$ 中使用的 $\\alpha$ 和 K 不同值的影响，我们进行交叉验证实验。结果显示在图 3 中，我们将图 3 中的 K = 20 固定，并将图 3 中 的 $\\alpha = 0.1$ 固定，所有关于以交叉验证方式训练数据的实验，我们将训练数据分成 5 次。如结果所示，端到端的细粒度分类结果对 $\\alpha$ 的选择非常敏感，并且 $\\alpha = 0.1$ 是没有任何几何约束的 $\\Delta_{box}$ 预测的情况。我们必须选择一个小 $\\alpha$ 的原因是高斯的 pdf 与我们的局部区域检测器输出的 logistic score 相比较大。另一方面，K 的选择不能太小，当 K 大于 10 时它的选择不是很敏感。  \n",
    "\n",
    "## 5. Conclusion  \n",
    "\n",
    "我们已经提出了一种能够进行最先进的细粒度目标识别的联合目标检测和区域定位系统。我们的方法学习探测器和区域模型，并强化区域之间和对象框架之间的几何约束。我们的实验结果表明，即使具有非常强大的特征表示和对象检测系统，通过区域来为具有高语义相似度的类别之间的细粒度区分的困难任务额外地建模对象的姿态也是非常有益的。在今后的工作中，我们将考虑在训练时联合建模对象类别及其各个部分和变形成本的方法。我们还计划探索弱监督设置，在该设置中，我们只会自动发现局部区域并将其模型化为仅来自对象边界框标注的潜在变量。最后，我们将考虑放宽对较小部分的 selective\n",
    "search，并采用密集窗口采样。\n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/1/7.jpg?raw=true)\n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/1/8.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained Image Recognition  \n",
    "\n",
    "\n",
    "## Abstract  \n",
    "\n",
    "细粒度的图像识别是一个具有挑战性的计算机视觉问题，因为由高度相似的从属类别引起的小类间变化，以及姿势，尺度和旋转引起的大的类内变化。本文中，我们对细粒度识别提出了一种新的没有全连接层的端到端 Mask-CNN 模型。基于细粒度图像的局部区域标注，所提出的模型由完全卷积网络组成，以定位可区分的部分（例如，头部和躯干），并且更重要的是生成用于选择有用和有意义的卷积描述符的目标/局部掩码。之后，建立一个 Mask-CNN 模型，用于同时聚合选定的对象和局部区域级描述符。与现有细粒度方法相比，所提出的 Mask-CNN 模型具有最少的参数，最低的特征维度和最高的识别准确度。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "细粒度的识别任务，例如识别鸟的种类，在计算机视觉中已经很流行。由于类别彼此相似，因此不同的类别只能通过轻微细微的差异来区分，这使得细粒度识别成为具有挑战性的问题。与一般的对象识别任务相比，细粒度识别更有益于学习对象的关键部分，这些部分这有助于区分不同的子类并对齐同一类的数据。  \n",
    "\n",
    "在深度学习时代，区域表示的直接方式是使用深度卷积特征/描述符。与全连接层的特征（即，整个图像）相比，卷积描述符包含更多的局部（即，局部区域）信息。另外，已知这些深度描述符对应于中级信息，例如目标局部区域。所有先前 part-based 的细粒度方法，都直接使用深卷积描述符并将它们编码成单一表示，而不评估获得的对象/局部区域深度描述符的有用性。通过使用强大的卷积神经网络，我们可能不需要在特征向量内选择有用的维度，就像我们对手工特征所做的那样。但是，由于大多数深度描述符对细粒度识别没有用处或意义，因此有必要选择有用的深度卷积描述符。最近，选择深度描述符对细粒度图像检索任务进行了阐述。而且，这对细粒度图像识别也是有益的。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/2/1.jpg?raw=true)\n",
    "\n",
    "在本文中，通过开发一种新颖的深度局部区域检测和描述符选择方案，我们提出了一种端到端的 Mask-CNN（M-CNN）模型，该模型丢弃全连接的层以进行细粒度识别。在训练期间，我们只需要局部区域标注和图像级标签。在 M-CNN 中，给定局部区域标注，我们首先将它们分成两个点集。一组对应于细粒鸟图像的头部部分，另一组对应于躯干。然后，将覆盖每个点集的最小凸多边形作为 ground-truth mask 返回，如图 1 所示。其他像素为背景。通过将局部定位视为三类分割任务，我们充分利用全卷积网络（FCN）在测试时生成 mask 并选择有效的深度描述符，在测试期间不使用任何标注。获得这两部分 mask 后，我们将它们组合起来形成对象。基于这些对象/局部区域 mask，构建了一个四流 four-stream Mask-CNN（图像，头部，躯干，对象），用于联合训练并同时聚合对象级和局部区域级特征。所提出的 four-stream M-CNN 的架构如图 2 所示。在每个 M-CNN 流中，我们丢弃全连接的 CNN 层。在最后的卷积层中，输入图像由多个深度描述符表示。为了选择有用的描述符以仅保留与对象相对应的描述符，使用 FCN 预先学习的对象/局部 mask。之后，每个流选定的描述符都被 averaged pooled 和 max pooled 成 512 维的向量，然后使用 $l_2$ 标准化。最后，将这四个流的特征向量连接起来，然后添加一个分类（fc + softmax）层进行端到端联合训练。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/2/2.jpg?raw=true)\n",
    "\n",
    "我们在 Caltech-UCSD Birds-200 2011 数据集上验证了提出的四流 M-CNN，我们实现了 85.5％ 的分类精度。也获得准确的局部区域定位（头部 84.62％和躯干 89.83％）。所提出的 M-CNN 模型的主要优点和贡献是：  \n",
    "\n",
    "- 据我们所知，Mask-CNN 是第一个选择深卷积描述符用于目标识别的端对端模型，特别是对于细粒度图像识别。  \n",
    "\n",
    "- 我们提出了一种用于细粒度识别的新颖且高效的 part-based four-stream 模型。丢弃全连接层，并且所提出的 M-CNN 在计算和存储方面是有效的。 与最先进的方法相比，M-CNN 的参数最小，特征维数最小（60.49M和8192-d）。同时，CUB200-2011 的分类准确率达到 85.4％，是现有方法中最高的。使用 SVD 白化方法，我们的特征表示可以压缩到 4,096-d，同时将精度提高到 85.5％。  \n",
    "\n",
    "- 该模型的局部区域定位性能优于其他需要额外标注框的 part-based 的细粒度方法。特别是，M-CNN 比用于头部定位的最新技术高出约 10％。  \n",
    "\n",
    "## 2. Related Work  \n",
    "\n",
    "细粒度识别是一个具有挑战性的问题，最近已成为一个热门话题。在过去的几年中，文献中已经发展了许多有效的细粒度识别方法。我们可以粗略地将这些方法分为三组。第一组试图通过开发用于分类细粒度图像的强大深度模型来学习更具辨别性的特征表示。第二组对细粒度图像汇总的目标进行矫正，以消除姿势变化和相机位置的影响。最后一组关注于 局部区域的表示，因为人们普遍认为细粒度图像之间的细微差别大多存在于对象局部区域的的独特属性中。  \n",
    "\n",
    "对于 part-based 的细粒度识别方法，有论文在训练期间使用鸟类的边界框和局部区域标注来学习准确的局部区域定位模型。然后，分别使用检测到的部分对不同的 CNN 进行微调。为了确保令人满意的定位结果，他们甚至在测试阶段使用了边界框。相比之下，我们的方法只需要在训练时使用局部区域标注，并且在测试过程中不需要任何标注。此外，我们的四流 M-CNN 是同时捕获对象和局部区域级信息的统一框架。其他一些 part-based 的方法被认为是一种弱监督的设置，在这种设置中，他们将细粒度图像仅分为图像级标签。正如我们的实验所显示的那样，M-CNN 的分类精确度明显高于这些弱监督方法。同时，M-CNN 的模型尺寸在所有最先进的方法中是最小的，这使得训练效率更高。  \n",
    "\n",
    "此外，还有基于分割的细粒度识别方法。它们与 M-CNN 最显著的区别在于：这些方法仅使用分割来定位整个对象或局部区域，而我们使用分割中的 mask 进一步选择有用的深度卷积描述符。其中，part-stacked 的 CNN 模型是与我们最相关的工作。part-stacked CNN 在训练中需要目标标注框和局部区域标注，甚至在测试期间也需要目标标注框。在边界框裁剪的图像块内，该方法将 15 个关键点中的每个关键点周围的图像作为 15 个分割前景类别，并使用FCN 解决 16 类分割任务。在获得训练的 FCN 之后，它将这些局部点位置定位在最后的卷积层中。然后，将对应于十五个区域的激活和整个对象堆叠在一起，全连接层被用于分类。与 part-stacked CNN 相比，M-CNN 只需要定位两个主要部分（头部和躯干），这使得分割问题更容易和更准确。如表 3 所示，M-CNN 实现了很高的定位精度。同时，part-stacked CNN 使用全部 15 个局部定位不会导致更好的分类准确率。此外，虽然我们在训练中使用较少的标注，并且在测试中不使用任何标注（参见4.2.2节），但 MUBN 的 CUB200-2011 的准确率比使用相同 baseline network 的 part-stacked CNN 高1.8％ 。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/2/3.jpg?raw=true)\n",
    "\n",
    "## 3. The Mask-CNN Model  \n",
    "\n",
    "在本节中，我们展示了提出的四流 Mask-CNN（M-CNN）模型。首先，我们采用全卷积网络（FCN）来生成用于定位对象/局部的对象/局部掩模，并且更重要的是选择深度描述符。然后，基于这些 mask，建立四流 M-CNN，用于联合训练并捕获对象和局部区域级别的信息。  \n",
    "\n",
    "### 3.1 Learning Object and Part Masks  \n",
    "\n",
    "全卷积网络（FCN）设计用于像素级别分类。FCN 可以以任何分辨率拍摄输入图像并产生相应尺寸的输出。在我们的方法中，我们使用 FCN 不仅将细粒度图像中的对象和局部区域进行定位，而且还将分割预测作为后面的描述符选择过程的 masks。  \n",
    "\n",
    "CUB200-2011 数据集中的每个细粒图像都对局部区域进行了标注，例如 15 个关键点。如图 1 所示，我们将这些关键点分成两组，包括头部关键点（即喙，额头，冠，左眼，右眼，颈背和喉咙）和躯干关键点（即背部，乳房，腹部，左腿，右腿，左翼，颈背，右翼，尾巴和喉咙）。基于关键点，生成了两个局部标注 mask。一个是头部 mask，它对应于覆盖所有头部关键点的最小凸多边形。另一个是躯干 mask，它是覆盖躯干关键点的最小凸多边形。在图 1中，红色多边形是头部 mask，蓝色是躯干 mask。图像的其余部分是背景。因此，我们将 part mask 学习过程建模为三类分割问题。为了进行有效的训练，所有训练和测试的细粒度图像均使用其原始分辨率。然后，我们在原始图像的中间裁剪一个 384×384 的图像块作为输入。mask 学习网络结构如图 3 所示。在我们的实验中，我们采用了 FCN-8s 来学习和预测 part masks。  \n",
    "\n",
    "在 FCN 推断期间，不使用任何标注，为每个图像返回三个类别热图（与原始输入图像大小相同）。我们随机选择一些预测的 part masks，并在图 4 中显示它们。在这些图中，学习的 mask 覆盖在原始图像上。头部以红色突出显示，躯干呈蓝色。预测的背景像素为黑色。从这些图中可以看出，即使真实标注的局部 mask 不是非常准确，学习的 FCN 模型也能够返回更准确的 part mask。同时，这些局部 mask 也可以定位局部位置。局部定位和对象分割的定量结果将在后面章节报告。  \n",
    "\n",
    "如果两局部 mask 都精确预测，将有利于后面的深度描述符选择过程和最终的细粒度分类。 因此，在训练和测试期间，我们将使用 M-CNN 预测的 masks 进行局部定位和描述符的选择。我们还将两个 mask 组合在一起，为整个对象形成一个 mask，称之为对象 mask。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/2/4.jpg?raw=true)\n",
    "\n",
    "### 3.2 Training Mask-CNN  \n",
    "\n",
    "在获得对象和局部 mask 模后，我们构建了四流 M-CNN 进行联合训练。该模型的整体结构如图 2 所示。我们以整个图像流为例来说明 M-CNN 中每个流的流水线。  \n",
    "\n",
    "整个图像流的输入是用 h×h 调整大小的原始图像。在我们的实验中，我们分别报告了 h = 224 和 h = 448 的结果。输入图像被馈送到传统的卷积神经网络中，但全连接层被丢弃。也就是说，我们提出的 M-CNN 中使用的 CNN 模型仅包含卷积，ReLU和汇聚层，这大大降低了 M-CNN 模型的大小。具体而言，我们使用 VGG-16 作为基准模型，并保留 pool5 之前的层（包括pool5）。如果输入图像为 224×224，我们在 pool5 中获得 7×7×512 激活张量。因此，我们有 49 个 512-d 的深度卷积描述符，它们也对应于输入图像中的 7×7 个空间位置。然后，学习对象 mask（参见3.1节）首先通过最近的插值重新调整为7×7，然后用于选择有用和有意义的深度描述符。如图 2（c）和（d）所示，描述符在位于目标区域时应保持不变。如果它位于背景区域中，则该描述符将被丢弃。在我们的实现中，mask 设置为二进制矩阵，其中 1 表示保留，0 表示丢弃。我们将选择过程作为卷积激活张量与 mask 矩阵之间的元素内积作来实现，这与 FCN 中的元素相加操作相似。因此，位于目标区域中的描述符将保留，而其他描述符将成为零向量。  \n",
    "\n",
    "对于这些选定的描述符，在端到端的 M-CNN 学习过程中，我们将它们分别平均和最大化为两个 512-d 特征向量。然后，对他们中的每一个都进行 $l_2$ 归一化。之后，我们将它们连接成 1024-d 特征作为整个图像流的最终表示。  \n",
    "\n",
    "用于头部和躯干的流与整个图像具有相似的处理步骤。然而，与整个图像流的输入不同，我们如下生成头部和躯干流的输入图像。在获得两部分 mask（即头部和躯干掩模）之后，我们使用 part masks 作为 part detectors 来定位输入图像中的头部部分和躯干部分。对于每个 part，我们返回包含 part masks 区域的最小矩形边界框。基于矩形边界框，我们裁剪作为 part 流输入的图像补丁。图 2 中间显示了 M-CNN 的头部和躯干流。最后一个流是对象流，它通过将两部分掩码组合到对象掩码中来裁剪图像补丁。因此，它的输入是我们的 FCN 分割网络检测到的主要对象（即，鸟）。在我们的实验中，这三个流的输入都调整为224×224。  \n",
    "\n",
    "在图 2（f）所示的分类步骤中，最终的 4096d 图像表示是整个图像，头部，躯干和对象特征的拼接。M-CNN 的最后一层是用于 CUB200-2011 数据集分类的 200 路分类（fc + softmax）层。 四个流 M-CNN 是端到端学习的，同时学习四个 CNN 的参数。在训练 M-CNN 期间，所学习的 FCN 分割网络的参数是固定的。  \n",
    "\n",
    "## 4 Experiments  \n",
    "\n",
    "在本节中，我们首先描述实验设置和实现细节。然后，我们报告分类的准确性，并对所提议的 M-CNN 模型进行讨论。最后，还将提供 part 定位和对象分割的性能。  \n",
    "\n",
    "### 4.1 Dataset and Implementation Details  \n",
    "\n",
    "是在广泛使用的细粒度基准鸟类数据集 Caltech-UCSD 2011 上进行的评估的。该数据集包含 200 种鸟类，每个类别包含大约 30 个训练图像。在训练阶段，采用 15 个局部标注生成 part masks，同时将图像级标签用于端到端的 M-CNN 联合训练。测试时，我们不需要监督信号（例如，局部标注或边界框）。  \n",
    "\n",
    "所提出的 Mask-CNN 模型和用于生成掩码的 FCN 使用开源库 MatConvNet 来实现。在我们的实验中，在获得学习到的 mask 后，我们首先生成鸟头，躯干和物体的图像块，如第二节所述。然后，为了促进四流 CNN 的收敛，对应于整个图像，头部，躯干和物体的每个单独的流在其输入图像上被分别微调。每个流中使用的 CNN 由在 ImageNet 上预先训练过的 VGG-16 模型初始化。另外，我们通过对所有四个流进行水平翻转来使训练数据翻倍。如图 2 所示，在对每个流进行微调之后，执行四流 M-CNN 的联合训练。在 M-CNN 中不使用 dropout。在测试时，我们对图像及其翻转副本的预测进行平均，并输出具有最高分数的类作为测试图像的预测。此外，与逻辑回归（LR）相比，直接使用 softmax 预测结果的准确性略有下降，这与之前中的观察结果一致。因此，在下文中，所报告的 M-CNN 结果全部是用提取的特征（4096-d）与默认超参数 $C_LR = 1$ 基于 one-vs-all logistic regression 实现的。  \n",
    "\n",
    "### 4.2 Classification Accuracy and Comparisons  \n",
    "\n",
    "我们报告了提出的四流 M-CNN 模型在 CUB200-2011 数据集上的分类准确率，并与文献中的基线方法和最新方法进行比较。  \n",
    "\n",
    "#### 4.2.1 Baseline Methods  \n",
    "\n",
    "为了验证 M-CNN 中描述符选择过程的有效性，我们执行了基于所提出的四流体系结构的两种基线方法。与我们的 M-CNN 不同，这两个基线方法不包含描述符选择部分，即图 2（d）所示的处理。  \n",
    "\n",
    "第一种基线方法使用传统的全连接层来对每个流进行分类，其被称为“4 流 FC”。 在“4 流 FC”中，我们用含有全连接层的 CNN（即，仅移除了 fc8 的VGG-16）代替图 2 中每个流的（b）到（e）部分。 因此，每个流的最后一层中生成的特征是一个 4096-d 单个矢量。 其余过程也是将 4 个 4,096d 特征连接到最后一个特征 16,384-d，并在 16,384d 图像表示上学习 200 路分类（fc + softmax）层。  \n",
    "\n",
    "第二个基线与提议的 M-CNN 类似。最显着的区别是它丢弃了描述符选择部分，即图 2（d）中的处理。因此，每个流中 pool5 的卷积深度描述符都是直接平均和最大值汇集，然后分别进行 $l_2$ 归一化。因此，我们称之为“4-stream Pooling”。其余程序与提出的 M-CNN 相同。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/2/5.jpg?raw=true)\n",
    "\n",
    "#### 4.2.2 Comparisons with state-of-the-art methods  \n",
    "\n",
    "表 2 列出了提议的四流 M-CNN 和 CUB200-2011 最先进方法的分类准确率。为了公平比较，我们只报告在测试中不使用局部标注的结果。  \n",
    "\n",
    "如前所述，当所有输入尺寸为 224×224 时，所提出的四流 M-CNN 模型的准确率为 83.1％。之后，我们将整个图像流的输入图像更改为 448×448 像素，这将分类性能提高了 2.1％。我们还将对象流的输入图像大小调整为 448×448。但精度略低于以前。  \n",
    "\n",
    "此外，由于多层的集合可以提高最终性能，所以在联合训练之后，我们从 pool5 前面三层的 relu5_2 层中提取深层描述符。然后，预测的 part mask 也用于选择四个流的相应描述符。完成与 pool5 相同拼接过程，我们可以获得 relu5_2 的另一个 4,096-d 图像表示形式。之后，我们将它与 pool5 结合成一个 8,192-d 特征向量（表2中称为“4-stream M-CNN +”），在 CUB200-2011 上实现了 85.4％ 的最佳分类精度。 另外，我们通过 SVD 白化将 8,192-d 特征向量压缩到 4,096。它可以降低维度，同时将精度提高到 85.5％。  \n",
    "\n",
    "特别的，由于 part-stacked CNN 使用了 Alex-Net 模型，我们还基于 Alex-Net 构建了另一个四流 M-CNN。我们的四流 M-CNN（Alex-Net）的准确率为78.0％。它比 part-stacked CNN 高1.8％。此外，在基于 Alex-Net 的四流 M-CNN 中，参数数量仅为 9.74M，最终的特征向量仅为 2,048 维。  \n",
    "\n",
    "### 4.3 Part Localization Results  \n",
    "\n",
    "除了在第二节中显示的定位结果外。在本节中，我们使用 Percentage of Correctly Localized Parts（PCP）度量来定量评估定位的准确性。如表 3 中所报告的，度量指标是定位的部分（即头部和躯干）与标准标注IOU超过 50% 的百分比。  \n",
    "\n",
    "通过比较躯干 PCP 的结果，我们的方法大大优于 part-based R-CNN 和强 DPM。但是，由于我们在测试中不使用任何监督，所以定位性能低于在测试期间使用边界框的 Deep LAC。另外，对于比躯干更具挑战性的头部定位任务，尽管我们的方法仅使用了局部注释来训练，但头部定位性能（84.62％）仍然明显高于其他方法。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/2/6.jpg?raw=true)\n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/classification/Fine-grained/2/7.jpg?raw=true)\n",
    "\n",
    "### 4.4 Object Segmentation Performance  \n",
    "\n",
    "由于 CUB200-2011 数据集也提供了对象分割标注，因此我们可以直接在分割上测试学习到的 mask。图 5 显示了定性分割结果。我们基于 FCN 的方法通常能够很好地对前景物体进行分割，但可以理解的是，分割更精细的鸟类部分，例如爪和喙是很麻烦的。由于我们的目标不是分割对象，因此我们不会将其作为预处理或后处理进行细化。此外，我们通过真实前景对象与预测对象 mask 的常见语义分割度量均值 IU（像素精度和区域相交联合）来定量评估分割性能。测试集合为 72.41％。实际上，更好的分割结果将导致更好的预测对象/局部 mask，并且也有利于最终的分类。为了进一步提高分类精度，一些预处理方法，例如 GrabCut，值得试着去获得比图 3（c）中的凸多边形更好的 mask。  \n",
    "\n",
    "## 5 Conclusion  \n",
    "\n",
    "在本文中，我们介绍了在目标识别中选择深度卷积描述符的好处，特别是细粒度图像识别。通过开发描述符选择方案，我们提出了一种不具有全连接层的新型端到端 Mask-CNN（M-CNN）模型，不仅能够精确地定位对象/局部，还能生成用于选择深读卷积的对象/局部 mask 描述。在聚合选定的描述符后，对象级和部分级特征由所提出的四流 M-CNN 模型编码。Mask-CNN 不仅在 CUB200-2011 上获得了 85.5％ 的分类精度，而且参数参数最少，维度特征表示最低。  \n",
    "\n",
    "今后，我们计划在弱监督环境中解决 M-CNN 的部分检测问题，其中我们只需要图像级标签。因此，要达到可比较的分类精确度，需要少得多的标注工作量。另外，另一个有趣的方向是探索用于一般对象分类的描述符选择的好处。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Application of Two-level Attention Models in Deep Convolutional Neural Network for Fine-grained Image Classification  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "细粒度分类是具有挑战性的，因为类别只能通过细微局部的差异来区分，位置，大小或者旋转都会使问题变得更加困难。大多数细粒度分类系统遵循寻找前景对象或对象局部区域来提取判别特征。在本文中，我们建议将视觉注意力应用于使用深度神经网络的细粒度分类任务。我们集成了三种注意力模型：提出候选 patches 的自下而上的注意力，选择特定目标相关 patches 的目标级别自顶向下的注意力，定位可区分局部区域的局部级别自顶向下的注意力。我们将这些注意力结合起来，对特定领域的深层网络进行训练。重要的是，我们避免使用昂贵的标注，如边界框或局部信息标注。弱监督约束使我们的工作更容易推广。我们已经验证了该方法对 ILSVRC2012 数据集和 CUB200 2011 数据集子集的有效性。我们的 pipeline 在最弱的监督条件下进行了重大改进并取得了最佳的准确性。该性能与其他依赖附加标注的方法相比具有竞争性。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "细粒度分类是指在一些基本层次类别下的次分类问题，例如，对不同的鸟类，狗品种，花种，飞机模型等进行分类。这是一个具有广泛应用的重要问题。 即使在 ILSVRC2012 1K 类别中，犬类和鸟类下分别有 118 个和 59 个子类。直观地，如图 1 所示类内差异可能大于类间差异。因此，细粒度分类在技术上具有挑战性。  \n",
    "\n",
    "具体而言，细粒度分类的困难来源于可区分特征不仅仅是前景中的物体了，更应该是物体的局部信息，比如说鸟的头部。因此，大多数细粒度分类系统都遵循以下流程：找到前景对象或对象局部区域以提取可区分特征。  \n",
    "\n",
    "为了这个工作，自下而上的过程是不可避免的，我们需要提出很多图像区域作为物体候选者，或者这些区域包含那些具有判别力的部分。selective search 是一个无监督的过程，可以提出数千个这样的区域。这个方法在最近的研究中被广泛使用，我们也采用了这一点。  \n",
    "\n",
    "自下而上的过程具有较高的召回率，但精度很低。如果对象相对较小，则大多数 patch 都是背景，对对象分类没有帮助。这给 pipeline 中的定位部分带来了问题，导致需要自顶向下的注意力模型来过滤掉嘈杂的 patch 并选择相关的 patch。在细粒度分类的背景下，查找前景对象和对象局部区域可以被看作是一个两层的注意力过程，一个在对象层面，另一个在局部区域层面。  \n",
    "\n",
    "现有大多数的方法依靠强大的监督来处理注意力问题。他们严重依赖于人为的标签，使用边界框来表示对象级别和局部级别的区域标注。最强大的监督设置既可以在训练中也可以在测试阶段使用，而最弱的设置在训练和测试中都不使用这两种标注。大部分工作介于两者之间。  \n",
    "\n",
    "由于标注价格昂贵且不可扩展，因此本研究的重点是使用最弱的监督。认识到粒度差异，我们采用两条独立的 pipeline 来实现对象级和部分级的注意力，但共享组件。以下是我们方法的总结：  \n",
    "\n",
    "- 我们将在 ILSVRC2012 1K 预先训练的卷积神经网络转变成一个 FilterNet。FilterNet 可以选择跟基准类别很相近的 patches，因此可以处理 object-level attention。选定的 patches 用来训练另外一个 CNN，训练成一个 domain 分类器，称为 DomainNet。  \n",
    "\n",
    "- 经验上，我们观察到了在 DomainNet 中隐藏的聚类模式。多组神经元对可区分部分表现出高度敏感性。因此，我们选择相应的滤波器作为部分检测器来实现 part-level attention。  \n",
    "\n",
    "在这两个步骤中，我们只需要图像级标签。  \n",
    "\n",
    "接下来的关键步骤是从这两个注意力模型选择的区域中提取可区分性特征。最近，有令人信服的证据表明，由 CNN 产生的特征可以提供超过手工特征的优越性能。遵循上面概述的两个注意力 pipeline，我们采用相同的策略。在对象级别上，DomainNet 直接输出由图像的多个相关 patch 驱动的 multi-view 预测。在 part-level 上，由被检测部件驱动的 CNN 隐藏层的激活通过基于部件的分类器产生另一个预测。以利用两级注意力的优势，最终的分类合并了两条 pipeline 的结果。  \n",
    "\n",
    "我们的初步结果证明了这种设计的有效性。使用最弱监督，我们改进了 ILSVRC2012 数据集中犬类和鸟类的细粒度分类，错误率从 40.1% 和 21.1% 降到了 28.1% 和 11%。在 CUB200-2011 数据集中，我们达到了 69.7％ 的准确性，与其他使用更强监督的方法相比具有竞争力。我们的技术通过更好的网络会有改善，例如使用 VGGNet 的精度达到近 78％。  \n",
    "\n",
    "本文的其余部分安排如下。我们首先在第 2 节中描述利用对象级和部分级注意力进行细粒度分类的 pipeline。详细的性能研究和分析将在第 3 节中介绍。相关工作在第 4 节中介绍。最后，讨论我们学到的东西和未来的工作。  \n",
    "\n",
    "## 2. Methods  \n",
    "\n",
    "我们的设计基于一个非常简单的直觉：执行细粒度分类首先需要“看到”物体，然后才能看到物体中最具有区别性的部分。在图像中寻找奇瓦瓦狗需要首先看到一只狗，然后专注于它的重要特征，以区别于其他品种的狗。  \n",
    "\n",
    "为了实现这一点，我们的分类器不应该在原始图像上工作，而应该在指定的 patch 上工作。这些 patch 还应该保留与识别步骤相关的可区分行。在上面的例子中，第一步的目标是在狗类分类阶段，第二阶段的目标是在奇瓦瓦狗与其他品种（例如耳朵，头部，尾巴）之间进行区分的部分。至关重要的是，认识到详细标签价格昂贵且难以扩展的事实，我们选择使用最薄弱的标签。具体来说，我们的 pipeline 只使用图像级标签。  \n",
    "\n",
    "原始候选 patch 是在自底向上的过程中生成的，将像素分组为几个区域，突出显示某些对象局部区域。在这个过程中，我们采用 selective search 从输入图像中提取 patch。这一步将提供原始图像的多尺度和多角度图像。但是，自下而上的方法将提供高召回率但低精度的 patch。需要应用自顶向下的注意力来选择对分类有用的相关 patch。  \n",
    "\n",
    "### 2.1. Object-Level Attention Model  \n",
    "\n",
    "使用对象级关注的修补程序选择此步骤通过自上而下的对象级关注过滤自下而上的原始修补程序。 目标是消除与对象无关的噪声补丁，这对训练分类器十分重要[13]。 我们通过将在1K级ILSVR2012数据集上训练的CNN转换为对象级FilterNet来实现这一点。 我们将属于细粒度类别（例如奇瓦瓦州父母班级是狗）的所有softmax神经元的激活总结为选择置信度分数，然后在分数上设置阈值以决定是否给定 应该选择补丁。 这在图2中示出。通过这种方式，多尺度和多视图的优势已被保留，并且噪声也被滤除了。  \n",
    "\n",
    "培训DomainNet由FilterNet选择的补丁用于在适当变形后从头开始训练新的CNN。 我们将这个第二CNN称为DomainNet，因为它提取了与属于特定域（例如，狗，猫，鸟）类别相关的特征。  \n",
    "\n",
    "我们注意到，从单个图像中可以得到很多这样的补丁，并且净效应是数据增强的推动。 与其他数据增强（如随机裁剪）不同，我们对补丁有相关性有更高的信心。 数据量也推动了对更大网络的培训，使其能够构建更多功能。 这有两个好处。 首先，DomainNet本身就是一个很好的细粒度分类器。 其次，它的内部特征现在允许我们构建部分探测器，我们将在下面解释。  \n",
    "\n",
    "使用对象级关注进行分类使用对象级关注的修补选择可以自然地应用于测试阶段。 为了获得图像的预测标签，我们向DomainNet提供由FilterNet选择的补丁来前馈。 然后计算所有补丁的softmax输出的平均分类分布。 最后，我们可以得到平均softmax分布的预测。  \n",
    "\n",
    "该方法包含超参数置信度阈值，它会影响选定补丁的质量和数量。 在实验中，我们将其设置为0.9，因为此值提供了最佳的验证准确度和可容忍的训练时间。  \n",
    "\n",
    "### 2.2. Part-Level Attention Model  \n",
    "\n",
    "构建部分检测器DPD [27]和Part-RCNN [26]的工作强烈表明，某些区分性局部特征（例如头部和身体）对于细粒度分类至关重要。 正如许多相关着作[27,26,4]所做的那样，我们不是在部件和关键点上使用强标签，而是从DomainNet的隐藏层显示聚类模式这一事实中受到启发。 例如，有多组神经元对鸟头作出反应，其他人则对鸟体作出反应，尽管它们可能对应于不同的姿势。 事后看来，这并不令人意外，因为这些功能确实“脱颖而出”和“代言”一个类别  \n",
    "\n",
    "图3从概念上显示了此步骤的功能。 本质上，我们对相似度矩阵S进行谱聚类，将中间层的滤波器划分为k个组，其中S（i，j）表示DomainNet中两个中间层滤波器Fi和Fj的权重的余弦相似度。 在我们的实验中，我们的网络与AlexNet基本相同[12]，我们从第四卷积层挑选神经元，其中k设为3.每个簇都作为部分探测器。  \n",
    "\n",
    "使用群集筛选器检测区域提案中的部件时，步骤如下：1）在patch4筛选器的输入图像上将补丁提案转换为接受字段大小。 2）将补丁前馈到conv4，以便为每个过滤器生成激活分数。 3）总结一个集群中的过滤器得分，得到集群评分。 4）选择每个集群中集群评分最高的修补程序作为部分修补程序。  \n",
    "\n",
    "图4显示了一些狗和鸟类的检测结果。很明显，DomainNet中的一组过滤器特别注意鸟头，另一组过滤鸟体。 同样，对于DomainNet来说，一组过滤器会关注狗头，另一个过滤狗腿。  \n",
    "\n",
    "构建基于零件的分类器然后将由零件检测器选择的补丁包回到DomainNet的输入大小以生成激活。 我们连接不同部分的激活和原始图像，然后训练SVM作为基于部分的分类器。  \n",
    "\n",
    "该方法包含几个超参数，例如 检测过滤器图层：conv4，聚类编号：3.我们遵循标准实践，并保留用于网格搜索的10％训练数据的验证集合以确定这些数字。 我们发现conv4比conv3或conv5更好，而设置k> 3并没有带来更好的准确性。 为了验证每个部分的效果，我们一次一个地修剪每个集群的特征。 我们注意到一个集群不可避免地引入了负面影响，因此在训练分类器时我们不使用该部分的特征; 视觉检查表明，该群集是噪音模式的过滤器聚集的地方。 这些选择可以根据数据集进行更改。  \n",
    "\n",
    "### 2.3. The Complete Pipeline  \n",
    "\n",
    "DomainNet分类器和基于部分的分类器都是细粒度分类器。 然而，他们的功能和实力不同，主要是因为他们承认不同性质的补丁。 使用选择性搜索的自下而上过程是未加工的补丁。 从它们中，FilterNet选择多个视图（希望）将焦点集中在整个对象上; 这些补丁驱动DomainNet。 另一方面，基于部分的分类器选择并专门处理包含区分和局部特征的补丁。 尽管两个分类器都允许使用一些补丁，但它们的特征在每个分类中都有不同的表示形式，并且可能会相互丰富。 最后，我们将两级关注方法的预测结果合并，以利用以下公式得出两者的优势：  \n",
    "\n",
    "其中对象得分是由对象关注选择的补丁平均的softmax值，部分得分是由支持向量机使用连接零件特征产生的决策值，并且使用验证方法选择α。 在实验中，我们将α设为0.5。 选择具有最高评分的班级作为预测结果。  \n",
    "\n",
    "图5显示了完整的流水线以及合并两级关注分类器的结果。  \n",
    "\n",
    "## 3. Experiment  \n",
    "\n",
    "本节介绍性能评估和我们提出的方法在三个细粒度分类任务中的分析：  \n",
    "\n",
    "ILSVRC2012中的两个子集，狗数据集（ILSVRC2012 Dog）和鸟类数据集（ILSVRC2012 Bird）的分类。 第一张包含118个品种的狗的153,773幅图像，第二张包含59种鸟类的79,491张图像。 列车/测试分组遵循ILSVRC2012的标准协议。 两个数据集都被弱注释，只有类标签可用。  \n",
    "\n",
    "广泛使用的细粒度分类基准Caltech-UCSD Birds数据集[21]（CUB200-2011），包含200种鸟类的11,788幅图像。 CUB200-2011中的每个图像都有详细的注释，包括图像级标签，边界框和部分地标。  \n",
    "\n",
    "### 3.1. Implementation Details  \n",
    "\n",
    "我们的CNN架构与流行的AlexNet等人基本相同。 [12]，有5个卷积层和3个完全连接的层。 它用于所有实验，除了输出层的神经元数量在需要时被设置为类别数量。 为了公平比较，我们尝试在同一网络体系结构上重现其他方法的结果。 当使用CNN作为特征提取器时，第一完全连接层的激活作为特征输出。 最后，为了证明我们的方法对于网络体系结构是不可知的，并且可以通过它改进，我们也尝试在特征提取阶段使用更新的VGGNet [18]。 由于时间限制，我们没有复制使用VGGNet的所有结果  \n",
    "\n",
    "ILSVRC 1K的Bird和Dog子集用于训练DomainNet，2011年的CUB200用于微调DomainNet Bird。 所有用于训练的图像都使用对象级关注方法进行增强。  \n",
    "\n",
    "### 3.2. Results on ILSVRC2012 Dog/Bird  \n",
    "\n",
    "在此任务中，只有图像级别的类标签可用。 因此，需要详细注释的细粒度方法不适用。 为简洁起见，我们只会报告狗的结果; 鸟的结果在性质上是相似的。  \n",
    "\n",
    "基线是CNN的表现，但有两种不同的策略训练，包括：  \n",
    "\n",
    "•CNN域：只对来自狗类的图像训练网络。 在训练阶段，从整幅图像中随机剪裁227×227块，以避免过度拟合。 在测试阶段，将10个固定视图（中心色块，四个角色色块和它们的水平反射）的softmax输出平均为最终预测。 在这种方法中，没有使用特别的注意并且同样选择了补丁。  \n",
    "\n",
    "•CNN 1K：网络在ILSVRC2012 1K类别的所有图像上进行训练，然后删除不属于狗的softmax神经元。 其他设置与上述相同。 这是一个多任务学习方法，可以同时学习所有模型，包括狗和鸟。 这种策略利用更多的数据来训练单个CNN，并且更好地抵制过度配合，但是在不需要的类别上浪费了容量。  \n",
    "\n",
    "这些基准数字与我们的方法的三种策略进行比较：仅使用对象级别和部分级别的注意，以及两者的组合。 选择性搜索提出了数百个补丁，我们让FilterNet选择其中的大约40个，使用0.9的置信度得分。  \n",
    "\n",
    "表1总结了所有五种策略的前1个错误率。 事实证明，两条基线的表现基本相同。 但是，我们的注意力方法可以实现更低的错误率。 使用对象级关注仅将错误率降低9.3％，与随机裁剪补丁训练的CNN相比较。 这清楚地表明了对象级关注的有效性：现在，DomainNet专注于从前景对象学习领域特定的功能。 结合部分层面的关注，错误率下降到28.1％，明显好于基线。 单独使用部分级别关注的结果不如对象级关注，因为部分级别还存在更多模糊性。 然而，它实现姿态规范化以抵抗大姿态变化，这与对象级关注是互补的。  \n",
    "\n",
    "### 3.3. Results on CUB200-2011  \n",
    "\n",
    "对于这项任务，我们首先演示基于对象级关注的学习深度特征的性能优势。 然后，我们针对其他最先进的方法展示完整结果。  \n",
    "\n",
    "学习深度特征的优势我们已经证明，受到对象级关注培训的鸟类DomainNet可以在ILSVRC2012 Bird上提供出色的分类性能。假设部分收益来自更好的学习功能是合理的。在这个实验中，我们使用DomainNet作为CUB200- 2011上的特征提取器来验证这些特征的优势。我们与两个基线特征提取器进行比较，一个是手工制作的核心描述符[3]（KDES），它在使用CNN特征之前被广泛用于细粒度分类，另一个是CNN特征提取器从所有数据中预训练ILSVRC2012 [16]。我们比较了两种分类管线下的特征提取器。第一个使用边界框，第二个是在Zhang等人提出的。 [27]（DPD）依靠基于可变形部分的检测器[8]来查找对象及其部分。在这两条管线中，特征都是在SVM分类器中输入的。在这个实验中，CUB200-2011没有对CNN进行微调。如图6所示，基于DomainNet的特征提取器可以在两条管道上实现最佳结果。这进一步表明，使用对象级别的注意过滤相关的补丁是CNN学习好的功能的重要条件。  \n",
    "\n",
    "分类管道的优势在本实验中，使用CUB200-2011对DomainNet进行了微调，并使用由对象级关注产生的补丁。 表2中报告了这些情况以及使用了多少注释。 这些方法分为三组。 第一组是我们基于注意力的方法，第二组使用与第一组相同的DomainNet特征提取器，但使用不同的流水线和注释，第三组包括最近文献中的最新结果。 由于训练数据的数量有限，第二和第三组中的大多数比较方法使用SVM作为分类器，例如， BBox + DomainNet，DPD，部分RCNN。 这些方法的区别在于提取特征的位置。  \n",
    "\n",
    "我们首先比较使用特征提取器相同的前两组结果，并将性能差异归因于不同的关注模型。 使用原始图像只能达到最低的准确度（58.8％）。 这表明了细粒度图像分类中对象和部分关注的重要性。 相比之下，我们的注意力为基础的方法取得了显着的改善，两级注意比使用人标签包围盒（69.7％比68.4％）提供了更好的结果，并且与DPD（70.5％）相当。 DPD结果基于使用我们的特征提取器的实现，它使用可变形的基于部件的检测器，用对象边界框进行训练。 标准DPD流水线在测试时也需要边界框来产生相对较好的性能。 就我们所知，69.7％是最弱监管下的最佳结果。  \n",
    "\n",
    "第三套总结了最先进的方法。 我们的结果比仅使用边界框进行训练和测试的结果要好得多，但与使用部分级注释的方法仍有差距。  \n",
    "\n",
    "我们的结果可以通过使用更强大的特征提取器来改进。 如果我们使用VGGNet [18]来提取特征，那么仅仅使用原始图像而没有关注的基线方法可以提高到72.1％。 增加对象级别的关注，部分级别的关注以及综合关注将性能分别提升至76.9％，76.4％和77.9％。  \n",
    "\n",
    "## 4. Related Work  \n",
    "\n",
    "细粒度分类最近被广泛研究[21,22,11,3,5,24,27,2,4]。 以前的作品旨在从三个主要方面提高识别精度：1.对象和部分定位，也可以将其视为对象/部分关注; 2.检测到的物体或部件的特征表示; 3.人在循环[20]。 由于我们的目标是自动细粒度分类，因此我们重点关注前两项的相关工作  \n",
    "\n",
    "### 4.1. Object/Part Level Attention  \n",
    "\n",
    "在细粒度分类任务中，判别特征主要局限于前景对象，甚至对象局部，这使得对象和部分关注成为第一个重要步骤。 由于细粒度分类数据集通常使用边界框和部分界标的详细注释，因此大多数方法都依赖其中一些注释来实现对象或部分级别的关注。  \n",
    "\n",
    "最强有力的监督设置是在训练和测试阶段使用边界框和部分地标，这通常用于测试性能上升[2]。 为了在细粒度任务上验证CNN特征，在训练和测试阶段假定边界框[7,16]。 使用提供的边界框，提出了几种以无监督或潜在方式学习部分检测器的方法[23,5]。 为了进一步提高性能，零件级别标注还用于训练阶段，以学习强监督的可变形零件模型[1,27]或直接用于微调预先训练的CNN [4]。  \n",
    "\n",
    "我们的工作也与最近提出的基于CNN特征的目标检测方法（R-CNN）密切相关[10]。 R-CNN首先通过一些自下而上的关注模型为每幅图像提出数千个候选包围盒[19，6]，然后选择分类得分高的包围盒作为检测结果。 基于R-CNN，Zhang等人 已经提出了Part-based R-CNN [26]利用深度卷积网络进行部分检测。  \n",
    "\n",
    "### 4.2. Feature Representation  \n",
    "\n",
    "直接提高准确度的另一方面是引入更具辨别性的特征来表示图像区域。 Ren等人 已经提出了内核描述符[3]，并被广泛用于细粒度分类管道中[27,23]。 Berg等人最近的一些作品尝试从数据中学习特征描述。 已经提出了基于部分的一对一特征库POOF [2]作为中级特征。 从ImageNet数据中预先训练的CNN特征提取器在细粒度数据集上也显示出显着的性能改善[16,7]。 Zhang等人 通过对细粒度数据集进行微调来进一步改善了CNN特征提取器的性能[26]。  \n",
    "\n",
    "我们的方法采用相同的一般原则。 我们也分享了在自下而上的过程中采用区域提案的策略，以推动分类管道，就像R-CNN和R-CNN一样。 一个区别是我们使用提供多个视图和缩放的相关修补程序丰富了对象级管道。 更重要的是，我们选择整个模型中最弱的监督，仅依靠CNN功能来实现注意力，检测部件和提取功能。  \n",
    "\n",
    "## 5. Conclusions  \n",
    "\n",
    "在本文中，我们提出了一种结合了自下而上和自上而下关注的细粒度分类流水线。对象级别的关注为网络提供了与具有不同视图和缩放比例的任务域相关的修补程序。这导致更好的CNN特征用于细粒度分类，因为网络由与移位/比例差异相关的域相关补丁驱动。部分级别的注意力集中在局部区分模式上，并且也实现了姿态规范化。这两种关注程度都可以带来显着的收益，并且它们可以很好地与后期融合相互弥补。我们的方法的一个重要优点是，注意力来源于用分类任务训练的CNN，因此它可以在只提供类别标签的最弱监督环境下进行。这与其他最先进的方法形成鲜明对比，这些方法需要对象边界框或部分地标进行训练或测试。据我们所知，我们在最弱的监管环境下获得了CUB200-2011数据集的最佳准确性  \n",
    "\n",
    "这些结果是有希望的。 同时，经验指出了一些教训和未来方向，我们总结如下：  \n",
    "\n",
    "处理部分级别关注中的含糊问题。 我们目前的方法没有充分利用在CNN学到的东西。 由于尺寸问题，零件特征可能出现在不同的图层中，因此应将不同图层的滤镜视为一个整体，以促进强大的零件检测。  \n",
    "\n",
    "•更紧密地整合对象层面和部分层面的关注。 对象级注意的一个优点是它可以提供大量的相关补丁以在一定程度上抵御变化。 但是，目前的部分级别关注流程并未利用这一点。 我们可以借用多贴片测试的思想来实现部分关注方法，从而导出更有效的姿态归一化  \n",
    "\n",
    "我们正在积极追求上述方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1407.3867.pdf  \n",
    "\n",
    "https://arxiv.org/pdf/1605.06878.pdf\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/24738319?utm_campaign=rss&utm_content=title&utm_medium=rss&utm_source=rss"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
