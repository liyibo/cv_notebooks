{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Neural Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Style 每次转换都需要指定风格图片和内容图片，最小化content loss 和 style loss，重新训练一个模型，每张图片的生成都需要花费很长的时间，所以就有了Fast Neural Style 方法的提出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 摘要  \n",
    "\n",
    "1 使用VGG16作为特征提取网络，用提取到的特征计算content loss 和 style loss，也称为loss network  \n",
    "2 用一个残差网络作为transform网络，用于将输入图像转换为带有风格的图像，neural style 是在像素级别上更新输入图像的像素，Fast Neural Style更新的是transform网络的参数，所以可以不使用total variation denoising\n",
    "3 训练数据中：style image 是任意一种风格图像，content image 批量的包含不同内容的图像，代码中用的是COCO dataset，从而可以针对某种风格图像，训练出一个通用的model，保存下来，就可以对任一内容图像做对应的风格转换  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 提取特征的VGG网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_16(inputs,\n",
    "           num_classes=1000,\n",
    "           is_training=True,\n",
    "           dropout_keep_prob=0.5,\n",
    "           spatial_squeeze=True,\n",
    "           scope='vgg_16'):\n",
    "  \"\"\"Oxford Net VGG 16-Layers version D Example.\n",
    "  Note: All the fully_connected layers have been transformed to conv2d layers.\n",
    "        To use in classification mode, resize input to 224x224.\n",
    "  Args:\n",
    "    inputs: a tensor of size [batch_size, height, width, channels].\n",
    "    num_classes: number of predicted classes.\n",
    "    is_training: whether or not the model is being trained.\n",
    "    dropout_keep_prob: the probability that activations are kept in the dropout\n",
    "      layers during training.\n",
    "    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n",
    "      outputs. Useful to remove unnecessary dimensions for classification.\n",
    "    scope: Optional scope for the variables.\n",
    "  Returns:\n",
    "    the last op containing the log predictions and end_points dict.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope, 'vgg_16', [inputs]) as sc:\n",
    "    end_points_collection = sc.name + '_end_points'\n",
    "    # Collect outputs for conv2d, fully_connected and max_pool2d.\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n",
    "                        outputs_collections=end_points_collection):\n",
    "      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "      # Use conv2d instead of fully_connected layers.\n",
    "      net = slim.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')\n",
    "      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                         scope='dropout6')\n",
    "      net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n",
    "      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                         scope='dropout7')\n",
    "      net = slim.conv2d(net, num_classes, [1, 1],\n",
    "                        activation_fn=None,\n",
    "                        normalizer_fn=None,\n",
    "                        scope='fc8')\n",
    "      # Convert end_points_collection into a end_point dict.\n",
    "      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "      if spatial_squeeze:\n",
    "        net = tf.squeeze(net, [1, 2], name='fc8/squeezed')\n",
    "        end_points[sc.name + '/fc8'] = net\n",
    "      return net, end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 提取style features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_style_features(FLAGS):\n",
    "    \"\"\"\n",
    "    For the \"style_image\", the preprocessing step is:\n",
    "    1. Resize the shorter side to FLAGS.image_size\n",
    "    2. Apply central crop\n",
    "    \"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        network_fn = nets_factory.get_network_fn(\n",
    "            FLAGS.loss_model,\n",
    "            num_classes=1,\n",
    "            is_training=False)\n",
    "        image_preprocessing_fn, image_unprocessing_fn = preprocessing_factory.get_preprocessing(\n",
    "            FLAGS.loss_model,\n",
    "            is_training=False)\n",
    "\n",
    "        # Get the style image data\n",
    "        size = FLAGS.image_size\n",
    "        img_bytes = tf.read_file(FLAGS.style_image)\n",
    "        if FLAGS.style_image.lower().endswith('png'):\n",
    "            image = tf.image.decode_png(img_bytes)\n",
    "        else:\n",
    "            image = tf.image.decode_jpeg(img_bytes)\n",
    "        # image = _aspect_preserving_resize(image, size)\n",
    "\n",
    "        # Add the batch dimension\n",
    "        images = tf.expand_dims(image_preprocessing_fn(image, size, size), 0)\n",
    "        # images = tf.stack([image_preprocessing_fn(image, size, size)])\n",
    "\n",
    "        _, endpoints_dict = network_fn(images, spatial_squeeze=False)\n",
    "        features = []\n",
    "        for layer in FLAGS.style_layers:\n",
    "            feature = endpoints_dict[layer]\n",
    "            feature = tf.squeeze(gram(feature), [0])  # remove the batch dimension\n",
    "            features.append(feature)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            # Restore variables for loss network.\n",
    "            init_func = utils._get_init_fn(FLAGS)\n",
    "            init_func(sess)\n",
    "\n",
    "            # Make sure the 'generated' directory is exists.\n",
    "            if os.path.exists('generated') is False:\n",
    "                os.makedirs('generated')\n",
    "            # Indicate cropped style image path\n",
    "            save_file = 'generated/target_style_' + FLAGS.naming + '.jpg'\n",
    "            # Write preprocessed style image to indicated path\n",
    "            with open(save_file, 'wb') as f:\n",
    "                target_image = image_unprocessing_fn(images[0, :])\n",
    "                value = tf.image.encode_jpeg(tf.cast(target_image, tf.uint8))\n",
    "                f.write(sess.run(value))\n",
    "                tf.logging.info('Target style pattern is saved to: %s.' % save_file)\n",
    "\n",
    "            # Return the features those layers are use for measuring style loss.\n",
    "            return sess.run(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 用于生成图片的转换网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(image, training):\n",
    "    # Less border effects when padding a little before passing through ..\n",
    "    image = tf.pad(image, [[0, 0], [10, 10], [10, 10], [0, 0]], mode='REFLECT')\n",
    "\n",
    "    with tf.variable_scope('conv1'):\n",
    "        conv1 = relu(instance_norm(conv2d(image, 3, 32, 9, 1)))\n",
    "    with tf.variable_scope('conv2'):\n",
    "        conv2 = relu(instance_norm(conv2d(conv1, 32, 64, 3, 2)))\n",
    "    with tf.variable_scope('conv3'):\n",
    "        conv3 = relu(instance_norm(conv2d(conv2, 64, 128, 3, 2)))\n",
    "    with tf.variable_scope('res1'):\n",
    "        res1 = residual(conv3, 128, 3, 1)\n",
    "    with tf.variable_scope('res2'):\n",
    "        res2 = residual(res1, 128, 3, 1)\n",
    "    with tf.variable_scope('res3'):\n",
    "        res3 = residual(res2, 128, 3, 1)\n",
    "    with tf.variable_scope('res4'):\n",
    "        res4 = residual(res3, 128, 3, 1)\n",
    "    with tf.variable_scope('res5'):\n",
    "        res5 = residual(res4, 128, 3, 1)\n",
    "    # print(res5.get_shape())\n",
    "    with tf.variable_scope('deconv1'):\n",
    "        # deconv1 = relu(instance_norm(conv2d_transpose(res5, 128, 64, 3, 2)))\n",
    "        deconv1 = relu(instance_norm(resize_conv2d(res5, 128, 64, 3, 2, training)))\n",
    "    with tf.variable_scope('deconv2'):\n",
    "        # deconv2 = relu(instance_norm(conv2d_transpose(deconv1, 64, 32, 3, 2)))\n",
    "        deconv2 = relu(instance_norm(resize_conv2d(deconv1, 64, 32, 3, 2, training)))\n",
    "    with tf.variable_scope('deconv3'):\n",
    "        # deconv_test = relu(instance_norm(conv2d(deconv2, 32, 32, 2, 1)))\n",
    "        deconv3 = tf.nn.tanh(instance_norm(conv2d(deconv2, 32, 3, 9, 1)))\n",
    "\n",
    "    y = (deconv3 + 1) * 127.5\n",
    "\n",
    "    # Remove border effect reducing padding.\n",
    "    height = tf.shape(y)[1]\n",
    "    width = tf.shape(y)[2]\n",
    "    y = tf.slice(y, [0, 10, 10, 0], tf.stack([-1, height - 20, width - 20, -1]))\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 计算loss  \n",
    "\n",
    "1 用转换网络根据输入图像生成转换后图像  \n",
    "2 将输入图像和转换后图像送入VGG中提取特征  \n",
    "3 根据提取到的特征计算loss  \n",
    "4 优化loss，更新转换网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generated = model.net(processed_images, training=True)\n",
    " _, endpoints_dict = vgg_16(tf.concat([generated, processed_images], 0), spatial_squeeze=False)\n",
    "content_loss = losses.content_loss(endpoints_dict, FLAGS.content_layers)\n",
    "style_loss, style_loss_summary = losses.style_loss(endpoints_dict, style_features_t, FLAGS.style_layers)\n",
    "tv_loss = losses.total_variation_loss(generated)  # use the unprocessed image\n",
    "loss = FLAGS.style_weight * style_loss + FLAGS.content_weight * content_loss + FLAGS.tv_weight * tv_loss\n",
    "train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step=global_step, var_list=variable_to_train)\n",
    "\n",
    "def content_loss(endpoints_dict, content_layers):\n",
    "    content_loss = 0\n",
    "    for layer in content_layers:\n",
    "        generated_images, content_images = tf.split(endpoints_dict[layer], 2, 0)\n",
    "        size = tf.size(generated_images)\n",
    "        content_loss += tf.nn.l2_loss(generated_images - content_images) * 2 / tf.to_float(size)  # remain the same as in the paper\n",
    "    return content_loss\n",
    "\n",
    "def style_loss(endpoints_dict, style_features_t, style_layers):\n",
    "    style_loss = 0\n",
    "    style_loss_summary = {}\n",
    "    for style_gram, layer in zip(style_features_t, style_layers):\n",
    "        generated_images, _ = tf.split(endpoints_dict[layer], 2, 0)\n",
    "        size = tf.size(generated_images)\n",
    "        layer_style_loss = tf.nn.l2_loss(gram(generated_images) - style_gram) * 2 / tf.to_float(size)\n",
    "        style_loss_summary[layer] = layer_style_loss\n",
    "        style_loss += layer_style_loss\n",
    "    return style_loss, style_loss_summary\n",
    "\n",
    "def total_variation_loss(layer):\n",
    "    shape = tf.shape(layer)\n",
    "    height = shape[1]\n",
    "    width = shape[2]\n",
    "    y = tf.slice(layer, [0, 0, 0, 0], tf.stack([-1, height - 1, -1, -1])) - tf.slice(layer, [0, 1, 0, 0], [-1, -1, -1, -1])\n",
    "    x = tf.slice(layer, [0, 0, 0, 0], tf.stack([-1, -1, width - 1, -1])) - tf.slice(layer, [0, 0, 1, 0], [-1, -1, -1, -1])\n",
    "    loss = tf.nn.l2_loss(x) / tf.to_float(tf.size(x)) + tf.nn.l2_loss(y) / tf.to_float(tf.size(y))\n",
    "    return loss \n",
    "\n",
    "def gram(layer):\n",
    "    shape = tf.shape(layer)\n",
    "    num_images = shape[0]\n",
    "    width = shape[1]\n",
    "    height = shape[2]\n",
    "    num_filters = shape[3]\n",
    "    filters = tf.reshape(layer, tf.stack([num_images, -1, num_filters]))\n",
    "    grams = tf.matmul(filters, filters, transpose_a=True) / tf.to_float(width * height * num_filters)\n",
    "\n",
    "    return grams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
