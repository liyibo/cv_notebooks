{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Neural Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Style 每次转换都需要指定风格图片和内容图片，最小化content loss 和 style loss，重新训练一个模型，每张图片的生成都需要花费很长的时间，所以就有了Fast Neural Style 方法的提出。   \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/Fast Neural Style_3.jpg?raw=true)\n",
    "<center> **论文效果图** </center >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 摘要  \n",
    "\n",
    "1 使用 VGG16 作为特征提取网络，用提取到的特征计算 content loss 和 style loss，也称为 loss network  \n",
    "\n",
    "2 用一个残差网络作为 transform 网络，用于将输入图像转换为带有风格的图像，neural style 是在像素级别上更新输入图像的像素，Fast Neural Style 更新的是 transform 网络的参数，所以可以不使用 total variation denoising(平滑图像)   \n",
    "\n",
    "3 训练数据中：style image 是任意一种风格图像，content image 批量的包含不同内容的图像，如 COCO dataset，从而可以针对某种风格图像，训练出一个通用的model，保存下来，就可以对任一内容图像做对应的风格转换  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/Fast Neural Style_1.jpg?raw=true)\n",
    "<center> **网络结构** </center >\n",
    "\n",
    "快速风格迁移的网络结构包含两个部分。一个图像变换网络 $f_W$ 和一个用来定义几个损失函数的损耗网络 $\\phi$。图像变换网络是一个深度残差卷积神经网络，由权重 W 参数化;它通过映射 $\\overline y  = f_W(x)$ 将输入图像 x 转换成输出图像 y 。每个损失函数计算测量输出图像 y 与目标图像 $y_i$ 之间的差的标量值 $l_i(\\overline y,y_i)$。图像变换网络使用随机梯度下降来训练以最小化损失函数的加权组合：  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/fast_style_formu1.jpg?raw=true)\n",
    "\n",
    "为了解决像素损失的缺点，并允许我们的损失函数更好地衡量图像之间的感知和语义差异，我们从最近通过优化生成图像的工作中汲取灵感。这些方法的关键点是预处理图像分类的卷积神经网络已经学会了编码我们想要在我们的损失函数中测量的感知和语义信息。因此，为了定义我们的损失函数，我们利用一个已经被训练为分类的网络 $\\phi$ 作为一个固定的损失网络。然后使用损失函数对变换网络进行训练。  \n",
    "\n",
    "损失网络 $\\phi$ 用于定义测量图像之间的内容和风格的差异的特征重构损失 $l_{feat}^{\\phi}$ 和风格重建损失 $l_{style}^{\\phi}$。对于每个输入图像 x，我们都有一个内容目标 $y_c$ 和一个样式目标 $y_s$。对于样式转换，内容目标 $y_c$ 是输入图像 x，输出图像 $\\overline y$ 应该将 $x = y_c$ 的内容与 $y_s$ 的样式相结合; 我们为每个样式目标训练一个网络。对于单幅图像超分辨率，输入图像 x 是低分辨率输入，内容目标 $y_c$ 是真实高分辨率图像，不使用样式重构损失;每个超分辨率因素训练一个网络。  \n",
    "\n",
    "\n",
    "### Image Transformation Networks  \n",
    "\n",
    "我们的图像转换网络大致遵循Radford等提出的架构指导原则。不使用任何 pooling 层，而使用 strided 和 fractionally strided 的卷积来进行网内下采样和上采样。我们的网络体由五个残余块组成。除了输出层之外，所有非残留卷积层之后都是空间批量归一化和ReLU非线性，而输出层使用缩放的 tanh 来确保输出图像的像素在 [0,255] 范围内。除了使用 9×9 内核的第一层和最后一层以外，所有卷积层都使用 3×3 内核。  \n",
    "\n",
    "输入和输出： 对于风格转移，输入和输出都是形状为 3×256×256 的彩色图像。  \n",
    "\n",
    "对于样式迁移网络，我们的使用两个步长 2 的卷积来对输入进行下采样，接着是几个残余块，然后使用两个卷积层，步长 1/2 来上采样。尽管输入和输出具有相同的大小，但对于下采样然后上采样的网络来说有几个好处。  \n",
    "\n",
    "首先是计算，在一个简单的实现中，在尺寸为 C×H×W 的输入上使用 C 滤波器的 3×3 卷积需要 $9HWC^2$ 的乘加，这与在形状 DC 的输入上具有 DC 滤波器的 3×3 卷积的成本相同 DC × H / D × W / D。下采样后，我们可以使用一个更大的网络相同的计算成本。  \n",
    "\n",
    "第二个好处是有效的接受字段大小。高质量的样式转换需要以一致的方式更改大部分图像; 因此输出中的每个像素在输入中具有大的有效感受场是有利的。 在没有下采样的情况下，每个额外的 3×3 卷积层将有效感受野大小增加2倍。在以 D 倍下采样之后，每个3×3卷积反而通过 2D 增加有效感受野大小，给出具有相同数量的更大有效感受野图层。  \n",
    "\n",
    "残差连接： He 等使用残差连接来训练非常深的网络进行图像分类。他们认为，残差连接使网络很容易学习识别功能; 这对于图像变换网络来说是一个吸引人的特性，因为在大多数情况下，输出图像应该与输入图像共享结构。因此，我们网络的主体由多个残余块组成，每个块包含两个 3×3 卷积层。我们使用补充材料中的残余块设计。  \n",
    "\n",
    "###  Perceptual Loss Functions  \n",
    "\n",
    "我们定义了两个感知损失函数，用于度量图像之间的高级感知和语义差异。用了一个用来进行图像分类的网络 $\\phi$ 作为损失网络，这意味着这些知觉损失函数本身是深度卷积神经网络。在我们所有的实验中，$\\phi$ 是在 ImageNet 数据集上预训练的 16 层 VGG 网络。  \n",
    "\n",
    "特征重建损失：不是鼓励输出图像的像素 $\\overline y  = f_W(x)$ 与目标图像 y 的像素完全匹配，而是鼓励它们具有与由损失网络 $\\phi$ 计算的相似的特征表示。令 $\\phi_j(x)$ 为处理图像 x 时网络的第 j 层的激活; 如果 j 是卷积层，则 $\\phi_j(x)$ 将是形状 $C_j$ × $H_j$ × $W_j$ 的特征映射。特征重构损失是特征表示之间的（平方，归一化）欧几里得距离：  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/fast_style_formu2.jpg?raw=true)\n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/Fast Neural Style_2.jpg?raw=true)\n",
    "<center> **Figure 3** </center >\n",
    "如图 3 所示，寻找使早期层的特征重建损失最小化的图像 y 往往产生与 y 不可区分的图像。当我们从更高层重建时，图像内容和整体空间结构被保留，但颜色，纹理和确切的形状并没有被保留。使用特征重构损失来训练我们的图像变换网络，鼓励输出图像 y 在感觉上与目标图像 y 相似，但是不强制它们精确匹配。  \n",
    "\n",
    "风格重建损失: 当特征重建损失与目标 y 的内容偏离时，特征重构损失会对输出图像 y 进行惩罚。为了达到这个效果，Gaty s等提出了以下样式重建的损失，我们也希望惩罚不同的风格：颜色，纹理，常见模式等。  \n",
    "\n",
    "如上所述，令 $\\phi_j(x)$ 为输入 x 的网络 $\\phi$ 的第 j 层的激活，其是形状 $C_j$ × $H_j$ × $W_j$ 的特征映射。将Gram矩阵 $G(\\phi_j(x))$ 定义为其元素由下式给出的 $C_j$ × $C_j$ 矩阵  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/fast_style_formu3.jpg?raw=true)  \n",
    "\n",
    "如果我们将 $\\phi_j(x)$ 解释为给出 $H_j$ × $W_j$ 网格上每个点的 $C_j$ 维特征，那么 $G(\\phi_j(x))$ 与 $C_j$ 维特征的非中心协方差成正比，将每个网格位置视为独立样品。因此，它捕获有关哪些功能一起激活的信息。通过将 $\\phi_j(x)$ 整形成形状为 $C_j$ × $H_j$ × $W_j$ 的矩阵 $\\psi$，可以有效地计算出格拉姆矩阵。\n",
    "\n",
    "样式重建损失是输出图像和目标图像的格拉姆矩阵之间差异的平方 Frobenius 范数：  \n",
    "\n",
    "$ l_{style}^{\\phi ,j}(\\overline y,y) = ||G_j^{\\phi}(\\overline y) - G_j^{\\phi}(y)|| _F^2 $   \n",
    "\n",
    "即使当 $\\overline y$ 和 y 具有不同的尺寸时，样式重建损失也是明确的，因为它们的格式矩阵将具有相同的形状。  \n",
    "\n",
    "生成一个图像 y，尽量减少样式重建损失，保留目标图像的风格特征，但不保留其空间结构。从更高层重建从目标图像转移更大规模的结构。  \n",
    "\n",
    "为了从一组层 J 而不是单层  执行样式重构，我们定义 $L_{style}^{\\phi,J}(\\overline y,y)$ 为每层j∈J的损失总和。  \n",
    "\n",
    "###  Simple Loss Functions  \n",
    "\n",
    "除了上面定义的感知损失之外，我们还定义了两个简单的损失函数，它们只依赖于低级像素信息。  \n",
    "\n",
    "像素损失。像素损失是输出图像 $\\overline y$ 与目标 y 之间的（归一化的）欧几里德距离。如果两者都具有 C×H×W 的形状，则像素损失被定义为 $l_{pixel}(\\overline y,y) = \\frac{{||\\overline y - y||}_2^2}{CHW}$。这只有当我们有一个真实目标并且与网络预期匹配时才能使用。  \n",
    "\n",
    "全变分正则化。为了鼓励输出图像 y 的空间平滑，我们遵循特征反演[6,20]和超分辨[48,49]的先前工作，并利用全变分正则化函数$l_{TV}(\\overline y)$。  \n",
    "\n",
    "在本文中，我们通过训练具有感知损失函数的前馈变换网络，将前馈图像变换任务和基于优化的图像生成方法的优点结合起来。我们已经将这种方法应用于样式转换，我们实现了可比较的性能，并且与现有方法相比大大提高了速度，并且在单一图像超分辨率的情况下，我们表明具有感知损失的训练允许模型更好地重构细节和边缘。  \n",
    "\n",
    "## Conclusion \n",
    "\n",
    "在未来的工作中，我们希望探索使用感知损失函数来进行其他图像转换任务，如着色和语义分割。我们还计划调查使用不同的损失网络，看看例如在不同的任务或数据集上训练的损失网络是否可以传递具有不同类型的语义知识的图像转换网络。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 提取特征的VGG网络  \n",
    "\n",
    "使用 VGG16 作为特征提取网络，用提取到的特征计算 content loss 和 style loss，VGG16 就是本文的 loss network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_16(inputs,\n",
    "           num_classes=1000,\n",
    "           is_training=True,\n",
    "           dropout_keep_prob=0.5,\n",
    "           spatial_squeeze=True,\n",
    "           scope='vgg_16'):\n",
    "  \"\"\"Oxford Net VGG 16-Layers version D Example.\n",
    "  Note: All the fully_connected layers have been transformed to conv2d layers.\n",
    "        To use in classification mode, resize input to 224x224.\n",
    "  Args:\n",
    "    inputs: a tensor of size [batch_size, height, width, channels].\n",
    "    num_classes: number of predicted classes.\n",
    "    is_training: whether or not the model is being trained.\n",
    "    dropout_keep_prob: the probability that activations are kept in the dropout\n",
    "      layers during training.\n",
    "    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n",
    "      outputs. Useful to remove unnecessary dimensions for classification.\n",
    "    scope: Optional scope for the variables.\n",
    "  Returns:\n",
    "    the last op containing the log predictions and end_points dict.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope, 'vgg_16', [inputs]) as sc:\n",
    "    end_points_collection = sc.name + '_end_points'\n",
    "    # Collect outputs for conv2d, fully_connected and max_pool2d.\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n",
    "                        outputs_collections=end_points_collection):\n",
    "      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "      # Use conv2d instead of fully_connected layers.\n",
    "      net = slim.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')\n",
    "      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                         scope='dropout6')\n",
    "      net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n",
    "      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                         scope='dropout7')\n",
    "      net = slim.conv2d(net, num_classes, [1, 1],\n",
    "                        activation_fn=None,\n",
    "                        normalizer_fn=None,\n",
    "                        scope='fc8')\n",
    "      # Convert end_points_collection into a end_point dict.\n",
    "      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "      if spatial_squeeze:\n",
    "        net = tf.squeeze(net, [1, 2], name='fc8/squeezed')\n",
    "        end_points[sc.name + '/fc8'] = net\n",
    "      return net, end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 提取style features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_style_features(FLAGS):\n",
    "    \"\"\"\n",
    "    For the \"style_image\", the preprocessing step is:\n",
    "    1. Resize the shorter side to FLAGS.image_size\n",
    "    2. Apply central crop\n",
    "    \"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        network_fn = nets_factory.get_network_fn(\n",
    "            FLAGS.loss_model,\n",
    "            num_classes=1,\n",
    "            is_training=False)\n",
    "        image_preprocessing_fn, image_unprocessing_fn = preprocessing_factory.get_preprocessing(\n",
    "            FLAGS.loss_model,\n",
    "            is_training=False)\n",
    "\n",
    "        # Get the style image data\n",
    "        size = FLAGS.image_size\n",
    "        img_bytes = tf.read_file(FLAGS.style_image)\n",
    "        if FLAGS.style_image.lower().endswith('png'):\n",
    "            image = tf.image.decode_png(img_bytes)\n",
    "        else:\n",
    "            image = tf.image.decode_jpeg(img_bytes)\n",
    "        # image = _aspect_preserving_resize(image, size)\n",
    "\n",
    "        # Add the batch dimension\n",
    "        images = tf.expand_dims(image_preprocessing_fn(image, size, size), 0)\n",
    "        # images = tf.stack([image_preprocessing_fn(image, size, size)])\n",
    "\n",
    "        _, endpoints_dict = network_fn(images, spatial_squeeze=False)\n",
    "        features = []\n",
    "        for layer in FLAGS.style_layers:\n",
    "            feature = endpoints_dict[layer]\n",
    "            feature = tf.squeeze(gram(feature), [0])  # remove the batch dimension\n",
    "            features.append(feature)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            # Restore variables for loss network.\n",
    "            init_func = utils._get_init_fn(FLAGS)\n",
    "            init_func(sess)\n",
    "\n",
    "            # Make sure the 'generated' directory is exists.\n",
    "            if os.path.exists('generated') is False:\n",
    "                os.makedirs('generated')\n",
    "            # Indicate cropped style image path\n",
    "            save_file = 'generated/target_style_' + FLAGS.naming + '.jpg'\n",
    "            # Write preprocessed style image to indicated path\n",
    "            with open(save_file, 'wb') as f:\n",
    "                target_image = image_unprocessing_fn(images[0, :])\n",
    "                value = tf.image.encode_jpeg(tf.cast(target_image, tf.uint8))\n",
    "                f.write(sess.run(value))\n",
    "                tf.logging.info('Target style pattern is saved to: %s.' % save_file)\n",
    "\n",
    "            # Return the features those layers are use for measuring style loss.\n",
    "            return sess.run(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 用于生成图片的转换网络  \n",
    "\n",
    "用一个残差网络作为 transform 网络，用于将输入图像转换为带有风格的图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(image, training):\n",
    "    # Less border effects when padding a little before passing through ..\n",
    "    image = tf.pad(image, [[0, 0], [10, 10], [10, 10], [0, 0]], mode='REFLECT')\n",
    "\n",
    "    with tf.variable_scope('conv1'):\n",
    "        conv1 = relu(instance_norm(conv2d(image, 3, 32, 9, 1)))\n",
    "    with tf.variable_scope('conv2'):\n",
    "        conv2 = relu(instance_norm(conv2d(conv1, 32, 64, 3, 2)))\n",
    "    with tf.variable_scope('conv3'):\n",
    "        conv3 = relu(instance_norm(conv2d(conv2, 64, 128, 3, 2)))\n",
    "    with tf.variable_scope('res1'):\n",
    "        res1 = residual(conv3, 128, 3, 1)\n",
    "    with tf.variable_scope('res2'):\n",
    "        res2 = residual(res1, 128, 3, 1)\n",
    "    with tf.variable_scope('res3'):\n",
    "        res3 = residual(res2, 128, 3, 1)\n",
    "    with tf.variable_scope('res4'):\n",
    "        res4 = residual(res3, 128, 3, 1)\n",
    "    with tf.variable_scope('res5'):\n",
    "        res5 = residual(res4, 128, 3, 1)\n",
    "    # print(res5.get_shape())\n",
    "    with tf.variable_scope('deconv1'):\n",
    "        # deconv1 = relu(instance_norm(conv2d_transpose(res5, 128, 64, 3, 2)))\n",
    "        deconv1 = relu(instance_norm(resize_conv2d(res5, 128, 64, 3, 2, training)))\n",
    "    with tf.variable_scope('deconv2'):\n",
    "        # deconv2 = relu(instance_norm(conv2d_transpose(deconv1, 64, 32, 3, 2)))\n",
    "        deconv2 = relu(instance_norm(resize_conv2d(deconv1, 64, 32, 3, 2, training)))\n",
    "    with tf.variable_scope('deconv3'):\n",
    "        # deconv_test = relu(instance_norm(conv2d(deconv2, 32, 32, 2, 1)))\n",
    "        deconv3 = tf.nn.tanh(instance_norm(conv2d(deconv2, 32, 3, 9, 1)))\n",
    "\n",
    "    y = (deconv3 + 1) * 127.5\n",
    "\n",
    "    # Remove border effect reducing padding.\n",
    "    height = tf.shape(y)[1]\n",
    "    width = tf.shape(y)[2]\n",
    "    y = tf.slice(y, [0, 10, 10, 0], tf.stack([-1, height - 20, width - 20, -1]))\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 计算loss  \n",
    "\n",
    "1 用转换网络根据输入图像生成转换后图像  \n",
    "\n",
    "2 将输入图像和转换后图像送入VGG中提取特征  \n",
    "\n",
    "3 根据提取到的特征计算loss  \n",
    "\n",
    "4 优化loss，更新转换网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generated = model.net(processed_images, training=True)\n",
    " _, endpoints_dict = vgg_16(tf.concat([generated, processed_images], 0), spatial_squeeze=False)\n",
    "content_loss = losses.content_loss(endpoints_dict, FLAGS.content_layers)\n",
    "style_loss, style_loss_summary = losses.style_loss(endpoints_dict, style_features_t, FLAGS.style_layers)\n",
    "tv_loss = losses.total_variation_loss(generated)  # use the unprocessed image\n",
    "loss = FLAGS.style_weight * style_loss + FLAGS.content_weight * content_loss + FLAGS.tv_weight * tv_loss\n",
    "train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step=global_step, var_list=variable_to_train)\n",
    "\n",
    "def content_loss(endpoints_dict, content_layers):\n",
    "    content_loss = 0\n",
    "    for layer in content_layers:\n",
    "        generated_images, content_images = tf.split(endpoints_dict[layer], 2, 0)\n",
    "        size = tf.size(generated_images)\n",
    "        content_loss += tf.nn.l2_loss(generated_images - content_images) * 2 / tf.to_float(size)  # remain the same as in the paper\n",
    "    return content_loss\n",
    "\n",
    "def style_loss(endpoints_dict, style_features_t, style_layers):\n",
    "    style_loss = 0\n",
    "    style_loss_summary = {}\n",
    "    for style_gram, layer in zip(style_features_t, style_layers):\n",
    "        generated_images, _ = tf.split(endpoints_dict[layer], 2, 0)\n",
    "        size = tf.size(generated_images)\n",
    "        layer_style_loss = tf.nn.l2_loss(gram(generated_images) - style_gram) * 2 / tf.to_float(size)\n",
    "        style_loss_summary[layer] = layer_style_loss\n",
    "        style_loss += layer_style_loss\n",
    "    return style_loss, style_loss_summary\n",
    "\n",
    "def total_variation_loss(layer):\n",
    "    shape = tf.shape(layer)\n",
    "    height = shape[1]\n",
    "    width = shape[2]\n",
    "    y = tf.slice(layer, [0, 0, 0, 0], tf.stack([-1, height - 1, -1, -1])) - tf.slice(layer, [0, 1, 0, 0], [-1, -1, -1, -1])\n",
    "    x = tf.slice(layer, [0, 0, 0, 0], tf.stack([-1, -1, width - 1, -1])) - tf.slice(layer, [0, 0, 1, 0], [-1, -1, -1, -1])\n",
    "    loss = tf.nn.l2_loss(x) / tf.to_float(tf.size(x)) + tf.nn.l2_loss(y) / tf.to_float(tf.size(y))\n",
    "    return loss \n",
    "\n",
    "def gram(layer):\n",
    "    shape = tf.shape(layer)\n",
    "    num_images = shape[0]\n",
    "    width = shape[1]\n",
    "    height = shape[2]\n",
    "    num_filters = shape[3]\n",
    "    filters = tf.reshape(layer, tf.stack([num_images, -1, num_filters]))\n",
    "    grams = tf.matmul(filters, filters, transpose_a=True) / tf.to_float(width * height * num_filters)\n",
    "\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考  \n",
    "\n",
    "1 [perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155)  \n",
    "2 [fast-neural-style-tensorflow 源码](https://github.com/hzy46/fast-neural-style-tensorflow)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
