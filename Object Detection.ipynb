{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "最先进的物体检测网络依靠区域提议算法来推测物体的位置。SPPnet 和 Fast R-CNN 等进展减少了这些检测网络的运行时间，使得区域提议计算成为瓶颈。在这项工作中，我们引入了一个区域提议网络（RPN），该网络与检测网络共享全图像卷积特征，从而实现了几乎免费的区域提议。RPN 是一个全卷积网络，可同时预测每个位置的对象边界和对象分数。RPN 经过端到端训练以生成高质量区域提议，Fast R-CNN 将其用于检测。我们将 RPN 和 Fast R-CNN 通过共享其卷积特征进一步合并为一个网络-使用最近流行的具有“注意”机制的神经网络术语，RPN 组件告诉统一网络在哪里寻找。对于非常深的 VGG-16 模型，我们的检测系统在 GPU 上的帧速率为 5fps（包括所有步骤），同时在 PASCAL VOC 2007,2012 和 MS COCO 数据集上实现了最先进的目标检测精度，每张图片只有300个提议。在 ILSVRC 和 COCO 2015 比赛中，Faster R-CNN 和 RPN 是多个比赛任务中获得第一名的基础。代码已公开发布。  \n",
    "\n",
    "## 1 INTRODUCTION  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/1.jpg?raw=true)\n",
    "\n",
    "目标检测的最新进展是由区域提议方法（例如[4]）和基于区域的卷积神经网络（RCNN）[5]的成功所驱动的。尽管[5]中最初开发的基于区域的 CNN 的计算量很大，但由于在各种提议中共享卷积，因此其成本已大幅降低。最新的版本 Fast R-CNN [2]使用非常深的网络实现接近实时的速率，而忽略了花在区域提议上的时间。现在，提议是最先进的检测系统中测试时间的计算瓶颈。  \n",
    "\n",
    "区域建议方法通常依赖廉价特征和经济推断方案。Selective Search [4]是最流行的方法之一，它贪婪地合并基于工程低级特征的超级像素。然而，与有效的检测网络[2]相比，选择性搜索速度慢了一个数量级，在 CPU 实现中每个图像 2 秒。EdgeBoxes[6]目前在提议质量和速度之间提供了最佳平衡，每张图像只有 0.2 秒。尽管如此，区域提议步骤仍然会消耗与检测网络一样多的运行时间。  \n",
    "\n",
    "有人可能会注意到，fast region-based CNN 利用了 GPU，而在研究中使用的区域提议方法在 CPU 上实现，使得这种运行时比较不公平。加速提议计算的一个显而易见的方法是将其重新实现为 GPU。这可能是一个有效的工程解决方案，但重新实施忽略了下游检测网络，因此错过了共享计算的重要机会。  \n",
    "\n",
    "在本文中，我们展示了一个算法改变计算提议与深卷积神经网络-导致一个优雅和有效的解决方案，提议计算几乎免费计算。为此，我们引入了新颖的区域提议网络（RPNs），它们共享对象检测网络的卷积层。通过在测试时共享卷积，计算提议的边际成本很小（例如，每个图像10ms）。  \n",
    "\n",
    "我们的观察结果是基于区域的检测器所使用的卷积特征映射（如Fast RCNN）也可用于生成区域提议。在这些卷积特征之上，我们通过添加一些额外的卷积层来构建 RPN，这些卷积层同时在规则网格上的每个位置处回归区域边界和对象分数。RPN 因此是一种全卷积网络（FCN），并且可以专门用于生成检测提议的任务。  \n",
    "\n",
    "RPN 旨在有效预测各种比例和纵横比的区域提议。与使用图像金字塔（图1，a）或过滤器金字塔（图1，b）的流行方法不同，我们引入 “anchor” boxes\n",
    " 作为多种比例和纵横比的参考。我们的方案可以被认为是回归参考金字塔（图1，c），它避免了枚举多个比例或长宽比的图像或过滤器。该模型在使用单一尺度图像进行训练和测试时表现良好，从而有利于运行速度。  \n",
    "\n",
    "为了将 RPN 与 Fast R-CNN [2]物体检测网络统一起来，我们提出了一种训练方案，该方案在区域提议任务的微调和微调物体检测之间进行交替，同时保持提议的固定。该方案迅速收敛并产生具有卷积特征的统一网络，这些特征在两个任务之间共享。  \n",
    "\n",
    "我们在 PASCAL VOC 检测基准[11]中综合评估了我们的方法，其中使用 Fast R-CNN 的 RPN 产生的检测精度优于使用 Fast R-CNN 的选择性搜索强基线。同时，我们的方法在测试时几乎免除了选择性搜索的所有计算负担 - 提议的有效运行时间仅为 10 毫秒。使用[3]的昂贵的非常深的模型，我们的检测方法在 GPU 上仍然具有 5fps 的帧率（包括所有步骤），因此在速度和准确性方面都是实用的对象检测系统。我们还报告了 MS COCO 数据集[12]的结果，并使用 COCO 数据研究了对 PASCAL VOC 的改进。代码已经在https://github.com/shaoqingren/faster_rcnn （MATLAB）和 https://github.com/rbgirshick/py-faster-rcnn （Python）上公开发布。  \n",
    "\n",
    "这篇手稿的初稿已经在之前发表[10]。从那时起，RPN 和 Faster R-CNN 的框架已被采用并推广到其他方法，如 3D 对象检测[13]，part-based 检测[14]，实例分割[15]和图像字幕[16]。我们的快速和有效的物体检测系统也已建立在商业系统中，如 Pinterests，并报告了用户参与度的提升。  \n",
    "\n",
    "在 ILSVRC 和 COCO 2015 比赛中，Faster R-CNN 和 RPN 是 ImageNet 检测，ImageNet 定位，COCO 检测和 COCO 分割任务中的几个第一名[18]的基础。 RPN 完全学习从数据中提出区域，因此可以从更深入和更具表现力的特征（例如[18]中采用的 101 层残差网络）中轻松获益。Faster R-CNN 和 RPN 也被这些比赛中的其他几个主要参赛者使用。这些结果表明，我们的方法不仅是实际使用的经济高效的解决方案，而且是提高目标检测精度的有效方法。  \n",
    "\n",
    "## 2 RELATED WORK  \n",
    "\n",
    "**Object Proposals** 有关对象提议方法有大量文献。目标提议方法的全面调查和比较可以在[19]，[20]，[21]中找到。广泛使用的对象提议方法包括基于超像素分组（例如 Selective Search [4]，CPMC [22]，MCG [23]）和基于滑动窗口的方法（例如，objectness in windows[24]，EdgeBoxes[6]）。对象建议方法被采用为独立于检测器（例如，选择性搜索[4]物体检测器，RCNN [5]和 Fast R-CNN[2]）的外部模块。  \n",
    "\n",
    "**Deep Networks for Object Detection** R-CNN方法[5]端到端地训练 CNN 以将提议区域分类为对象类别或背景。R-CNN 主要用作分类器，它不能预测对象边界（除了通过边界框回归进行修正）。其准确度取决于区域提议模块的性能（请参见[20]中的比较）。有几篇论文提出了使用深度网络来预测对象边界框的方法。在 OverFeat 方法[9]中，训练全连接层以预测假定单个对象定位任务的框坐标。然后将全连接的层变成卷积层以检测多个特定类别的对象。MultiBox 方法[26]，[27]从一个网络生成区域提议，该网络的最后一个全连接层同时预测多个不同类别的盒子，从而推广 OverFeat 的“单盒子”方式。这些与类别不相关的盒子被用作 R-CNN 的提议[5]。与我们的全卷积方案相比，MultiBox 提议网络适用于单幅图像 crop 或多幅大型图像crop（例如224×224）。MultiBox 不共享提议和检测网络之间的特征。稍后在我们的方法的上下文中，我们将更深入地讨论 OverFeat 和 MultiBox。与我们的工作同时，DeepMask 方法[28]是为学习分割提议而开发的。  \n",
    "\n",
    "卷积的共享计算已经越来越受到人们越来越多的关注，从而获得高效而准确的视觉识别。OverFeat 论文[9]计算来自图像金字塔的卷积特征用于分类，定位和检测。为了有效的基于区域的对象检测[1]，[30]和语义分割[29]，开发了共享卷积特征映射上的自适应大小的 pooling（SPP）[1]。Fast R-CNN [2]能够对共享卷积特征进行端到端检测器训练，并显示出令人信服的准确性和速度。  \n",
    "\n",
    "## 3 FASTER R-CNN  \n",
    "\n",
    "我们的对象检测系统称为 Faster R-CNN，由两个模块组成。第一个模块是提出区域的深度全卷积网络，第二个模块是使用提出的区域的 Fast R-CNN 检测器[2]。整个系统是用于物体检测的单一统一网络（图2）。RPN 模块使用最近流行的具有'注意力'[31]机制的神经网络术语，告诉 Fast R-CNN 模块在哪里寻找。在第 3.1 节中，我们介绍区域建议网络的设计和属性。在第 3.2 节中，我们开发了用于训练具有共享特征模块的算法。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/2.jpg?raw=true)\n",
    "\n",
    "### 3.1 Region Proposal Networks  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/3.jpg?raw=true)\n",
    "\n",
    "一个区域提议网络（RPN）以任意大小的图像作为输入，并输出一组矩形对象提议，每个提议都有一个对象评分。我们用全卷积网络对这个过程进行建模[7]。因为我们的最终目标是与 Fast R-CNN 对象检测网络共享计算[2]，所以我们假设两个网络共享一组共同的卷积层。在我们的实验中，我们研究了具有5 个可共享卷积层的 Zeiler and Fergus 模型[32]（ZF）和具有 13 个可共享卷积层的 Simonyan and Zisserman 模型[3]（VGG-16）。  \n",
    "\n",
    "为了生成区域提议，我们在由最后的共享卷积层输出的卷积特征映射上滑动一个小网络。这个小网络将输入卷积特征映射的 n×n 空间窗口作为输入。每个滑动窗口被映射到一个较低维度的特征（ZF 为256-d，VGG为512-d）。这个特征被送入两个全连接层 - 一个盒子回归图层（reg）和一个盒子分类图层（cls）。我们在本文中使用 n = 3，注意到输入图像上的有效接受场很大（ZF和VGG分别为171和228像素）。这个迷你网络在图3（左）的单个位置进行说明。请注意，因为迷你网络以滑动窗口的方式运行，所有空间位置共享全连层。这种架构自然地用一个 n×n 卷积层，然后是两个1×1卷积层（分别用于reg和cls）来实现。  \n",
    "\n",
    "#### 3.1.1 Anchors  \n",
    "\n",
    "在每个滑动窗口位置，我们同时预测多个区域提议，其中每个位置的最大可能提议的数量被表示为 k。因此，reg 层具有 4k 个输出，编码 k 个框的坐标，并且 cls 层输出 2k 个评分，其估计每个提议的对象或不对象的概率。k 个提议相对于 k 个参考框被参数化，我们称之为锚。锚点位于所讨论的滑动窗口的中心，并且与比例和纵横比相关（图3左）。默认情况下，我们使用 3 个比例和 3 个高宽比，在每个滑动位置产生 k = 9 个锚点。对于大小为W×H（通常约为2400）的卷积特征映射，总共有 WHk 个锚。  \n",
    "\n",
    "##### Translation-Invariant Anchors  \n",
    "\n",
    "我们的方法的一个重要特性是它是 translates 不变的，无论是在锚点还是计算提议相对于锚点的函数方面。如果在图像中 translates 对象，则应该 translates 提议，并且相同的函数应该能够在任一位置预测提议。这种平移不变的特性由我们的方法保证。作为比较，MultiBox 方法[27]使用 k-means 生成 800 个锚点，这不是平移不变量。因此，如果 translates 对象，MultiBox 不保证会生成相同的提议。  \n",
    "\n",
    "平移不变属性也减小了模型的大小。MultiBox 有一个（4 + 1）×800 维全连接输出层，而我们的方法在 k = 9 个锚点的情况下有一个（4 + 2）×9 维卷积输出层。因此，我们的输出层具有 $2.8×10^4$ 个参数（对于VGG-16为512×（4 + 2）×9），比具有 $6.1×10^6$ 参数（1536 ×(4 + 1)GoogleNet）的 MultiBox 输出层少两个数量级。如果考虑特征投影层，我们的提议层仍然比 MultiBox 少一个数量级的参数。我们认为我们的方法在小数据集上过拟合的风险较低，如 PASCAL VOC。  \n",
    "\n",
    "##### Multi-Scale Anchors as Regression References  \n",
    "\n",
    "我们的锚点设计提出了一种解决多尺度（和纵横比）的新方案。如图1 所示，多尺度预测有两种流行的方法。第一种方法是基于图像/特征金字塔，例如在DPM [8]和基于 CNN 的方法中。图像被调整多尺度大小，并且针对每个尺度（图1（a））计算特征图（HOG [8]或深卷积特征）。这种方式通常很有用，但是非常耗时。第二种方法是在特征图上使用多个比例（和/或宽高比）的滑动窗口。例如，在DPM [8]中，使用不同的滤波器尺寸（例如5×7和7×5）分别对不同长宽比的模型进行训练。如果这种方式用于解决多个尺度问题，则可以将其视为“过滤器金字塔”（图1（b））。第二种方式通常与第一种方式共同采用[8]。  \n",
    "\n",
    "作为比较，我们的基于锚的方法建立在 pyramid of anchors 上，这是更具成本效益的。我们的方法参考多个比例和长宽比的锚箱来分类和回归边界框。 它只依赖单一尺度的图像和特征图，并使用单一尺寸的 filter（特征地图上的滑动窗口）。我们通过实验展示了这种方案对多种尺度和尺寸的效果（表8）。   \n",
    "\n",
    "由于这种基于锚点的多尺度设计，我们可以简单地使用在单尺度图像上计算的卷积特征，像 Fast R-CNN 检测器中的一样。多尺度锚的设计是共享特征的关键组件，不需要额外的成本来解决尺度问题。  \n",
    "\n",
    "#### 3.1.2 Loss Function  \n",
    "\n",
    "为了训练 RPN，我们为每个锚点分配一个二进制类标签（是或不是目标）。 我们为两种锚点分配一个正向标签：（i）与真实标注具有最高 Intersection-over-Union（IoU）的锚点，或者（ii）与任何真实标注具有高于 0.7 的 IoU 重叠的锚点。请注意，单个真实框可以为多个锚点分配肯定的标签。通常第二个条件足以确定正样本；但我们仍然采用第一个条件，原因是在少数情况下，第二个条件可能找不到正样本。如果锚点以所有真实边界框的 IoU 比率低于 0.3，就认为该锚点是负样本。既不正面也不负面的锚不会有助于训练目标。  \n",
    "\n",
    "通过这些定义，我们将 Fast R-CNN 中的多任务损失的目标函数最小化[2]。我们对图像的损失函数定义为：  \n",
    "\n",
    "$L(\\{p_i\\},\\{t_i\\}) = \\frac{1}{N_{cls}} \\sum_i L_{cls}(p_i,p_i^*) + \\lambda \\frac{1}{N_{reg}} \\sum_i p_i^* L_{reg}(t_i,t_i^*) \\tag{1}$\n",
    "\n",
    "在这里，i 是一个小批量中的锚点的索引，$p_i$ 是 i 作为对象的预测概率。如果锚为正，则真实标签 $p^*_i$ 为 1，如果锚为负，则为0。$t_i$ 是表示预测边界框的 4 个参数化坐标的矢量，并且 $t^*_i$ 是与正锚点相关联的真实框的矢量。分类损失 Lcls 是两个类（对象与非对象）的对数损失。对于回归损失，我们使用 $L_{reg}(t_i,t_i^*) = R(t_i − t_i^*)$ 其中 R 是[2]中定义的鲁棒损失函数（smooth L1）。术语 $p_i^* L_{reg}$ 表示回归损失仅在正锚 $(p_i^*=1)$ 时被激活，否则被禁用 $(p_i^*=0)$。cls 和 reg 层的输出分别由{pi}和{ti}组成。  \n",
    "\n",
    "这两项由 $N_{cls}$ 和 $N_{reg}$ 进行归一化，并由平衡参数 $\\lambda$ 加权。在我们当前的实现中（如在发布的代码中），方程(1)中的 cls 项通过mini-batch 规模（即，Ncls=256）进行归一化，并且 reg 项通过锚点位置的数量（即，Nreg〜2,400）进行归一化。默认情况下，我们设λ= 10，因此 cls和 reg 项的权重大致相等。我们通过实验表明，结果在很宽的范围内对 λ 的值不敏感（表9）。 我们还注意到，如上所述的标准化不是必需的，可以简化。 \n",
    "\n",
    "对于边界框回归，我们采用[5]之后的4个坐标的参数化：  \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "t_x = (x-x_a)/w_a, \\ t_y = (y-y_a)/h_a, \\\\\n",
    "t_w = \\log(w/w_a), \\ t_w = = \\log(h/h_a), \\\\\n",
    "t_x^* = (x^*-x_a)/w_a, \\ t_y^* = (y^*-y_a)/h_a, \\\\\n",
    "t_w^* = \\log(w^*/w_a), \\ t_h^* = = \\log(h^*/h_a),\n",
    "\\end{align} \\tag{2}\n",
    "$\n",
    "\n",
    "其中 x，y，w 和 h 表示框的中心坐标及其宽度和高度。变量 $x,x_a$ 和 $x^*$ 分别预测框，锚点框和 ground-truth 框（y，w，h 也一样）。这可以被认为是从锚箱到附近的真实边界框回归。  \n",
    "\n",
    "尽管如此，我们的方法通过与之前的 RoI-based（Region of Interest）方法不同的方式实现了边界框回归。在[1]，[2]中，对从任意大小的 RoI 池化的特征执行边界框回归，并且回归权重由所有区域大小共享。在我们的公式中，用于回归的特征在特征图上具有相同的空间大小（3×3）。为了说明不同的尺寸，学习一组 k 个边界框回归器。每个回归者负责一个尺度和一个纵横比，而 k 个回归者不共享权重。因此，由于锚的设计，即使特征具有固定的尺寸/比例，仍然可以预测各种尺寸的边框。  \n",
    " \n",
    "#### 3.1.3 Training RPNs  \n",
    "\n",
    "RPN 可以通过反向传播和随机梯度下降（SGD）进行端对端训练[35]。我们遵循[2]中的“以图像为中心”的采样策略来训练这个网络。每个小批量产生于包含许多正面和负面示例锚点的单个图像。可以针对所有锚点的损失函数进行优化，但是由于它们占主导地位，这将偏向于负样本。相反，我们在图像中随机采样 256 个锚点以计算小批量的损失函数，其中采样的正锚和负锚具有高达 1：1 的比率。如果图像中少于 128 个正面样本，我们使用负样本填充mini-batch。  \n",
    "\n",
    "我们通过从标准偏差为 0.01 的零均值高斯分布中绘制权重来随机初始化所有新图层。所有其他层（即共享卷积层）通过预训练用于 ImageNet 分类的模型[36]来初始化，如同标准实践[5]一样。我们调整 ZF 网络的所有层。对于 60k 小批量，我们使用 0.001 的学习率，PASCAL VOC 数据集中接下来的 20k 小批量使用 0.0001。我们使用 0.9 的动量和 0.0005 的重量衰减[37]。我们使用 Caffe[38]实现。  \n",
    "\n",
    "### 3.2 Sharing Features for RPN and Fast R-CNN  \n",
    "\n",
    "到目前为止，我们已经描述了如何训练用于区域提议生成的网络，而没有考虑在 region-based object detection CNN 中使用这些提议。对于检测网络，我们采用 Fast R-CNN [2]。接下来，我们将介绍用共享卷积层学习由 RPN 和 Fast R-CNN 组成的统一网络的算法（图2）。  \n",
    "\n",
    "独立训练的 RPN 和 Fast R-CNN 将以不同的方式修改卷积层。因此，我们需要开发一种允许在两个网络之间共享卷积层的技术，而不是学习两个单独的网络。我们讨论了特征共享的三种训练网络的方法：  \n",
    "\n",
    "（i）交替训练。在这个解决方案中，我们首先训练 RPN，并使用这些提议来训练 Fast R-CNN。由 Fast R-CNN 微调的网络然后用于初始化 RPN，并且该过程被重复。这是本文所有实验中使用的解决方案。  \n",
    "\n",
    "（二）近似的联合训练。在这个解决方案中，RPN 和 Fast R-CNN 网络在训练期间合并为一个网络，如图2 所示。在每次 SGD 迭代中，正向传递生成区域提议，这些提议在训练 Fast R-CNN 检测器时像之前单独训练 Fast R-CNN 时一样。反向传播像往常一样发生，其中对于共享层，来自 RPN 损失和 Fast R-CNN 损失的反向传播信号被组合。这个解决方案很容易实现。但是这个解决方案忽略了导数 w.r.t.提议框的坐标也是网络响应，也是近似的。在我们的实验中，我们凭经验发现这个解决方案产生了接近的结果，但与交替训练相比，训练时间减少了约 25-50%。这个求解器包含在我们发布的 Python 代码中。  \n",
    "\n",
    "（三）非近似联合训练。如上所述，由 RPN 预测的边界框也是输入的函数。Fast R-CNN 中的 RoI pooling 层[2]接受卷积特征并将预测的边界框作为输入，所以理论上有效的反向传播求解器也应该包含梯度 w.r.t.箱子坐标。在上述近似联合训练中，这些梯度被忽略。在一个非近似的联合训练解决方案中，我们需要一个可区分 wi.t. 的 RoI 池化层。这是一个非凡问题，可以通过[15]中开发的“RoI warping”层给出解决方案，这超出了本文的范围。  \n",
    "\n",
    "**四步交替训练** 在本文中，我们采用实用的 4 步训练算法通过交替优化学习共享特征。在第一步中，我们按照 3.1.3 节的描述训练 RPN。该网络使用ImageNet 预先训练的模型进行初始化，并针对区域提议任务进行端到端微调。在第二步中，我们使用由步骤1 RPN 生成的提议，由 Fast R-CNN 训练单独的检测网络。该检测网络也由 ImageNet 预先训练的模型初始化。此时两个网络不共享卷积层。在第三步中，我们使用检测器网络初始化 RPN 训练，但是我们修复了共享卷积层，并且只对 RPN 特有的层进行了微调。现在这两个网络共享卷积层。最后，保持共享卷积层的固定，我们对 Fast R-CNN 的特有的层进行微调。因此，两个网络共享相同的卷积层并形成统一的网络。类似的交替训练可以运行更多的迭代，但我们观察到可以忽略的改进。  \n",
    "\n",
    "### 3.3 Implementation Details  \n",
    "\n",
    "我们在单一尺度的图像上训练和测试区域提议和对象检测网络。我们重新缩放图像，使它们的短边为 s = 600像素[2]。多尺度特征提取（使用图像金字塔）可能会提高精度，但不会表现出良好的速度精度折衷[2]。在重新缩放的图像上，最后一个卷积层上的 ZF 和 VGG 网络的总步幅为16个像素，因此在调整大小（〜500×375）之前，典型的 PASCAL 图像上的约 10 个像素。即使如此大的步幅也能提供良好的效果，但步幅更小，精确度可能会进一步提高。  \n",
    "\n",
    "对于锚点，我们使用 3 个比例，盒子面积分别为 1282,2562 和 5122 像素，以及 3：1：1，1：2 和 2：1 的高宽比。这些超参数不是针对特定数据集仔细选择的，我们将在下一节中提供有关其影响的消融实验。如前所述，我们的解决方案不需要图像金字塔或 filter 金字塔来预测多个比例的区域，从而节省大量运行时间。图3（右）显示了我们的方法在广泛的尺度和宽高比方面的能力。表1 显示了使用 ZF 网络的每个锚的学习平均 proposal 大小。我们注意到，我们的算法允许预测值大于潜在感受域。这样的预测并非不可能 - 如果只有对象的中间部分是可见的，那么仍然可以大致推断出对象的范围。  \n",
    "\n",
    "跨越图像边界的锚箱需要小心处理。在训练期间，我们忽略了所有的跨界锚，所以他们不会造成损失。对于典型的 1000×600 图像，总共将有大约 20000 个（≈60×40×9）锚点。在忽略跨界锚点的情况下，每个图像有大约 6000 个锚点用于训练。如果跨界异常值在训练中不被忽略，它们会在目标中引入大的，难以纠正的错误项，并且训练不会收敛。然而，在测试过程中，我们仍然将全卷积 RPN 应用于整个图像。这可能会生成跨边界 proposal，我们将其剪切到图像边界。  \n",
    "\n",
    "一些 RPN 提议高度重叠。为了减少冗余，我们根据他们的 cls 分数对提议区域采用非极大值抑制（NMS）。我们将 NMS 的IoU阈值修正为 0.7，这使得我们每个图像约有 2000 个提议区域。正如我们将要展示的那样，NMS 不会损害最终检测的准确性，但会大大减少提议的数量。在 NMS 之后，我们使用排名top-N 提议区域进行检测。接下来，我们使用 2000 个 RPN 提议对 Fast R-CNN 进行训练，但在测试时评估不同数量的提议。  \n",
    "\n",
    "## 4 EXPERIMENTS  \n",
    "\n",
    "### 4.1 Experiments on PASCAL VOC  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/12.jpg?raw=true)\n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/4.jpg?raw=true)\n",
    "\n",
    "我们全面评估了我们在 PASCAL VOC 2007 检测基准[11]中的方法。该数据集包含 20 个对象类别的大约 5k 个训练图像和 5k 个测试图像。我们还为几个模型提供 PASCAL VOC 2012 基准测试结果。对于 ImageNet 预训练网络，我们使用具有 5 个卷积层和 3 个全连接层的 ZF net [32]的“fast”版本，以及具有 13 个卷积层和 3 个全连接层的公共 VGG-16 model[3]。我们主要评估检测 mean Average Precision（mAP），因为这是检测对象的实际指标（而不是关注对象提议代理指标）。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/6.jpg?raw=true)\n",
    "\n",
    "表2（顶部）显示了使用各种区域建议方法进行训练和测试时的 Fast R-CNN 结果。这些结果使用 ZF 网络。对于选择性搜索（SS）[4]，我们通过“快速”模式生成约 2000 个提议。对于 EdgeBoxes（EB）[6]，我们通过调整 0.7 IoU 的默认 EB 设置生成提议。在 Fast R-CNN 框架下，SS 拥有 58.7% 的mAP，EB 拥有 58.6% 的 mAP。采用 RPN 的 Fast R-CNN 实现了有竞争力的结果，使用 300 个提议时，mAP 为59.9%。由于共享卷积计算，使用 RPN 产生比使用 SS 或 EB 更快的检测系统；较少的提议也会减少 region-wise 全连接层的成本（表5）。  \n",
    "\n",
    "**RPN 上的消融实验** 为了研究 RPNs 作为提议方法的行为，我们进行了几项消融研究。首先，我们展示在 RPN 和 Fast R-CNN 检测网络之间共享卷积层的效果。为此，我们在四步培训过程的第二步之后停止。使用单独的网络将结果略微降低至58.7%（RPN + ZF，非共享，表2）。我们观察到这是因为在第三步中，当使用 detector tuned 特征来微调 RPN 时，提议质量得到改善。  \n",
    "\n",
    "接下来，我们分析了 RPN 对训练 Fast R-CNN 检测网络的影响。为此，我们通过使用 2000 个 SS 提议和 ZF 网络来训练 Fast R-CNN模型。我们固定此检测器并通过更改测试时使用的提议区域来评估检测 mAP。在这些消融实验中，RPN 不与检测器共享特征。  \n",
    "\n",
    "在测试期间用 300 个RPN提议替换 SS 导致 56.8% 的mAP。mAP 中的损失是因为训练/测试提议不一致。该结果作为以下比较的基准。  \n",
    "\n",
    "有些令人惊讶的是，RPN 在测试时使用 top-ranked 100个提议仍然会输出有竞争性的结果（55.1%），表明排名最高的 RPN 提议是准确的。另一方面，使用排名最高的 6000 个 RPN 提议（无NMS）具有可比的 mAP（55.2%），这表明 NMS 不会损害检测 mAP 并可能减少误报。  \n",
    "\n",
    "接下来，我们通过在测试时关闭 RPN 的 cls 和 reg 输出来分别研究其作用。当在测试时删除 cls 层（因此不使用 NMS/ranking）时，我们从中随机抽样N 个提议。mAP 几乎没有变化，N = 1000（55.8%），但当 N = 100 时显着下降到44.6%。这表明，cls 分数是最高级别提议的准确度的原因。  \n",
    "\n",
    "另一方面，当在测试时去除 reg 层（因此提议成为 anchor boxes）时，mAP 下降到52.1%。这表明高质量的提议主要是由于回归框限制。锚箱虽然具有多种比例尺和纵横比，但不足以进行精确检测。  \n",
    "\n",
    "我们还评估了更强大的网络对 RPN 提议质量的影响。我们使用 VGG-16 来训练 RPN，并仍然使用 SS + ZF 的上述检测器。mAP从56.8%（使用 RPN + ZF）提高到 59.2%（使用 RPN + VGG）。这是一个有希望的结果，因为它表明 RPN + VGG 的建议质量比 RPN + ZF 的建议质量好。由于 RPN+ZF 的提议与 SS 结果差不多，因此我们可能预计 RPN + VGG 优于 SS。以下实验验证了这个假设。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/5.jpg?raw=true)\n",
    "\n",
    "**VGG-16 的性能** 表3 显示了 VGG-16 的提议和检测结果。使用 RPN + VGG，非共享特征的结果为68.5%，略高于 SS 基线。如上所示，这是因为 RPN+VGG 生成的提议比 SS 更准确。与预定义的 SS 不同，RPN 被主动训练并受益于更好的网络。对于特征共享变体，结果是 69.9% - 比 SS 基线要好，但提议几乎没有成本费用。我们进一步使用 PASCAL VOC 2007 和 2012 训练 RPN 和检测网络。该 mAP 是73.2%。图5 显示了 PASCAL VOC 2007 测试集的一些结果。在 PASCAL VOC 2012 测试集（表4）中，我们的方法具有 70.4%。表6 和表7 显示了详细的数字。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/7.jpg?raw=true)\n",
    "\n",
    "在表5 中我们总结了整个物体检测系统的运行时间。根据不同内容，SS 需要1-2秒（平均大约1.5s），而使用 VGG-16 的 Fast R-CNN 在 2000 个 SS 建议中需要 320ms（如果在全连接层上使用 SVD，则需要 223ms）[2]）。我们的 VGG-16 系统在提议和检测中总共需要 198ms。在共享卷积特性的情况下，仅RPN 仅需要 10ms 来计算附加层。我们的区域计算也较低，这归功于较少的提议（每张图片300张）。我们的系统采用 ZF 网络，帧速率为 17 fps。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/8.jpg?raw=true)\n",
    "\n",
    "**对超参数的敏感度** 在表8 中，我们调查了锚的设置。默认情况下，我们使用 3 个尺度和 3 个纵横比（表8中的69.9%mAP）。如果在每个位置仅使用一个锚点，则 mAP 将下降 3-4% 的相当大的余量。如果使用 3 个尺度（1个纵横比）或 3 个纵横比（1个尺度），则 mAP 更高，这表明使用多个尺寸的锚点作为回归参考是一个有效的解决方案。仅使用具有 1 个纵横比（69.8%）的 3 个尺度与在该数据集上使用 3 个尺度比例的 3 个尺度一样好，这表明检测准确率不严重依赖尺度和纵横比。但我们仍然在设计中采用这两个维度来保持我们系统的灵活性。  \n",
    "\n",
    "在表9 中，我们比较了公式(1)中λ的不同值。默认情况下，我们使用λ= 10，这使得方程(1)中的两项在标准化之后大致相等地加权。表9 显示，当λ处于大约两个数量级（1至100）的范围内时，我们的结果仅略微受到影响（约1%）。这表明结果在很大范围内对λ不敏感。  \n",
    "\n",
    "**Recall-to-IoU 分析** 接下来，我们用不同的 IoU 比例计算召回，值得注意的是，Recall-to-IoU 度量仅仅与最终检测精度有关。使用此指标来诊断提议方法比评估更合适。  \n",
    "\n",
    "在图4 中，我们显示了使用 300,1000 和 2000 个提议的结果。我们与 SS 和 EB 进行比较，根据这些方法产生的置信度，N 个提议是排名前 N 的提议。图表显示，当提议数量从 2000 个降至 300 个时，RPN 方法表现优雅。这解释了为什么 RPN 在使用少至 300 个提议时具有良好的最终检测 mAP。正如我们以前分析过的，这个属性主要归因于 RPN 的 cls 术语。当提议较少时， SS 和 EB 召回降低的速度比 RPN 快。  \n",
    "\n",
    "**一阶段检测与两阶段提议+检测** OverFeat 论文[9]提出了一种在卷积特征映射的滑动窗口上使用回归器和分类器的检测方法。OverFeat 是一阶段，特定类别的检测流水线，我们的是两阶段级联，包括类别不可知的提议和类别特定的检测。在 OverFeat 中，区域特征来自一个比例金字塔上一个长宽比的滑动窗口。这些特征用于同时确定对象的位置和类别。在 RPN 中，这些特征来自方形（3×3）滑动窗口并预测相对于具有不同比例和高宽比的锚的提议。尽管这两种方法都使用滑动窗口，但区域提议任务只是 Faster RCNN 的第一阶段 - 下游 Fast R-CNN 检测器会参照提议来修正它们。在我们级联的第二阶段，区域特征自适应地池化[1]，[2]来自更忠实地覆盖区域特征的提议框。我们相信这些特征可以带来更准确的检测结果。  \n",
    "\n",
    "为了比较一阶段和两阶段系统，我们通过一阶段 Fast R-CNN 模拟 OverFeat 系统（从而也避开了实现细节的其他差异）。在这个系统中，“提议”是 3 个尺度（128,256,512）和 3 个纵横比（1：1,1：2,2：1）的密集滑动窗口。Fast R-CNN 训练来预测特定类别分数并从这些滑动窗口中回归框位置。由于OverFeat 系统采用图像金字塔，因此我们还使用从 5 个比例提取的卷积特征进行评估。我们使用[1]，[2]中的5个尺度。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/9.jpg?raw=true)\n",
    "\n",
    "表10 比较了两阶段系统和两阶段系统的变体。使用 ZF 模型，一阶段系统的 mAP 为53.9%。这比两阶段系统（58.7%）低4.8%。该实验验证了级联区域提议和对象检测的有效性。在[2]，[39]中报道了类似的观察结果，其中用滑动窗代替 SS 区域提议导致两篇论文中的〜6% 的退化。我们也注意到，一阶系统比较慢，因为它更多的处理提议。  \n",
    "\n",
    "### 4.2 Experiments on MS COCO  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/13.jpg?raw=true)\n",
    "\n",
    "我们在 Microsoft COCO 对象检测数据集[12]上提供了更多结果。该数据集涉及 80 个对象类别。我们用训练集上的 80k 图像，验证集上的 40k 图像以及测试集上的 20k 图像进行实验。我们评估了 IoU∈[0.5：0.05：0.95]（COCO 的标准度量，简称为 mAP @ [.5，.95]）和 mAP@0.5（PASCAL VOC度量）的平均 mAP。  \n",
    "\n",
    "我们的系统对这个数据集做了一些小的改动。我们在 8-GPU 上训练我们的模型，RPN（每GPU 1个）的有效最小批量大小为8，Fast R-CNN（每 GPU 2个）的有效小批量大小为 16。RPN 步骤和 Fast R-CNN 步骤都以 240k 次迭代进行训练，学习率为 0.003，然后 80k 次迭代后为 0.0003 。我们修改学习率（从0.003而不是0.001开始），因为小批量的大小发生了变化。对于锚点，我们使用 3 个纵横比和 4 个尺度（加上642），主要是处理这个数据集上的小对象。此外，在我们的 Fast R-CNN 步骤中，负样本被定义为在[0,0.5]区间内与 ground truth 具有最大 IoU 的提议，而不是[1]，[2]中使用的[0.1,0.5] ]。我们注意到，在 SPPnet 系统[1]中，[0.1,0.5] 中的负样本用于网络微调，但[0,0.5]中的负样本仍然在支持向量机的步骤中使用 hard-negative mining。但 Fast R-CNN 系统[2]放弃了 SVM 步骤，所以[0,0.1]中的负样本都不会被访问。使用这些[0,0.1]样本在 COCO 数据集上改进了 mAP@0.5（但对PASCAL VOC 的影响可以忽略不计）。  \n",
    "\n",
    "其余的实施细节与 PASCAL VOC 相同。特别是，我们继续使用 300 个提议和单一尺度（s = 600）进行测试。COCO 数据集上的每个图像的测试时间仍然大约为 200 毫秒。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/10.jpg?raw=true)\n",
    "\n",
    "在表11 中，我们首先报告 Fast R-CNN 系统[2]的结果，使用本文中的实现。我们的 Fast R-CNN 基线在测试集上有 39.3% 的mAP@0.5，高于[2]中的报告。我们猜想造成这种差距的原因主要是由于负样本的定义以及小批量大小的变化。我们还注意到，mAP @ [.5，.95]只是可比的。  \n",
    "\n",
    "接下来我们评估我们的 Faster R-CNN 系统。使用 COCO 训练集训练，COCO 测试组中 Faster R-CNN 拥有 42.1% mAP@0.5 和 21.5% mAP @ [.5，.95]。与同样方案下的 Fast RCNN 相比，mAP@0.5 和 mAP@[.5，.95]分别提高 2.8% 和 2.2%（表11）。这表明 RPN 在提高 IoU 阈值时可以提高定位精度。使用COCO 训练集验证集训练，Faster RCNN 在 COCO 测试组中具有 42.7% 的 mAP@0.5 和 21.9% 的 mAP@[.5，.95]。图6 显示了 MS COCO test-dev 集合中的一些结果。  \n",
    "\n",
    "**Faster R-CNN in ILSVRC & COCO 2015 competitions** 我们已经证明，由于 RPN 完全学习了通过神经网络提出区域，因此更好的特征使 Faster R-CNN 获益更多。即使将深度增加到 100 层以上，这种观察仍然有效[18]。只用 101 层残差网络（ResNet-101）代替 VGG-16，在COCO val set上 Faster R-CNN 系统将 mAP从 41.5%/21.2%（VGG-16）增加到 48.4%/27.2%（ResNet -101）。随着与 Faster RCNN 正交的其他改进，He 等人[18]在 COCO 测试开发组中获得了 55.7%/34.9% 的单模型结果和 59.0%/37.4% 的 ensemble 结果，在 COCO 2015 对象检测竞赛中获得了第一名。同样的系统[18]也在 ILSVRC 2015 对象检测竞赛中获得第一名，超过第二名的绝对 8.5%。RPN 也是 ILSVRC 2015 定位和 COCO 2015 分割比赛第一名获奖作品的基石，详情请参见[18]和[15]。  \n",
    "\n",
    "### 4.3 From MS COCO to PASCAL VOC  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/11.jpg?raw=true)\n",
    "\n",
    "大规模数据对改善深度神经网络至关重要。接下来，我们调查 MS COCO 数据集如何帮助 PASCAL VOC 的检测性能。  \n",
    "\n",
    "作为一个简单的基线，我们直接评估 PASCAL VOC 数据集上的 COCO 检测模型，无需对任何 PASCAL VOC 数据进行微调。这种评估是可能的，因为 COCO 类别是 PASCAL VOC 类别的超集。COCO 专用的类别在本实验中被忽略，softmax 层仅在 20 个类别加背景下执行。在 PASCAL VOC 2007 测试装置中，该设置下的 mAP 为76.1%（表12）。这一结果好于对 VOC07 + 12（73.2%）进行培训的结果。  \n",
    "\n",
    "然后我们对 VOC 数据集上的 COCO 检测模型进行微调。在这个实验中，COCO 模型代替了 ImageNet 预先训练的模型（用于初始化网络权重），Faster R-CNN 系统按照 3.2 节的描述进行了微调。这样做会导致 PASCAL VOC 2007 测试组中 78.8% 的 mAP。来自 COCO 集合的额外数据增加了 5.6%的mAP。表6 显示，在 PASCAL VOC 2007 中，使用 COCO + VOC 进行训练的模型具有每个类别中最佳的 AP。在 PASCAL VOC 2012 测试集上观察到类似的改进（表12和表7）。我们注意到，获得这些强大结果的测试时间速度仍然大约为每个图像 200 毫秒。  \n",
    "\n",
    "## 5 CONCLUSION  \n",
    "\n",
    "我们展示了 RPN 可以生成高效和准确的区域提议。通过与下游检测网络共享卷积特征，区域提议步骤几乎没有成本。我们的方法使统一的，基于深度学习的物体检测系统能够以接近实时的帧率运行。学习到的 RPN 也提高了区域提议质量，从而提高了整体物体检测的准确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light-Head R-CNN: In Defense of Two-Stage Object Detector  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "在本文中，我们首先研究为什么典型的两阶段方法不如单阶段快速检测器如 YOLO[26,27]和 SSD[22]快。我们发现 Faster RCNN[28]和 R-FCN[17]在 RoI 变形之前或之后执行密集计算。Faster R-CNN 涉及两个全连接 RoI 识别层，而 RFCN 生成一个大的分数图。因此，由于架构中的 heavy-head 设计，这些网络的速度很慢。即使我们显着减少基本模型，计算成本也不能相应地大幅度降低。  \n",
    "\n",
    "我们提出了一个新的两阶段检测器，Light-Head RCNN，以解决当前两阶段方法中的缺点。在我们的设计中，我们通过使用精简特征映射和廉价的 R-CNN 子网（pooling 和单个全连接层）使网络头部尽可能轻。我们基于 ResNet-101 的 Light-Head R-CNN 在 COCO 上优于目前最先进的物体检测器，同时保持时间效率。更重要的是，我们的 LightHead R-CNN 只需用一个小型网络（例如Xception）替换骨干网络，就可以在 COCO 上以 102 FPS 获得 30.7 mmAP，明显优于单级快速检测器，如 YOLO[26,27]和 SSD [ 22]的速度和准确性。代码将公开发布。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "最近基于 CNN 的物体检测器可分为单级检测器和两级检测器。单级检测器通常在速度和精度之间有很好的折中。两阶段检测器将任务分为两步：第一步(body)生成许多提议，第二步(head)侧重于提议的认可。通常，为了达到最佳精度，头部的设计很重。两级检测器通常具有（相对）慢速和非常高的精确度。  \n",
    "\n",
    "两级检测器能否以效率和准确度击败单级检测器？我们发现像 Faster R-CNN [5]和 RFCN [17]这样典型的两阶段目标检测器具有类似的特征：与骨干网连接的头部较重。例如，Faster RCNN 在 ResNet 第 5 阶段[28,29]中使用了两个大的全连接层或所有卷积层，以实现每个 RoIs 的识别和回归。就每个区域的预测而言，这是非常耗时的，而在大量提议被使用时甚至会变得更糟。另外，ROI pooling 之后的特征通道数量很大，这使得第一个全连接消耗大量内存，并且可能影响计算速度。与 Fast/Faster R-CNN 不同，基于区域的全卷积网络（R-FCN）[17]多次应用每个区域的子网络试图在所有 RoI 中共享计算。然而，R-FCN 需要用 $#class \\times p \\times p$（p是 pooling size）信道产生非常大的附加分数图，这也是耗内存和耗时的。如果我们使用小型骨干网络，Faster R-CNN 或 R-FCN 的 heavy head 设计使两阶段方法的竞争力降低。  \n",
    "\n",
    "在本文中，我们提出了一个 LightHead 设计来构建一个高效但精确的两级检测器。具体而言，我们应用 large-kernel 可分离卷积来生成具有小信道数的“thin”特征映射（在我们的实验中使用 $\\alpha \\times p \\times p,\\alpha \\leq 10 $）。这种设计极大地减少了 RoI-wise 子网络的计算量，并使检测系统占用更少的内存。一个便宜的单个全连接层连接到池化层之后，该层利用特征表示进行分类和回归。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/2/1.jpg?raw=true)\n",
    "\n",
    "由于我们的 LightHead 结构，我们的检测器能够达到速度和准确度的最佳折中，无论使用大型还是小型骨干网络。如图1 所示，LightHead R-CNN 可以显着优于像 SSD [22]和 YOLOv2 [27]这样的快速单级检测器，而且计算速度更快。另外，我们的算法对大型骨干网络也很灵活。基于 ResNet-101 骨干网，我们可以胜过最先进的算法，包括像 Mask R-CNN 这样的两级检测器和像 RetinaNet 这样的一级检测器[20]。  \n",
    "\n",
    "## 2. Related works  \n",
    "\n",
    "受益于深卷积网络的快速发展，物体检测问题取得了很大进展。我们从两个维持简要回顾一下最近物体检测工作：  \n",
    "\n",
    "**Accuracy 观点:** R-CNN [6]是最早将深度神经网络特征用于检测系统的方法之一。诸如选择性搜索[37]，Edge Boxes[40]，MCG[1]等手工设计的方法涉及为 R-CNN 提出建议。然后提出 Fast R-CNN[5]加入训练对象分类和边界框回归，提高多任务训练的性能。继 Fast R-CNN 之后，Faster R-CNN[28]引入了区域提议网络（RPN），通过使用网络特征生成提议。受益于更丰富的提议，它稍微提高了准确性。Faster RCNN 被视为 R-CNN 系列检测器的里程碑。以下大部分工作通过将更多计算带入网络来加强 RCNN。Dai 等人提出可变形的卷积网络[3]通过学习额外的偏移而无需监督来建模几何变换。Lin 等人提出特征金字塔网络（FPN）[19]，它利用深度卷积网络的固有多尺度金字塔结构来构建特征金字塔。基于 FPN，Mask RCNN[7]通过在边界框识别的基础上增加一个额外分支来进一步扩展 mask 预测器。RetinaNet[20]是另一种基于 FPN 的单级检测器，其使用 Focal-Loss 来解决极端前后背景比导致的类别失衡问题。  \n",
    "\n",
    "**速度观点** 物体检测文献也在努力提高检测器的速度。回到原来的 R-CNN，它通过整个网络分别前向计算每个提议。He 等人提出 SPPnet [8]共享候选框之间的计算。Fast/Faster R-CNN[5,28]通过统一检测流水线来加速网络。R-FCN[17]共享 RoI 子网络之间的计算，当使用大量提议时可以加速推理。另一个热门研究课题是 proposal free 检测器。YOLO 和 YOLO v2[26,27]将对象检测简化为一个回归问题，直接预测边界框和相关的类别概率，而不需要生成提议。SSD[22]通过产生来自不同层的不同尺度的预测来进一步提高性能。与基于盒子中心的检测器不同，DeNet[36]首先预测所有盒子的角，然后快速搜索角分布以寻找非平凡的边界框。  \n",
    "\n",
    "总之，从精确度的角度来看，一级和两级检测器都能以相近的速度达到最先进的精度。然而，从速度的角度来看，在相近的准确率条件下，目标检测器缺乏可以可以媲美一级检测器的两级检测器。在本文中，我们试图设计一种更好更快的两级检测器，称为 LightHead R-CNN 来弥补这一缺陷。  \n",
    "\n",
    "## 3. Our Approach  \n",
    "\n",
    "在本节中，我们将首先介绍我们的LightHead RCNN，然后描述对象检测中的其他设计细节。  \n",
    "\n",
    "### 3.1. Light-Head R-CNN  \n",
    "\n",
    "正如我们在第1 节中讨论的那样，传统的两级物体检测器通常包含一个 heavy head，这对计算速度有负面影响。我们论文中的“head”是指连接到我们骨干基础网络的结构。更具体地说，它包含有两个组件：R-CNN 子网和 ROI 变形。  \n",
    "\n",
    "#### 3.1.1 R-CNN subnet  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/2.jpg?raw=true)\n",
    "\n",
    "Faster R-CNN 采用强大的 R-CNN，它利用两个大的全连接层或整个 Resnet stage 5[28,29]作为第二级分类器，有利于检测性能。因此，Faster R-CNN 及其扩展在像 COCO 这样最具挑战性的基准测试中具有领先的准确率。但是，特别是在对象提议数量很大的情况下，计算可能是密集的。为了加速 RoI-wise subnet，R-FCN 首先为每个区域生成一组评分图，其信道数为 $classes \\times p \\times p$（p是后续的 pooling size），然后沿着每个 RoI 进行 pooling，并进行最终的预测。使用免计算的 R-CNN 子网，R-FCN 通过在 RoI 共享分数图生成中涉及更多计算而获得可比较的结果。  \n",
    "\n",
    "如上所述，Faster R-CNN和 R-FCN 都具有 heavy head，但位置不同。从准确性的角度来看，虽然 Faster R-CNN 在 RoI 分类方面很好，但它通常涉及全局 global average pooling，以减少第一个全连接层的计算，这对空间定位是有害的。对于 R-FCN，它在 position-sensitive pooling 之后直接 pools 预测结果，并且性能通常不像没有 Roi-wise 计算层的 Faster R-CN那么强。从速度的角度来看，Faster R-CNN 将通过昂贵的 R-CNN 子网独立前向计算每个 RoI，这会降低网络速度，特别是在提议数量很大时。R-FCN 使用免费的 R-CNN 子网作为第二级检测器。但是，由于 R-FCN 需要为 RoI 池提供非常大的分数图，整个网络仍然耗费时间/内存。  \n",
    "\n",
    "考虑到这些问题，在我们新的 Light-Head RCNN 中，我们建议为我们的 R-CNN 子网使用一个简单，廉价的全连接层，这在性能和计算速度之间取得了很好的平衡。图2（C）提供了我们的 LightHead RCNN 的概述。由于全连接层的计算和存储成本还取决于 ROI 操作后的数量信道映射，我们接下来讨论我们如何设计 ROI 变形。  \n",
    "\n",
    "#### 3.1.2 Thin feature maps for RoI warping  \n",
    "\n",
    "在将提议送入 R-CNN 子网之前，需要通过 RoI warping 来固定特征图的形状。  \n",
    "\n",
    "在我们的 Light-Head R-CNN 中，我们建议生成具有小信道数量（薄特征图）的特征图，然后进行传统的 RoI warping。在我们的实验中，我们发现在薄特征映射上的 RoI warping 不仅会提高准确性，还会在训练和推理期间节省内存和计算量。考虑在薄特征图上的 PSRoI pooling，我们可以通过更多的计算来加强 R-CNN 并减少信道数量。另外，如果在我们的精简特征映射上应用 RoI pooling，我们可以减少 R-CNN 开销并放弃 Global Average Pooling 以同时提高性能。此外，在不损失时间效率的情况下，可以在薄特征图生成上使用大型卷积。  \n",
    "\n",
    "### 3.2. Light-Head R-CNN for Object Detection  \n",
    "\n",
    "在上述讨论之后，我们介绍了一般对象检测的实现细节。我们的方法流程如图2（C）所示。我们有两个设置：1）设置“L”来验证我们的算法与大型骨干网集成时的性能；2）设置“S”以验证我们的算法在使用小骨干网络时的有效性和效率。除非另有说明，否则设置 L 和设置 S 共享相同的其他设置。  \n",
    "\n",
    "**基本特征提取器** 对于设置 L，我们采用 ResNet 101[9]作为我们的基本特征提取器。另一方面，我们利用类似于 Xception 的小型基础模型来设置 S。Xception 模型的网络结构可以在表7 中找到。在图2 中，“Conv layers”表示我们的基础模型。最后的卷积块 conv4 和 conv5 表示为C4，C5。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/3.jpg?raw=true)\n",
    "\n",
    "**薄特征图** 我们在 C5 上应用大的可分离的卷积层[35,25]，结构如图3 C所示。在我们的方法中，我们令 k 为 15，Cmid = 64 用于设置S，Cmid = 256 用于设置L。我们也将 Cout 减少到 10×p×p，这与 R-FCN 中使用的 #class×p×p 的相比非常小。受益于 large kernel 引起的更大的有效接受场，我们 pooled 特征图更加强大。  \n",
    "\n",
    "**R-CNN subnet** 这里我们只在 R-CNN 子网中使用一个带有 2048 个通道（no dropout）的全连接层，然后是两个全连接层来预测 RoI 分类和回归。 因为我们共享不同类别之间的回归，所以每个边界框位置仅应用 4 个通道。受益于用来进行 RoI warping 的强大的特征图，简单的 LightHead R-CNN 也可以在保持效率的同时取得显着成果。  \n",
    "\n",
    "**RPN** 区域提议网络是一个滑动窗口分类对象检测器，它使用来自 C4 的特征。RPN 预先定义了一组由几个特定比例和长宽比控制的锚点。在我们的模型中，我们设置了三个宽高比 {1：2,1：1,2：1} 和五个比例 {322,642,1282,2562,5122} 来覆盖不同形状的物体。由于存在很多重叠的提议，因此使用非极大值抑制（NMS）来减少提议的数量。在将它们馈送到 RoI 预测子网络之前。我们为 NMS 设置 0.7 的交叉联合（IoU）阈值。然后，我们根据基于他们的IoU 比率的锚定训练标签与真实边界框。如果锚与任意真实边界框具有超过 0.7 的 IoU ，则将其设置为正面标签。与真实边界框具有最高 IoU 的锚也将被分配一个正面标签。同时，如果锚点与所有真实边界框中的 IoU 小于0.3，则它们的标签将是负面的。更多的细节可以参考[28]。  \n",
    "\n",
    "## 4. Experiments  \n",
    "\n",
    "在本节中，我们造 COCO [21,18]数据集评估我们的方法，该数据集有 80 个对象类别。有 80k 训练集和 40k 验证集，将进一步划分为 35k val-minusmini 和 5k mini-validation。按照常规设置，我们将训练集和 val-minusmin 组合起来，以获得用于训练的 115K 图像，并使用 5K mini-validation 图像进行验证。  \n",
    "\n",
    "### 4.1. Implementation Details  \n",
    "\n",
    "我们的检测器是基于 8 Pascal TITAN XP GPU 使用同步 SGD 进行端到端训练的，重量衰减为 0.0001，动量为 0.9。对每个GPU上 mini-batch 有 2 个图像，每个图像具有 2000/1000 个用于训练/测试的 RoI。我们通过将零填充到图像的右下方来将小图像内的图像填充到相同的大小。对于最初的 1.5M 迭代，学习速率设置为 0.01（通过 1 个图像将被视为 1 次迭代），对于后面的 0.5M 迭代，将学习速率设置为 0.001。  \n",
    "\n",
    "所有的实验在 Resnet 的第五阶段和 online hard example mining （OHEM）[31]技术中采用 atrous[24,23,2]算法。除非明确说明，否则我们的骨干网络将根据预先训练好的 ImageNet [30]基础模型进行初始化，并将 pooling size 设置为 7。我们将基准模型中的阶段1 和阶段2 的参数固定，batch normalization 也固定在实验中固定。除非另有说明，否则将采用水平图像翻转来增强数据。  \n",
    "\n",
    "在下面的讨论中，我们将首先进行一系列消融实验来验证我们方法的有效性。随后，我们将在 COCO 测试数据集上介绍与 FPN[19]，Mask R-CNN[7]，RetinaNet[20]等先进检测器的比较。  \n",
    "\n",
    "### 4.2. Ablation Experiments  \n",
    "\n",
    "为了与现有方法进行比较，我们采用 Resnet101 作为我们的消融研究的骨干网络，类似于 3.2 节中描述的设置 L。  \n",
    "\n",
    "#### 4.2.1 Baselines  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/5.jpg?raw=true)\n",
    "\n",
    "在公开可用的 R-FCN 代码提供的详细设置之后，我们首先在我们的实验中评估 R-FCN，表示为 B1，并且其在 COCO 迷你验证集中达到 32.1% mmAP。  \n",
    "\n",
    "通过修改基准设置，我们可以得到一个更强的基线，用 B2 表示，但有以下区别：（i）我们将图像的较短边缘调整为 800 像素，并将较长边缘的最大尺寸限制为 1200。由于较大的图像，因此我们设置 RPN 有 5 个锚点{322,642,1282,2552,5122}。（ii）我们发现 R-CNN 中的回归损失肯定小于分类损失。 因此，我们加倍 R-CNN 回归损失以平衡多任务训练。（iii）我们选择基于反向传播损失排名的 256 个样本。我们使用每个图像 2000 个 RoIs 进行训练，并使用 1000 个 RoIs 进行测试。如表1 所示，我们的方法将 mmAP 提高了近 3 个点。  \n",
    "\n",
    "#### 4.2.2 Thin feature maps for RoI warping  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/4.jpg?raw=true)\n",
    "\n",
    "我们调查了在 4.2.1 节中描述的基线设置之后减少用于 ROI warping 的特征映射通道的影响。为了实现这一目标，我们设计了一个简单的网络结构，如图4 所示。整个管道与原来的 R-FCN 完全相同，用于比较。除了以下小差异：（i）我们将 PSRoI pooling 的特征映射通道减少到 490（10×7×7）。注意到它与包含 3969（81×7×7）个通道的原始 R-FCN 完全不同。（ii）当我们修改特征图的通道数量时，我们不能直接投票给最终的预测。最终预测涉及一个简单的全连接层。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/6.jpg?raw=true)\n",
    "\n",
    "结果如表2 所示。虽然通道数量大幅减少，3969 vs 490，但我们有相当的表现。此外，需要指出的是，使用我们的 Light-head R-CNN 设计，它使我们能够有效地集成特征金字塔网络[19]，如表5 所示。如果我们想要在 Conv2（Resnet阶段2）等高分辨率特征图上执行 position-sensitive pooling，用原始 RFCN 几乎是不可能，这将消耗更多的内存。  \n",
    "\n",
    "为了验证在 Faster R-CNN 中减少通道的影响，我们也尝试用传统的 RoI pooling 替换 PSRoI pooling，它稍微提高了 0.3 的精度增益。一个假设是 RoI pooling 在第二阶段涉及更多特征（49x），准确性增益受益于更多的计算。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/7.jpg?raw=true)\n",
    "\n",
    "**大的可分离卷积** 在我们的设计中，用于 RoI warping 的特征映射通道非常小。在原始实现中，1×1 卷积涉及重构一个小通道，这降低了特征映射能力。我们用大的可分离卷积在保持小的通道数的同时，增强这些特征图。我们在图3 中显示了大核的结构。我们设置的超参数是 k = 15，Cmid = 256，Cout = 490。在表3 中，与基于我们的再现 R-FCN 设置 B2 的结果相比，由大内核产生薄特征映射可以将性能提高 0.7 个百分点。  \n",
    "\n",
    "#### 4.2.3 R-CNN subnet  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/8.jpg?raw=true)\n",
    "\n",
    "在这里，我们评估 R-CNN 的轻型版本对 R-CNN 子网的影响，如图3 所示。在 RoI 子网中采用了一个具有 2048 个通道的全连接层（without dropout）。 由于我们的特征是在通道数量很小的特征图中 pooling 得到的（在我们的实验中为 10 个），因此对于每个区域的分类和回归，Light RCNN 非常快。Light RCNN 与我们复现的 stronger Faster R-CNN 和 R-FCN B2 的比较如表4 所示。  \n",
    "\n",
    "当组合大核特征图和 Light RCNN 时，我们的方法实现了 37.7 mmAP。在我们的实验中，在相同的基本设置下，Faster R-CNN 和 R-FCN 得到 35.5/35.1 mmAP的结果，这远远低于我们的方法。更重要的是，由于极薄的特征图和 light R-CNN，即使使用数千个提议，我们也能保持时间效率。  \n",
    "\n",
    "### 4.3. Light-Head R-CNN: High Accuracy  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/9.jpg?raw=true)\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/10.jpg?raw=true)\n",
    "\n",
    "结合最先进的物体检测器，我们遵循 3.2 节中描述的设置 L。此外，我们还用了 RoRelign [7]中提出的 PSRoI 池化插值技术。结果如表6 所示，可以带来 1.3% 的收益。我们也应用常用的 scale jitter 方法。在我们的训练过程中，我们从{600,700,800,900,1000}像素中随机抽样比例，然后将图像的较短边调整为采样比例。图像的最大边缘约束在 1400 像素内，因为较短的边缘可能达到 1000 像素。由于数据增加，训练时间也增加。Multi-scale 训练使mmAP 获得近 1% 的改善。我们最终用非极大值抑制（NMS）的 0.5 代替了原始的 0.3 阈值。通过提高召回率，它可以提高 0.6% 的mmAP。  \n",
    "\n",
    "表5 还总结了最新技术检测器的结果，包括 COCO 测试开发数据集上的一阶段和两阶段方法。我们的单一比例测试模型可以达到 40.8% mmAP 而无需花时间，显着超越所有竞争对手（40.8 vs 39.1）。它证实了我们的 Light Head R-CNN 对于大型骨干模型是一个不错的选择，并且可以在没有大量计算成本的情况下获得有希望的结果。一些说明性的结果如图5 所示。  \n",
    "\n",
    "### 4.4. Light-Head R-CNN: High Speed  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/11.jpg?raw=true)\n",
    "\n",
    "基于计算速度，像 Resnet 101 这样较大的骨干网速度较慢。为了验证 LightHead R-CNN 的效率，我们产生了一个有效的类似 bottleneck xception 的网络来评估我们的方法(with 34.1 top 1 error at 224 × 224 in ImageNet)，来评估我们的方法。骨干网架构如表7 所示。在 xception 设计策略之后，我们用通道式卷积替换瓶颈结构中的所有卷积层。然而，由于网络浅，我们不使用在身份映射[10]中提出的预激活设计。我们的 Light Head R-CNN 的实施细节可以在3.2 节的设置 S 中找到。  \n",
    "\n",
    "更具体地说，我们对快速推理速度进行了以下更改：（i）我们用一个类似 xception 的小网络来替换设置 L 中的 Resnet-101 骨干网。（ii）在我们的快速模型中放弃了 atrous 算法，因为它与小型骨干网相比涉及很多计算。（iii）我们将 RPN 卷积设置为 256 个通道，这是 Faster R-CNN 和 R-FCN 一半。（iv）我们应用 kernel_size= 15，Cmid = 64，Cout = 490（10×7×7）的大分离卷积。由于我们使用的中间通道非常小，因此大内核对于推断仍然有效。（v）我们采用带对齐技术的 PSPooling 作为我们的 RoI warping,，因为它将合并的特征映射通道减少了 k×k 倍（k是 pooling 大小）。注意到如果包含 RoI-align，它将获得更好的结果。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/1/12.jpg?raw=true)\n",
    "\n",
    "我们将我们的方法与 YOLO，SSD 和 DeNet 等近期快速检测器进行了比较。结果在 COCO test-dev 数据集上进行评估，只采用一个批次，并采用 Batch-normalization [15]进行快速推理。如表8 所示，我们的方法在 MS COCO 上以 102 FPS 获得 30.7 mmAP，明显优于 YOLO 和 SSD 等快速检测器。图6 中显示了一些结果。  \n",
    "\n",
    "## 5. Conclusion  \n",
    "\n",
    "在本文中，我们提出 LightHead R-CNN，其中包含了两阶段物体检测器的更好的设计原理。与像 Faster R-CNN 和 R-FCN 这些通常头部重的传统两级检测器相比，我们的 LightHead 设计使我们能够在不影响计算速度的情况下显着提高检测结果。更重要的是，与像 YOLO 和 SSD 这样的快速单级检测器相比，即使计算速度更快，我们也可以获得优异的性能。例如，我们的 Light Head R-CNN 与小型 Xception-like 基础模型相结合，可以以 102 FPS 的速度达到30.7 mmAP。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cascade R-CNN: Delving into High Quality Object Detection  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "在对象检测中，需要通过联合交叉（IoU）阈值来定义正样本和负样本。用低 IoU 阈值进行训练的物体检测器，例如 0.5，通常会产生嘈杂的检测结果。但是，随着 IoU 阈值的增加，检测性能会降低。造成这种情况的原因有两个：1）由于指数级正样本消失引起的训练过拟合；2）输入假设与检测器最优的 IOU 推理时不一致。提出了多级对象检测结构 Cascade R-CNN 来解决这些问题。它由一系列随着 IoU 阈值增加而训练的检测器组成，以对 close false positives 依次更具选择性。检测器逐步进行训练，利用检测器的输出是一个良好的分布来训练下一个更高质量的检测器。改进的 hypotheses 的重采样保证所有检测器都有一组正确的相等大小示例，减少了过拟合问题。在推断中应用相同的级联程序，使得假设和每个阶段的检测器质量之间更接近匹配。级联R-CNN 的简单实现超过具有挑战性的 COCO 数据集上的所有单模型对象检测器。实验还表明，级联 R-CNN 可广泛应用于不同的检测器架构，获得与基准检测器强度无关的一致增益。该代码将在 https://github.com/zhaoweicai/cascade-rcnn 上提供。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "目标检测是一个复杂的问题，需要解决两个主要任务。首先，检测器必须解决识别问题，将前景物体与背景区分开来，并为其分配正确的物体类别标签。 其次，检测器必须解决定位问题，将精确的边界框分配给不同的对象。这两种情况都非常困难，因为检测器会面临许多 close false positives，对应于“接近但不正确”的边界框。检测器必须找到真正的正样本，同时抑制这些 close false positives。  \n",
    "\n",
    "最近提出的许多物体检测器都是基于两阶段 R-CNN 框架，其中检测被定义为一个多任务学习问题，它结合了分类和边界框回归。与对象识别不同，需要联合交叉（IoU）阈值来定义 positives/negatives。然而，通常使用的阈值 u（通常u = 0.5）对正样本提出了相当宽松的要求。如图1（a）所示，所得到的检测器经常会产生噪声包围盒。大多数人认为 close false positives 的假设经常通过 IoU≥0.5 检验。虽然在 u = 0.5 标准下组装的例子很丰富而且多样化，但它们很难训练能够有效拒绝 close false positives 的检测器。  \n",
    "\n",
    "在这项工作中，我们将假设的质量定义为与真实边界框的 IoU，并将检测器的质量定义为用于训练的 IoU 阈值。目标是研究迄今为止研究的高质量物体检测器的研究不足，其输出包含很少的 close 误报，如图1（b）所示。基本思想是单个检测器只能针对单个质量水平进行优化。这在 cost-sensitive 的学习文献[7,24]中是已知的，其中 receiver operating characteristic（ROC）的不同点的优化需要不同的损失函数。主要区别在于我们考虑了给定 IoU 阈值的优化，而不是误报率。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/3/1.jpg?raw=true)\n",
    "\n",
    "图1（c）和（d）说明了这个想法，它分别表示了用 u = 0.5,0.6,0.7 的 IoU 阈值训练的三个检测器的定位和检测性能。如 COCO [20]所述，定位性能被评估为输入提议的 IoU 函数，检测性能用 IoU 阈值函数进行评估。请注意，在图1（c）中，每个边界框回归器在输入样本 IOU 与 用相近 IOU 阈值训练的检测器上表现的最好，这也适用于检测性能，直到过拟合。图1（d）显示，对于低 IoU 的样本，u = 0.5 的检测器优于 u = 0.6 的检测器，在较高的IoU 水平下表现不及 u = 0.6 检测器。通常，在单个 IoU 级别优化的检测器在其他级别不一定是最佳的。这些观察结果表明，更高质量的检测需要检测器与其处理的假设之间更接近的质量匹配。一般来说，如果提供高质量的提议，检测器只能具有高质量。  \n",
    "\n",
    "然而，为了生产高质量的检测器，在训练期间仅仅增加 u 是不够的。实际上，如图1（d）中 u = 0.7 的检测器所示，这可能会降低检测性能。问题在于检测器之外的假设分布通常严重失衡，导致质量低下。一般而言，强迫更大的 IoU 阈值导致训练样本数量呈指数级地减少。这对神经网络来说尤其有问题，神经网络被称为非常耗费数据，并且使得“high u”训练策略很容易过拟合。另一个难点是检测器质量与推断测试样本之间的不匹配。如图1所示，高质量的检测器只对于高质量的样本是最佳的。当他们被要求研究其他质量水平的样本时，检测可能并不理想。  \n",
    "\n",
    "在本文中，我们提出了一种新的检测器体系结构 Cascade R-CNN 来解决这些问题。它是 R-CNN 的多阶段延伸，其中级联更深的检测器依次对 close 误报有更强的选择性。R-CNN 阶段的级联是按顺序训练的，使用一个阶段的输出来训练下一阶段。这是因为观察到回归器的输出 IoU 几乎总是比输入 IoU 好。这个观察可以在图1（c）中进行，其中所有图都在灰线之上。它表明用某个 IoU 阈值训练的检测器的输出训练下一个较高 IoU 阈值检测器的良好的分布。这与通常用于在物体检测文献中 assemble 数据集的 boostrapping 方法类似[31,8]。主要区别在于 Cascade R-CNN 的重采样过程并不旨在挖掘困难负样本。相反，通过调整边界框，每个阶段的目的都是为了找到一组好的 close false positives 来训练下一阶段。当以这种方式操作时，适用于越来越高的IoU 的一系列检测器可以击败过拟合问题，并且因此被有效地训练。在推断中，应用相同的级联程序。逐步改进的假设更好地匹配每个阶段检测器质量的提高。这可以实现更高的检测精度，如图1（c）和（d）所示。  \n",
    "\n",
    "Cascade R-CNN 的实施和端到端训练非常简单。我们的研究结果表明，在没有任何花哨的情况下，尤其是在较高质量的评估指标下，在具有挑战性的 COCO 检测任务[20]中的简单实现大大超过了先前的所有先进的单模型检测器。此外，可以使用基于 R-CNN 框架的任何两阶段目标检测器构建级联 R-CNN。我们观察到一致的收益（2〜4%），计算的时间也增加。这个增益独立于基线对象检测器的强度。因此，我们相信这种简单而有效的检测体系结构对于许多目标检测研究工作可能是有意义的。  \n",
    "\n",
    "## 2. Related Work  \n",
    "\n",
    "由于 R-CNN[12]体系结构的成功，通过结合提议检测器和区域分类器的两阶段检测问题的制定已成为最近的主流。为了减少 R-CNN 中 CNN 的冗余计算，SPP-Net[15]和 Fast-RCNN[11]引入了 region-wise feature extraction 的思想，显着加快了整个检测器的速度。后来，Faster-RCNN[27]通过引入区域提议网络（RPN），进一步提高了速度。这种架构已经成为领先的对象检测框架。最近的一些作品已将其扩展到解决各种细节问题。例如，R-FCN [4]提出了高效的区域全卷积而没有精度损失，以避免 Faster-RCNN 的重区域 CNN 计算；而 MS-CNN [1]和 FPN [21]检测多个输出层的提议，以缓解 RPN 接受域与实际对象大小之间的尺度不匹配，从而实现高召回提议检测。  \n",
    "\n",
    "或者，单级对象检测体系结构也变得流行，主要是由于它们的计算效率。这些架构接近于经典的滑动窗口策略[31,8]。YOLO[26]通过对输入图像一次前向计算输出非常稀疏的检测结果。当使用高效的主干网络实施时，它可以实现较高性能的实时对象检测。SSD[23]以类似于 RPN[27]的方式检测物体，但使用多个不同分辨率的特征图来覆盖各种尺度的物体。这些架构的主要限制是它们的精度通常低于两级检测器的精度。最近，RetinaNet[22]被提出来解决密集对象检测中极端的前景-背景类别不平衡问题，比现现有的两阶段对象检测器获得更好的结果。  \n",
    "\n",
    "还提出了一些多阶段目标检测的探索。多区域检测器[9]引入迭代边界框回归，其中应用多次 R-CNN 以生成更好的边界框。CRAFT[33]和 AttractioNet[10]使用多阶段程序来生成准确的提议，并将它们输入到 Fast-RCNN。[19,25]在物体检测网络中嵌入了[31]的经典级联结构。[3]在分割任务中迭代检测和分割任务。  \n",
    "\n",
    "## 3. Object Detection  \n",
    "\n",
    "在本文中，我们扩展了 Faster-RCNN[27,21]的两阶段架构，如图3（a）所示。第一阶段是提议子网络（“H0”），应用于整个图像，以产生初步检测假设，称为对象提议。在第二阶段，这些假设然后由感兴趣区域检测子网络（“H1”）处理，表示为检测 head。最终的分类评分（“C”）和边界框（“B”）被分配给每个假设。我们专注于对多级检测子网进行建模，并采用（但不限于）泳衣提议检测的 RPN[27]。  \n",
    "\n",
    "### 3.1. Bounding Box Regression  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/3/2.jpg?raw=true)\n",
    "\n",
    "边界框 $b =(b_x,b_y,b_w,b_h)$ 包含图像块 x 的四个坐标。边界框回归的任务是使用回归器 $f(x,b)$ 将候选边界框 b 回归到目标边界框 g 中。这是从训练样本 $\\{g_i,b_i\\}$ 中学习的，以尽量减少边界框的风险  \n",
    "\n",
    "$R_{loc}[f] = \\sum_{i=1}^N L_{loc}(f(x_i,b_i),g_i) \\tag{1}$\n",
    "\n",
    "其中 $L_{loc}$ 是 R-CNN 中的 L2 损失函数[12]，但更新为 Fast-RCNN 中的平滑 L1 损失函数[11]。为了鼓励回归对 scale and location 不变，$L_{loc}$ 在距离向量 $\\Delta = (\\delta_x, \\delta_y, \\delta_w, \\delta_h)$ 上通过下式进行操作   \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\delta_x = (g_x-b_x)/b_w, \\ \\delta_y = (g_y-b_y)/b_h, \\\\\n",
    "\\delta_w = \\log(g_w/b_w), \\ \\delta_h = = \\log(g_h/b_h), \\\\\n",
    "\\end{align} \\tag{2}\n",
    "$\n",
    "\n",
    "由于边界框回归通常对 b 进行微小调整，因此(2)的数值可能非常小。因此，(1)的风险通常比分类风险小得多。为了提高多任务学习的有效性，Δ通常用其均值和方差标准化，即$\\delta_x$ 由 $\\delta'_x = (\\delta_x - \\mu_x)/\\sigma_x$ 代替。 这在文献中广泛使用[27，1，4，21，14]。  \n",
    "\n",
    "有些作品[9,10,16]认为 f 的单一回归步骤不足以进行精确定位。相反，f 作为修正边界框 b 的后处理步骤，是迭代应用的，  \n",
    "\n",
    "$f'(x,b) = f \\circ f \\circ ... \\circ f(x,b) \\tag{3}$\n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/3/3.jpg?raw=true)\n",
    "\n",
    "这被称为迭代边界框回归，表示为迭代 BBox。它可以用图3（b）中的所有头部相同的推理架构来实现。然而，这个想法忽略了两个问题。首先，如图1 所示，在 u = 0.5 时训练的回归器 f 对于较高 IoU 的假设是次优的。它实际上降低了 IoU 大于 0.85 的边界框。其次，如图2 所示，边界框的分布在每次迭代后都会发生显著变化。尽管回归方法对于初始分布是最佳的，但在此之后它可能并不理想。由于这些问题，迭代式 BBox 需要大量的人工工程，如提议累积，边界框投票等[9,10,16]，并且有一些不可靠的收益。通常，应用 f 两次以上就没有任何好处。  \n",
    "\n",
    "### 3.2. Classification  \n",
    "\n",
    "分类器是一个函数 h(x)，它将图像补丁 x 分配给 M + 1 个类中的一个，其中类 0 是背景，剩余的类是要检测的对象。h(x) 是 M + 1 维分类器，即 $h_k(x) = p(y=k|x)$，其中 y 是类别标签。给定训练集(xi,yi)，通过最小化分类风险来学习  \n",
    "\n",
    "$R_{cls}[h] = \\sum_{i=0}^N L_{cls}(h(x_i),y_i)$\n",
    "\n",
    "其中 $L_{cls}$ 是典型的交叉熵损失。  \n",
    "\n",
    "### 3.3. Detection Quality  \n",
    "\n",
    "由于边界框通常包含一个对象和一定量的背景，因此很难确定检测是正面还是负面的。这通常由 IoU 指标确定。如果 IoU 高于阈值u，则该补丁被认为是该类的一个正样本。因此，提议 x 的类标签是 u 的函数，  \n",
    "\n",
    "$\n",
    "y = \\begin{cases} g_y, \\ \\text{IOU $(x,g) \\geq u$} \\\\ 0 , \\ \\text{otherwise} \\end{cases} \\tag{5}\n",
    "$\n",
    "\n",
    "$g_y$ 是真实对象 g 的类别标签。这个 IoU 阈值 u 定义了检测器的质量。  \n",
    "\n",
    "对象检测具有挑战性，因为无论阈值如何，检测设置都是非常对抗的。当你很高时，正样本包含较少的背景，但很难收集用于训练的足够的正样本。当 u 低时，可以获得更丰富，更多元化的正样本训练集，但训练好的检测器几乎没有动机拒绝 close false positives。总的来说，要求单个分类器在所有 IoU级别上均匀执行是非常困难的。推理中，由于大多数提议是由检测器生成的，例如 RPN[27]或选择性搜索[30]，检测器必须对较低质量的提议进行更多判别。这些相互矛盾的要求之间的标准折中是解决 u = 0.5。然而，这是一个相对较低的阈值，导致低质量的检测，大多数人认为 close false positives，如图1(a)所示。  \n",
    "\n",
    "一个简单的解决方案是开发一个分类器集合，图3(c)的体系结构，针对不同质量等级的损失进行优化，  \n",
    "\n",
    "$L_{cls}(h(x),y) = \\sum_{u \\in U} L_{cls}(h_u(x),y_u) \\tag{6}$\n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/3/4.jpg?raw=true)\n",
    "\n",
    "其中 U 是一组 IoU 阈值。这与[34]的 integral loss 密切相关，其中 U = {0.5,0.55，...，0.75}，其被设计为适合 COCO 挑战的评估度量。根据定义，分类器需要在推理中进行整合。该解决方案未能解决(6)中的不同损失在不同数目的正样本上运行的问题。如图4 的第一幅图所示，正样本的集合随着u 迅速下降。这是特别有问题的，因为高质量分类器易于过拟合。此外，要求这些高质量分类器处理压倒性低质量推荐的提议，因为这些提议并未得到优化。 由于所有这些，(6)的集合在大多数质量水平上不能达到更高的精度，并且该架构与图3（a）相比几乎没有增益。  \n",
    "\n",
    "## 4. Cascade R-CNN  \n",
    "\n",
    "在本节中，我们将介绍图3（d）提出的级联 RCNN 物体检测架构。  \n",
    "\n",
    "### 4.1. Cascaded Bounding Box Regression  \n",
    "\n",
    "如图1（c）所示，要求单个回归器在所有质量水平上表现完全一致是非常困难的。困难的回归任务可以分解为一系列更简单的步骤，受到级联姿态回归[6]和人脸对齐[2,32]的启发。在 Cascade R-CNN 中，它被构建为级联回归问题，其结构如图3（d）所示。这依赖于一系列专业的回归模型  \n",
    "\n",
    "$f(x,b) = f_T \\circ f_{T-1} \\circ ... \\circ f_1(x,b) \\tag{7}$\n",
    "\n",
    "其中 T 是级联阶段的总数  \n",
    "\n",
    "它有几种不同于图3（b）的迭代 BBox 架构。首先，虽然迭代 BBox 是用于改进边界框的后处理程序，但级联回归是一个重采样过程，它改变了不同阶段要处理的假设分布。其次，因为它既用于训练也用于推理，在训练和推理分布之间没有差异。第三，针对不同阶段的重采样分布对多个专用回归器{fT,fT-1,...,f1}进行优化。这与(3)的单 f 相反，它于只对初始分布来说是最优的。这些差异使得比迭代 BBox 更精确的定位，而不需要进一步的人体工程学。  \n",
    "\n",
    "正如第3.1节所讨论的，(2)中的 $\\Delta = (\\delta_x, \\delta_y, \\delta_w, \\delta_h)$ 需要通过其均值和方差进行归一化，以便进行有效的多任务学习。在每个回归阶段之后，这些统计量将按顺序演变，如图2 所示。在训练时，使用相应的统计量来对每个阶段的Δ进行归一化。  \n",
    "\n",
    "### 4.2. Cascaded Detection  \n",
    "\n",
    "如图4 的左边所示，初始假设的分布，例如，RPN 提议，严重倾向于低质量。这不可避免地导致对更高质量分类器的无效学习。级联 R-CNN 通过依赖级联回归作为重采样机制来解决这个问题。这是由图1（c）中所有曲线高于对角灰线的事实所驱动的，即对某个 u 训练的边界框回归器倾向于产生较高 IoU 的边界框。因此，从一组示例（xi，bi）开始，级联回归连续重新采样较高 IoU 的示例分布$(x'_i,b'_i)$。以这种方式，即使当检测器质量（IoU阈值）增加时，也可以将连续阶段的一组正例保持在大致恒定的大小。如图4 所示，每次重采样后，分布倾向于更高质量的示例。两个后果随之而来。首先，没有过拟合，因为各个层面样本都很丰富。其次，较深阶段的检测器针对较高的 IoU 阈值进行了优化。请注意，如图2 所示，通过增加 IoU 阈值，可以逐步删除一些异常值，从而实现更好的专用检测器。  \n",
    "\n",
    "在每个阶段 t，R-CNN 包括一个分类器 $h_t$ 和一个为 IoU 阈值 $u^t$ 优化的回归器 $f_t$，其中 $u^t> u^{t-1}$。这可以通过减少以下损失来保证  \n",
    "\n",
    "$L(x^t,g) = L_{cls}(h_t(x^t),y^t) + \\lambda[y^t \\geq 1] L_{loc}(f_t(x^t,b^t),g) \\tag{8}$\n",
    "\n",
    "其中 $b^t = f_{t-1}(x^{t-1},b^{t-1})$，g 是 xt 的 ground truth，λ= 1 是 trade-off 系数，[·]是指示函数，yt 是给定 ut 的 xt 的标签。与(6)的整体损失不同，这保证了一系列经过有效训练的质量提高的检测器。在推论中，通过应用相同的级联过程，假设的质量被顺序地改进，并且只需要更高质量的检测器来操作更高质量的假设。这可以实现高质量的对象检测，如图1（c）和（d）所示。  \n",
    "\n",
    "## 5. Experimental Results  \n",
    "\n",
    "Cascade R-CNN 在 MS-COCO2017 上进行了评估[20]，其中包含约 118k 图像用于训练，5k 用于验证（val），约 20k 用于没有提供注释的测试（测试开发）。COCO-style 平均精确度（AP）以 0.05 为间隔，在的 IoU 阈值从 0.5 到 0.95 上计算平均 AP。这些评估指标衡量各种质量的检测性能。所有模型都在 COCO 训练集上训练，并在 val set 上进行评估。最终结果也在测试开发集中报告。  \n",
    "\n",
    "### 5.1. Implementation Details  \n",
    "\n",
    "为简单起见，所有的回归者都是类别不可知的。Cascade R-CNN 中的所有级联检测级都具有相同的架构，即基线检测网络的头部。除非另有说明，级联 R-CNN 总共有四个阶段，一个 RPN 和三个用于 U = {0.5,0.6,0.7} 的检测。第一个检测阶段的采样和[11,27]中的一样。在以下阶段，重新采样通过简单地使用前一阶段的回归输出来实现，如4.2节所述。 除了标准的水平图像翻转之外，没有使用数据增强。推论是在一个单一的图像尺度上进行的。所有基线检测器都用 Caffe 重新实现，在相同的代码基础上进行公平比较。  \n",
    "\n",
    "#### 5.1.1 Baseline Networks  \n",
    "\n",
    "为了测试级联 R-CNN 的多功能性，使用三种流行的基线检测器进行实验：具有主干 VGG-Net 的 Faster-RCNN，R-FCN 和具有 ResNet 主干的 FPN 的。这些基线具有广泛的检测性能。除非另有说明，否则使用其默认设置。使用了端到端的训练，而不是多步骤的训练。  \n",
    "\n",
    "**Faster-RCNN:** 网络头部有两个全连接层。为了减少参数，我们使用[13]修剪较不重要的连接。每个全连接层保留 2048 个神经元，并删除 dropout 层。训练以 0.002 的学习率开始，在 60k 和 90k 迭代时减少 10 倍，并在 100k 迭代后停止，两路 CPU，每个GPU每次迭代训练 4 张图像。每个图像使用 128  个RoI。  \n",
    "\n",
    "**R-FCN:** R-FCN 向 ResNet 添加了一个卷积层，一个边界框回归层和分类层。Cascade R-CNN 的所有 heads 都有这样的结构。没有使用 Online hard\n",
    "negative mining。训练以 0.003 的学习率开始，在 160k 和 240k 迭代时减少了 10 倍，4 路 CPU，每个GPU每次迭代训练一个图像，在 280k 迭代时停止。每个图像使用 256 个 RoI。  \n",
    "\n",
    "**FPN:** 由于 FPN 尚未公开提供源代码，因此我们的实现细节可能会有所不同。RoIAlign [14]被用于更强的基线。这被表示为 FPN+ 并被用于所有消融研究。像往常一样，ResNet-50 用于消融研究，ResNet-101 用于最终检测。在初始 120k 迭代时使用 0.005 的学习率，接下来 60k 迭代使用 0.0005的学习率，8 路 CPU，，每个GPU每次迭代训练一个图像，每个图像使用256个RoI。  \n",
    "\n",
    "### 5.2. Quality Mismatch  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/3/5.jpg?raw=true)\n",
    "\n",
    "图5（a）显示了 U = {0.5,0.6,0.7} 增加 IoU 阈值的三个单独训练检测器的 AP 曲线。在低 IoU 水平下，u = 0.5 的检测器优于 u = 0.6 的检测器，但在较高水平下表现不佳。然而，u = 0.7 的检测器不如其他两个。为了理解为什么会发生这种情况，我们改变了推理的建议质量。图5（b）显示了当 ground truth 边界框被添加到提议集时获得的结果。尽管所有检测器都有所改进，但 u = 0.7 的检测器获得的增益最大，几乎在所有的 IoU 级别都能达到最佳性能。这些结果表明两个结论。首先，u = 0.5 对于精确检测来说不是一个好的选择，对于低质量的提议更简单。其次，高度精确的检测需要符合检测器质量的假设。接下来，原始检测器提议被更高质量的 Cascade R-CNN 提议取代（u = 0.6，u = 0.7 分别用于第二和第三阶段提议）。图5（a）也表明，当测试方案更接近检测器质量时，两个检测器的性能显着提高。  \n",
    "\n",
    "在所有级联阶段测试所有 Cascade R-CNN 检测器产生了相似的观察结果。图6 显示，当使用更精确的假设时，每个检测器都得到了改进，而更高质量的检测器具有更大的增益。例如，对于第一阶段的低质量建议，u = 0.7 的检测器表现不佳，但对于更深层叠阶段中更精确的假设更好。另外，即使在使用相同的提议时，图6 的联合训练的检测器也优于图5（a）的单独训练的检测器。这表明检测器在 Cascade R-CNN 框架内得到了更好的训练。  \n",
    "\n",
    "### 5.3. Comparison with Iterative BBox and Integral Loss  \n",
    "\n",
    "在本节中，我们将 Cascade R-CNN 与迭代 BBox 和积分损耗检测器进行比较。通过反复应用 FPN+ 基线三次实施迭代BBox。积分损失检测器具有与级联 R-CNN 相同数量的 classification heads，U = {0.5,0.6,0.7}。  \n",
    "\n",
    "**Localization:** 图7（a）比较了级联回归和迭代 BBox 的定位性能。单个回归器的使用降低了对高 IoU 假说的定位。迭代应用回归器时会产生这种效应，如迭代式 BBox 中所示，并且性能实际上下降。注意3 次迭代后迭代 BBox 的性能很差。相反，级联回归器在后期阶段具有更好的性能，在几乎所有的 IoU 水平上都优于迭代 BBox。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/3/6.jpg?raw=true)\n",
    "\n",
    "**Integral Loss:** 共享单个回归器的积分损失检测器中所有分类器的检测性能如图7（b）所示。u = 0.6 的分类器在所有的 IoU 水平上都是最好的，而 u = 0.7 的分类器是最差的。所有分类器的集合显示没有明显的收益。  \n",
    "\n",
    "表1 显示，迭代 BBox 和积分损失检测器略微改善了基线检测器。级联 R-CNN 对于所有评估指标具有最佳性能。对于较低的 IoU 阈值，收益是轻微的，但对较高IoU，收益较大。  \n",
    "\n",
    "### 5.4. Ablation Experiments  \n",
    "\n",
    "我们也进行了消融实验。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/3/7.jpg?raw=true)\n",
    "\n",
    "**Stage-wise Comparison:** 表2 总结了 stage 性能。由于多阶段多任务学习的好处，第一阶段已经胜过基线检测器。第二阶段大幅度提高性能，第三阶段与第二阶段相当。这不同于 integral 损失检测器，其中较高的 IOU 分类器相对较弱。尽管前（后）阶段在低（高）IoU 指标方面表现更好，但所有分类器的集合总体上是最好的。  \n",
    "\n",
    "**IoU Thresholds:** 对所有头部使用相同的 IoU 阈值 u = 0.5 对初级级联 R-CNN 进行训练。在这种情况下，这些阶段仅在他们收到的假设上有所不同。每个阶段都用相应的假设进行训练，即考虑图2 的分布。表3 的第一行显示级联在基线检测器上得到改进。这表明优化相应样本分布阶段的重要性。 第二行显示，通过增加阶段阈值 u，可以使检测器对接近的假阳性更具选择性，并且专用于更精确的假设，从而导致额外的收益。这支持了第4.2节的结论。  \n",
    "\n",
    "**Regression Statistics:** 利用图2 中逐步更新的回归统计，有助于有效地进行分类和回归的多任务学习。通过比较表3 中的 with/without it 的模型可以看出它的好处。学习对这些统计数据不敏感。  \n",
    "\n",
    "**Number of Stages:** 阶段数的影响总结在表4 中。添加第二个检测阶段显着改善了基线检测器。三个检测阶段仍然产生不小的改进，但是增加第四阶段（u = 0.75）导致性能略微下降。但是请注意，尽管整体 AP 性能下降，但四级级联对于较高的 IoU 级别具有最佳性能。三阶段级联达到最佳平衡。  \n",
    "\n",
    "### 5.5. Comparison with the state-of-the-art  \n",
    "\n",
    "基于 FPN+ 和 ResNet-101 骨干网的 Cascade R-CNN 与表5 中最先进的单模型物体检测器进行了比较。设置如 5.1.1 节所述，但总共为 280k 训练迭代运行，并且学习速率在 160k 和 240k 迭代时降低。表5 中的第一组检测器是一级检测器，第二组两级，以及最后一组多级（Cascade R-CNN 的三级+RPN ）。所有比较最先进的检测器都经过 u = 0.5 的训练。值得注意的是，我们的 FPN+ 实现比原始的 FPN 更好[21]，提供了非常强大的基线。此外，从 FPN+ 到 Cascade R-CNN 的延伸性能提升了约 4%。在所有的评估指标下，Cascade R-CNN 的表现也优于单模式检测器。其中包括 2015 年和 2016 年 COCO 挑战获胜者（Raster CNN +++ [16]和 G-RMI [17]）以及最近可变形 R-FCN [5]，RetinaNet [22]和 Mask R-CNN [14]。COCO上最好的多级检测器，AttractioNet [10]，使用迭代 BBox 生成提议。尽管 AttractioNet 中使用了许多增强功能，但 vanilla Cascade R-CNN 的表现仍然优于 7.1%。请注意，与 Mask R-CNN 不同，在 Cascade R-CNN 中不使用分割信息。最后，vanilla 单模型 Cascade R-CNN 也超越了在 2015 年和 2016 年赢得 COCO 挑战的重工程集合检测器（分别为 AP 37.4 和 41.6）。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/3/8.jpg?raw=true)\n",
    "\n",
    "### 5.6. Generalization Capacity  \n",
    "\n",
    "表6 中比较了所有三个基线检测器的三级串联 R-CNN。所有设置如上。  \n",
    "\n",
    "**Detection Performance:** 同样，我们的实现比原始检测器更好[27,4,21]，Cascade R-CNN 在这些基线上一直提高 2〜4 个点。这些收益在 val 和test-dev 上也是一致的。这些结果表明 Cascade R-CNN 可广泛应用于各种检测器架构。  \n",
    "\n",
    "**Parameter and Timing:** 级联 RCNN 参数的数量随着级联级数的增加而增加。基线检测器头的参数数量的增加是线性的。另外，由于与 RPN 相比，检测头的计算成本通常较小，所以在训练和测试中，级联 R-CNN 的计算开销都很小。  \n",
    "\n",
    "## 6. Conclusion  \n",
    "\n",
    "在本文中，我们提出了一个多级目标检测框架 Cascade R-CNN，用于设计高质量的物体检测器。该架构被证明可以避免训练过拟合和推理质量不匹配的问题。级联 R-CNN 在具有挑战性的 COCO 数据集上的稳固一致的检测改进表明，需要对各种并发因子进行建模和理解，以推进对象检测。Cascade R-CNN 被证明适用于许多目标检测体系结构。我们相信它可能对许多未来的目标检测研究工作有用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You Only Look Once: Unified, Real-Time Object Detection  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "我们介绍 YOLO，一种新的物体检测方法。先前的物体检测工作重新设置分类器来执行检测。相反，我们框架将对象检测化为空间分离边界框和相关类别概率的回归问题。单个神经网络在一次评估中直接从完整图像预测边界框和类概率。由于整个检测流水线是单个网络，因此可以直接针对检测性能端到端地进行优化。  \n",
    "\n",
    "我们的统一架构非常快。我们的基础 YOLO 模型以45帧/秒的速度实时处理图像。较小版本的网络 Fast YOLO 每秒处理 155 帧，同时实现其他实时检测器的 mAP 的两倍。与最先进的检测系统相比，YOLO 产生更多的定位误差，但不太可能预测背景上的误报。最后，YOLO 学习对象的非常一般的表示。它比其他检测方法（包括 DPM 和 R-CNN）在从自然图像推广到其他领域（如图稿）时更胜一筹。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/4/1.jpg?raw=true)\n",
    "\n",
    "人们看一眼图像，就立即知道图像中有哪些物体，在哪里以及它们如何相互作用。人类的视觉系统是快速和准确的，使我们能够执行复杂的任务，例如驾驶时几乎没有意识的想法。用于物体检测的快速而准确的算法将允许计算机在没有专门传感器的情况下驾驶汽车，使辅助设备能够向人类用户传达实时的场景信息，并释放一般用途的潜力，响应机器人系统。  \n",
    "\n",
    "当前的检测系统将分类器重新用于执行检测。为了检测对象，这些系统为该对象提供一个分类器，并在不同的位置评估它，并在测试图像中进行缩放。像可变形零件模型（DPM）这样的系统使用滑动窗口方法，其中分类器在整个图像上均匀间隔的位置运行[10]。  \n",
    "\n",
    "最近的方法，如 R-CNN 使用区域建议方法首先在图像中生成潜在的边界框，然后在这些建议的框上运行分类器。分类后，后处理用于细化边界框，消除重复检测，并根据场景中的其他对象重新定位框[13]。这些复杂的管道很慢并且难以优化，因为每个单独的组件都必须单独进行训练。  \n",
    "\n",
    "我们将对象检测重新设计为单一回归问题，从图像像素到边界框坐标和类概率。使用我们的系统，您只在图像上看一次（YOLO），就可以预测出现物体的位置和位置。  \n",
    "\n",
    "YOLO非常简单：参见图1。单个卷积网络可同时预测多个边界框和类概率。YOLO 在全图像上训练并直接优化检测性能。这种统一的模型与传统的物体检测方法相比有几个优点。  \n",
    "\n",
    "首先，YOLO速度非常快。由于我们将检测视为回归问题，因此我们不需要复杂的管道。我们只是在测试的时候在一幅新图像上运行我们的神经网络来预测检测结果。我们的基础网络在 Titan X GPU 上没有批处理情况下，以每秒 45 帧的速度运行，而快速版本运行速度超过 150 fps。这意味着我们可以在不到25 毫秒的延迟时间内实时处理流媒体视频。此外，YOLO 实现了其他实时系统平均精度的两倍以上。有关我们的系统在网络摄像头上实时运行的演示，请参阅我们的项目网页：http://pjreddie.com/yolo/。  \n",
    "\n",
    "其次，在进行预测时，YOLO 会在全局范围内对图像进行推理。与基于滑动窗口和区域提议的技术不同，YOLO 在训练和测试期间看到整个图像，因此它隐式地编码关于类的上下文信息以及它们的外观。Fast R-CNN，一种顶级的检测方法[14]，由于无法看到较大的上下文，因此在图像中出现对象的背景补丁。 与 Fast R-CNN 相比，YOLO 的背景错误数量少了一半。  \n",
    "\n",
    "第三，YOLO 学习物体的一般化表示。在对自然图像进行训练并进行测试时，YOLO 大幅优于 DPM 和 R-CNN 等顶级检测方法。由于 YOLO 具有高度概括性，因此在应用于新域或意外输入时不太可能发生故障。  \n",
    "\n",
    "YOLO 在准确性方面仍落后于最先进的检测系统。虽然它可以快速识别图像中的物体，但它正努力精确定位某些物体，尤其是小物体。我们在实验中进一步检查了这些折衷。  \n",
    "\n",
    "我们所有的训练和测试代码都是开源的。各种预训练模型也可以下载。  \n",
    "\n",
    "## 2. Unified Detection  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/4/2.jpg?raw=true)\n",
    "\n",
    "我们将对象检测的分开的组件集成到单个神经网络中。我们的网络使用整个图像的特征来预测每个边界框。它还同时预测图像中所有类的所有边界框。这意味着我们的网络在全局范围内关于整个图像和图像中的所有对象进行推理。YOLO 设计可实现端到端训练和实时速度，同时保持较高的平均精度。  \n",
    "\n",
    "我们的系统将输入图像划分为 S×S 网格。如果对象的中心落入网格单元格中，则该网格单元格负责检测该对象。  \n",
    "\n",
    "每个网格单元预测这些边界框 B 和置信度分数。这些置信度分数反映了该模型对盒子是否包含对象的信心，以及盒子预测的定位准确度。形式上，我们将置信度定义为 $Pr(Object) * IOU^{truth}_{pred}$。如果该单元格中不存在对象，则置信度分数应为零。否则，我们希望置信度分数等于预测框与 ground truth 之间的 intersection over union（IOU）。  \n",
    "\n",
    "每个边界框由 5 个预测组成：x，y，w，h 和置信度。（x，y）坐标表示相对于网格单元边界的框的中心。宽度和高度是相对于整个图像预测的。最后，置信度预测表示预测框与任何 ground truth 之间的 IOU。  \n",
    "\n",
    "每个网格单元还预测 C 类条件概率 $Pr(Class_i | Object)$。这些概率取决于包含对象的网格单元。我们只预测每个网格单元的一组类别概率，而不管箱子的数量是多少。  \n",
    "\n",
    "在测试时间，我们将条件类概率和单个盒子置信度预测相乘，  \n",
    "\n",
    "$pr(Class_i|Object) * Pr(Object) * IOU_{pred}^{truth} = Pr(Class_i) * IOU_{pred}^{truth} \\tag{1}$\n",
    "\n",
    "这为我们提供了每个箱子的类别特定的置信度分数。这些分数编码该类出现在盒子中的概率以及这个边界框有多适合该物体。  \n",
    "\n",
    "为了评估 PASCAL VOC 上的 YOLO，我们使用 S = 7，B = 2。PASCAL VOC 有 20 个标记类别，因此 C = 20。我们的最终预测是 7×7×30 张量。  \n",
    "\n",
    "### 2.1. Network Design  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/4/3.jpg?raw=true)\n",
    "\n",
    "我们将卷积神经网络作为此模型实施并在 PASCAL VOC 检测数据集上进行评估[9]。网络的初始卷积层从图像中提取特征，而全连接层预测输出概率和坐标。  \n",
    "\n",
    "我们的网络架构受到 GoogLeNet 图像分类模型的启发[34]。网络有 24 个卷积层，其次是 2 个全连接层。与 GoogLeNet 使用的 inception 模块不同，我们简单地使用 1×1 的 reduction 层，然后是 3×3 的卷积层，这与 Lin 等[22]相似。完整的网络如图3 所示。  \n",
    "\n",
    "我们还训练快速版本的YOLO，旨在发现快速对象检测的边界。Fast YOLO 使用较少卷积层（9 而不是 24）的神经网络，并在这些层中使用较少的滤波器。 除了网络规模之外，YOLO 和 Fast YOLO 之间的所有训练和测试参数都相同。  \n",
    "\n",
    "我们网络的最终输出是 7×7×30 张量预测。  \n",
    "\n",
    "### 2.2. Training  \n",
    "\n",
    "我们在 ImageNet 1000-class 数据集上预训练我们的卷积层[30]。对于预训练，我们使用图3 中的前 20 个卷积层，接着是平均池化层和全连接层。我们对这个网络进行了大约一周的训练，并且在 ImageNet 2012 验证集上获得了 88% 的 single crop top-5 accuracy，堪比 Caffe Model Zoo 中的 GoogLeNet 模型[24]。我们使用 Darknet 框架进行所有训练和推理[26]。  \n",
    "\n",
    "然后，我们将模型转换来执行检测。Ren 等人表明，将卷积层和连接层添加到预训练网络可以提高性能[29]。按照他们的例子，我们添加四个卷积层和两个全连接层，并且随机初始化权重。检测通常需要细粒度的视觉信息，因此我们将网络的输入分辨率从 224×224 提高到 448×448。  \n",
    "\n",
    "我们的最后一层预测类别概率和边界框坐标。我们通过图像的宽度和高度来规范边界框的宽度和高度，使它们落在 0 和 1 之间。我们将边界框 x 和 y 坐标参数化为特定网格单元位置的偏移量，使它们也在 0 和 1 之间。  \n",
    "\n",
    "我们对最后一层使用线性激活函数，而其他所有层使用以下 leaky rectified linear activation：  \n",
    "\n",
    "$\\phi(x) = \\begin{cases} x, \\ \\text{if x > 0} \\\\ 0.1x, \\ \\text{otherwise} \\end{cases} \\tag{2}$\n",
    "\n",
    "我们优化了模型输出中的平方和误差。我们使用平方和误差，因为它很容易进行优化，但它并不完全符合我们最大化平均精度的目标。它对分类错误进行同等权重的定位，可能并不理想。而且，在每个图像中，许多网格单元不包含任何对象。这将这些单元格的“置信度”分数推向零，通常压倒包含对象的单元格的梯度。这可能导致模型不稳定，从而导致训练早期发生分歧。  \n",
    "\n",
    "为了弥补这一点，我们增加了边界框坐标预测的损失，并减少了不包含对象的框的置信度预测的损失。我们使用两个参数 $\\lambda_{coord}$ 和 $\\lambda_{noobj}$ 来实现这一点。我们设定 $\\lambda_{coord} = 5$ 和 $\\lambda_{noobj} = 0.5$。  \n",
    "\n",
    "平方和误差也同样对大盒子和小盒子中的误差进行加权。我们的错误指标应该反映出，大箱子的小偏差比小箱子小很多。为了部分解决这个问题，我们直接预测边界框宽度和高度的平方根，而不是宽度和高度。  \n",
    "\n",
    "YOLO 预测每个网格单元的多个边界框。在训练时，我们只需要一个边界框预测器对每个对象负责。我们将一个预测变量指定为“负责任”，以根据哪个预测与 ground truth 具有最高的 IOU 来预测目标。这导致了边界框预测器之间的 specialization。每个预测变量可以更好地预测某些尺寸，纵横比或对象类别，从而改善整体召回率。  \n",
    "\n",
    "在训练期间，我们优化以下多部分损失函数  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/4/4.jpg?raw=true)\n",
    "\n",
    "其中，$l^{obj}_i$ 表示对象是否出现在单元格 $i$ 中，并且 $l^{obj}_{ij}$ 表示单元格 i 中的第 j 个边界框预测器对于该预测是“负责任的”。  \n",
    "\n",
    "请注意，如果对象存在于该网格单元中，则损失函数仅惩罚分类错误。如果该预测器对 ground truth box“负责”（该网格单元中的任何预测器具有的最高 IOU），则它也仅惩罚边界框坐标错误。  \n",
    "\n",
    "我们在 PASCAL VOC 2007 和 2012 的训练和验证数据集上对网络进行了大约 135 个 epochs 的训练。在 2012 上进行测试时，我们训练数据还包括 VOC 2007 测试数据。在整个训练过程中，我们使用 64 的 batch，0.9 的动量和 0.0005 的衰减。  \n",
    "\n",
    "我们的学习率计划如下：对于第一个 epochs，我们慢慢地将学习率从 10-3 提高到 10-2。如果我们从高学习率开始，我们的模型通常会由于不稳定的梯度而发散。我们继续以 10-2 进行 75 个 epochs 的训练，然后以 10-3 进行 30 个epochs的训练，最后以 10-4 进行 30 个 epochs 的训练。  \n",
    "\n",
    "为避免过拟合，我们使用 dropout 和额外的数据增强。在第一个连接层之后，rate = .5 的 dropout 层防止了层之间的协调[18]。对于数据增强，我们引入随机缩放和 translations 高达20%的原始图像大小。我们还在 HSV 色彩空间中随机调整图像的曝光和饱和度达 1.5 倍。  \n",
    "\n",
    "### 2.3. Inference  \n",
    "\n",
    "就像在训练中一样，预测测试图像的检测只需要一个网络评估。在 PASCAL VOC 上，网络预测每个图像的 98 个边界框和每个框的类别概率。YOLO 在测试时非常快，因为它只需要一个网络评估，而不像基于分类器的方法。  \n",
    "\n",
    "网格设计强化了边界框预测中的空间多样性。通常很清楚一个对象落到哪个网格中，网络仅为每个对象预测一个盒子。但是，一些大的物体或 multiple cells 边界旁边的物体可以很好地被 multiple cells 定位。非极大值抑制可用于修正这些多重检测。对于 R-CNN 或 DPM 而言，对性能不是至关重要的，非极大值抑制在 mAP 中增加2-3%。  \n",
    "\n",
    "### 2.4. Limitations of YOLO  \n",
    "\n",
    "YOLO 对边界框预测附加强空间约束，因为每个网格单元只能预测两个方框，并且只能有一个类。这种空间约束限制了我们的模型可以预测的附近物体的数量。我们的模型与群体中出现的小物体（例如鸟群）发生斗争。  \n",
    "\n",
    "由于我们的模型学习从数据中预测边界框，因此它很难将对象推广到新的或不寻常的高宽比或配置。我们的模型还使用相对粗糙的特征来预测边界框，因为我们的体系结构具有来自输入图像的多个下采样层。  \n",
    "\n",
    "最后，当我们训练一个接近检测性能的损失函数时，我们的损失函数将小包围盒与大包围盒的误差相同。大箱子中的小错误通常是良性的，但小箱子中的小错误对 IOU 有更大的影响。我们的主要错误来源是不正确的定位。  \n",
    "\n",
    "## 3. Comparison to Other Detection Systems  \n",
    "\n",
    "目标检测是计算机视觉中的核心问题。检测流水线通常从输入图像（Haar，SIFT，HOG，卷积特征）提取一组鲁棒特征开始。然后，分类器或定位器用于识别特征空间中的对象。这些分类器或定位器在整个图像上或在图像中某些区域子集上以滑动窗口方式运行。我们将 YOLO 检测系统与几种顶级检测框架进行比较，突出了关键的相似性和差异。  \n",
    "\n",
    "**可变形零件模型** 可变形零件模型（DPM）使用滑动窗口方法进行物体检测[10]。DPM 使用不相交的管道来提取静态特征，对区域进行分类，预测高评分区域的边界框等。我们的系统用单个卷积神经网络替换所有这些分离的部分。网络同时执行特征提取，边界框预测，非极大值抑制和上下文推理。不像静态特征，网络在线训练特征并优化检测任务。我们的统一架构比 DPM 更快，更精确。  \n",
    "\n",
    "**R-CNN** R-CNN 及其变体使用区域提议而不是滑动窗口来查找图像中的对象。选择性搜索[35]生成潜在的边界框，卷积网络提取特征，SVM 对框进行评分，线性模型调整边界框，非极大值抑制消除重复检测。这个复杂流水线的每个阶段都必须独立地进行精确调整，所得到的系统非常缓慢，在测试时间每个图像需要超过40秒[14]。  \n",
    "\n",
    "YOLO 与 R-CNN 有一些相似之处。每个网格单元提出潜在的边界框并使用卷积特征对这些框进行评分。然而，我们的系统对网格单元提案施加空间限制，这有助于缓解对同一对象的多次检测。我们的系统还提出了更少的边界框，每张图像只有 98 个，而选择性搜索则只有约 2000 个。最后，我们的系统将这些单独的组件组合成一个单一的，共同优化的模型。  \n",
    "\n",
    "**其他快速检测器** Fast 和 Faster R-CNN 专注于通过共享计算并使用神经网络来提出区域而不是选择性搜索来加速 R-CNN 框架。虽然它们提供了比 R-CNN 更快的速度和准确度改进，但两者仍然达不到实时性能。  \n",
    "\n",
    "许多研究工作集中于加快 DPM 管道。他们加速 HOG 计算，使用级联，并将计算推送到 GPU。但是，实际上只有 30Hz 的 DPM [31]实际运行。  \n",
    "\n",
    "YOLO 不是试图优化大型检测管道的单个组件，而是完全抛出管道，并且设计速度很快。  \n",
    "\n",
    "像人脸或行人等单个类别的检测器可以高度优化，因为他们处理更少的变化[37]。YOLO 是一种通用的检测器，可以同时学习检测各种物体。  \n",
    "\n",
    "**Deep MultiBox** 与 R-CNN 不同，Szegedy 等人训练一个卷积神经网络来预测感兴趣的区域[8]，而不是使用选择性搜索。MultiBox 还可以通过用单个类别预测替换置信度预测来执行单个对象检测。但是，MultiBox 无法执行通用对象检测，并且仍然只是更大检测流水线中的一部分，需要进一步对图像 patch 进行分类。YOLO 和 MultiBox 都使用卷积网络来预测图像中的边界框，但 YOLO 是一个完整的检测系统。  \n",
    "\n",
    "**OverFeat** Sermanet 等人训练一个卷积神经网络来执行定位并使定位器执行检测[32]。OverFeat 高效地执行滑动窗口检测，但它仍然是一个不相交的系统。OverFeat 优化定位，而不是检测性能。与 DPM 一样，定位器在进行预测时只能看到局部信息。OverFeat 无法推断全局上下文，因此需要大量的后处理才能生成一致的检测结果。  \n",
    "\n",
    "**MultiGrasp** 我们的工作在设计上与 Redmon 等人[27]的 grasp 检测相似。我们的网格边界框预测方法基于 MultiGrasp 系统进行回归分析。然而，grasp 检测比对象检测更简单。MultiGrasp 只需要为包含一个对象的图像预测一个可抓取区域。它不必估计对象的大小，位置或边界或预测它的类，只找到适合抓取的区域。YOLO 预测图像中多个类的多个对象的边界框和类概率。  \n",
    "\n",
    "## 4. Experiments  \n",
    "\n",
    "首先，我们将 YOLO 与 PASCAL VOC 2007 上的其他实时检测系统进行比较。要了解 YOLO 和 R-CNN 变体之间的差异，我们将探索由 YOLO 和 Fast R-CNN 在 VOC 2007 产生的错误，我们显示 YOLO 可用于重新调整 Fast R-CNN 检测并减少背景误报带来的错误，从而显着提升性能。我们还展示了 VOC 2012 的结果，并将 mAP 与当前最先进的方法进行比较。最后，我们展示 YOLO 比其他检测器在两个图形数据集上的更好地推广到新域。  \n",
    "\n",
    "### 4.1. Comparison to Other Real-Time Systems  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/4/5.jpg?raw=true)\n",
    "\n",
    "目标检测方面的许多研究工作都集中在快速制定标准检测流水线上。然而，只有Sadeghi 等人实际产生了一个实时运行的检测系统（每秒 30 帧或更好）[31]。我们将 YOLO 与其在 30Hz 或 100Hz 下运行的 DPM 的 GPU 实现进行比较。虽然其他努力没有达到实时里程碑，我们还会比较它们的相对 mAP 和速度，以检查物体检测系统中可用的精度-性能折衷。  \n",
    "\n",
    "Fast YOLO 是 PASCAL 上最快的对象检测方法；据我们所知，它是现存最快的物体探测器。有了 52.7% 的 mAP，它比以前的实时检测工作的准确率高出一倍以上。YOLO 将 mAP 推到63.4%，同时仍保持实时性能。  \n",
    "\n",
    "我们还使用 VGG-16 训练 YOLO。这个模型比 YOLO 更准确，但也显着更慢。它与依赖于 VGG-16 的其他检测系统相比是有用的，但由于它比实时更慢，所以本文的其他部分将重点放在我们更快的模型上。  \n",
    "\n",
    "Fastest DPM 可以在不牺牲太多 mAP 的情况下有效加速 DPM，但仍然会将实时性能降低2倍[38]。与神经网络方法相比，DPM 的检测精度也受到限制。  \n",
    "\n",
    "R-CNN 减去 R 用静态 bounding box proposals 取代选择性搜索[20]。虽然速度比 R-CNN 快得多，但它仍然缺乏实时性，并且由于没有好的提案而精确度受到严重的影响。  \n",
    "\n",
    "Fast R-CNN 加速了 R-CNN 的分类阶段，但它仍然依赖于选择性搜索，每个图像需要大约 2 秒才能生成边界框提议。因此它具有很高的 mAP，但在 0.5 fps 时仍然远未实时。  \n",
    "\n",
    "最近 Faster R-CNN 用神经网络替代了选择性搜索来提出边界框，类似于 Szegedy 等人的工作。在我们的测试中，他们最准确的模型达到了 7 fps，而较小的，不太准确的模型以 18 fps 运行。Faster R-CNN 的 VGG-16 版本比 YOLO 高出 10 mAP，但比 YOLO 慢 6 倍。ZeilerFergus Faster R-CNN 只比 YOLO 慢 2.5倍，但也不太准确。  \n",
    "\n",
    "### 4.2. VOC 2007 Error Analysis  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/4/6.jpg?raw=true)\n",
    "\n",
    "为了进一步检查 YOLO 和最先进的探测器之间的差异，我们详细分析了 VOC 2007 的结果。我们将 YOLO 与 Fast RCNN 进行比较，因为 Fast R-CNN 是PASCAL 上性能最高的探测器之一，它的检测结果是公开的。  \n",
    "\n",
    "我们使用 Hoiem 等人的方法和工具。对于测试时的每个类别，我们看看该类别的前 N 个预测。每个预测都是正确的，或者根据错误的类型进行分类：  \n",
    "\n",
    "- Correct: correct class and IOU > .5\n",
    "- Localization: correct class, .1 < IOU < .5\n",
    "- Similar: class is similar, IOU > .1\n",
    "- Other: class is wrong, IOU > .1\n",
    "- Background: IOU < .1 for any object\n",
    "\n",
    "图4 显示了所有 20 个类别的平均错误类型的细分。  \n",
    "\n",
    "YOLO 努力正确地定位对象。定位错误占所有 YOLO 错误的总和超过所有其他来源的总和。Fast R-CNN 使定位错误更少，但背景错误更多。其最高检测结果中有 13.6% 是误报，不包含任何对象。Fast R-CNN 将背景作为检测结果的可能性比 YOLO 高 3 倍。  \n",
    "\n",
    "### 4.3. Combining Fast R-CNN and YOLO  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/4/7.jpg?raw=true)\n",
    "\n",
    "YOLO 的背景错误比 Fast R-CNN 少得多。通过使用 YOLO 消除 Fast R-CNN 的背景检测，我们获得了显着的性能提升。对于 R-CNN 预测的每个边界框，我们检查 YOLO 是否预测了一个类似的框。如果确实如此，那么我们会根据 YOLO 预测的概率和两个框之间的重叠情况给出预测。  \n",
    "\n",
    "最好的 Fast R-CNN 模型在 VOC 2007 测试集中达到了 71.8% 的 mAP。当与 YOLO 合并时，其 mAP 增加了 3.2% 至 75.0%。我们还尝试将最好的 Fast R-CNN 模型与其他几个版本的 Fast R-CNN 相结合。这些集合的平均增幅在 0.3% 至 0.6% 之间，详情见表2。  \n",
    "\n",
    "YOLO 的 boost 不仅仅是模型集成的副产品，因为组合不同版本的 Fast R-CNN 几乎没有什么好处。相反，正是因为 YOLO 在测试时出现了各种各样的错误，所以它在提高 Fast R-CNN 的性能方面非常有效。  \n",
    "\n",
    "不幸的是，这种组合不会从 YOLO 的速度中受益，因为我们分别运行每个模型，然后合并结果。但是，由于 YOLO 速度如此之快，与 Fast R-CNN 相比，它不会增加任何显着的计算时间。  \n",
    "\n",
    "### 4.4. VOC 2012 Results  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/4/8.jpg?raw=true)\n",
    "\n",
    "在 VOC 2012 测试集中，YOLO 评分为57.9%。这比现有技术水平低，使用 VGG-16 更接近原始 R-CNN，参见表3。我们的系统与其最接近的竞争对手相比，在小物体上效果不好。在瓶子，羊，电视/监视器等类别上，YOLO 得分比 R-CNN 或 Feature Edit 低 8-10%。然而，在其他类别，如猫和火车 YOLO 实现更高的性能。  \n",
    "\n",
    "我们的组合 Fast R-CNN + YOLO 模型是性能最高的检测方法之一。Fast R-CNN 与 YOLO 的组合提高了 2.3%，在公共排行榜上提升了 5 个位置。  \n",
    "\n",
    "### 4.5. Generalizability: Person Detection in Artwork  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/4/9.jpg?raw=true)\n",
    "\n",
    "用于对象检测的学术数据集从同一分布中绘制训练和测试数据。在现实世界的应用中，很难预测所有可能的用例，而测试数据可能与系统在之前看到的不同[3]。我们将 YOLO 与 Picasso 数据集[12]和人物艺术数据集[3]中的其他检测系统进行了比较，这两个数据集用于在 artwork 中检测人员。  \n",
    "\n",
    "图5 显示了 YOLO 和其他检测方法之间的比较性能。作为参考，我们用在 VOC 2007 数据集上训练的模型人员检测的 AP。毕加索模型在 VOC 2012 数据集上训练，而 People-Art 则在 VOC 2010 上训练。  \n",
    "\n",
    "R-CNN 在 VOC 2007 上有很高的 AP 值。然而，当应用于艺术品时，R-CNN 显着下降。R-CNN 使用选择性搜索来调整自然图像的边界框提案。R-CNN 中的分类器步骤只能看到小区域，需要很好的建议。  \n",
    "\n",
    "DPM 在应用于艺术品时可以很好地维护其 AP。之前的工作认为 DPM 表现良好，因为它具有强大的物体形状和布局空间模型。虽然 DPM 不会像 R-CNN 那样退化，但它从较低的 AP 开始。  \n",
    "\n",
    "YOLO 在 VOC 2007 上表现出色，其应用于艺术品时其 AP 降低程度低于其他方法。与 DPM 一样，YOLO 模拟对象的大小和形状，以及对象之间的关系和对象通常出现的位置之间的关系。艺术品和自然图像在像素级别上有很大不同，但它们在物体的大小和形状方面相似，因此 YOLO 仍然可以预测好的边界框和检测结果。  \n",
    "\n",
    "## 5. Real-Time Detection In The Wild  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/4/10.jpg?raw=true)\n",
    "\n",
    "YOLO 是一款快速，精确的物体检测器，非常适合计算机视觉应用。我们将 YOLO 连接到网络摄像头，并验证它是否保持实时性能，包括从摄像头获取图像并显示检测结果的时间。  \n",
    "\n",
    "由此产生的系统是互动的和参与的。虽然 YOLO 单独处理图像，但当连接到网络摄像头时，它的功能类似于跟踪系统，可在物体四处移动并在外观上发生变化时检测物体。系统演示和源代码可在我们的项目网站上找到：http://pjreddie.com/yolo/。  \n",
    "\n",
    "## 6. Conclusion  \n",
    "\n",
    "我们介绍 YOLO，一种用于物体检测的统一模型。我们的模型构造简单，可以直接在完整图像上训练。与基于分类器的方法不同，YOLO 是通过与检测性能直接对应的损失函数进行训练的，并且整个模型是共同训练的。  \n",
    "\n",
    "Fast YOLO 是文献中最快的通用对象检测器，YOLO 推动实时对象检测的最新技术。YOLO 还很好地推广到新领域，使其成为依赖快速，强大物体检测的应用的理想选择。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO9000: Better, Faster, Stronger  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "我们推出了 YOLO9000，这是一款先进的实时物体检测系统，可检测 9000 多种物体类别。首先，我们提出对 YOLO 检测方法的各种改进，既有新颖性，也有以前的工作。改进后的模型 YOLOv2 在 PASCAL VOC 和 COCO 等标准检测任务中处于技术领先地位。使用一种新颖的多尺度训练方法，同样的 YOLOv2 模型可以以不同的尺寸运行，在速度和准确性之间提供了一个简单的折衷。在 67 FPS 时，YOLOv2 在 VOC 2007 上获得了 76.8 mAP。在 40 FPS 时，YOLOv2 获得了 78.6 mAP，超越了最先进的方法，超过采用 ResNet 的 Faster RCNN 和 SSD，同时运行速度更快。最后我们提出一种联合训练对象检测和分类的方法。使用这种方法，我们在 COCO 检测数据集和 ImageNet 分类数据集上同时训练 YOLO9000。我们的联合训练使 YOLO9000 能够预测未标注检测数据的对象类别的检测结果。我们在 ImageNet 检测任务上验证了我们的方法。YOLO9000 在 ImageNet 检测验证集上获得 19.7 mAP，尽管只有 200 个类中的 44 个具有检测数据。在没有 COCO 的 156 类上，YOLO9000 获得 16.0 mAP。但 YOLO 可以检测超过 200 个类；它可以实时预测超过 9000 个不同的对象类别的检测结果。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/5/1.jpg?raw=true)\n",
    "\n",
    "通用对象检测应该快速，准确，并且能够识别各种各样的对象。自引入神经网络以来，检测框架变得越来越快速和准确。但是，大多数检测方法仍然局限于一小组对象。  \n",
    "\n",
    "与分类和序列标记等其他任务的数据集相比，当前的对象检测数据集是有限的。最常见的检测数据集包含成千上万到数十万个具有几十到几百个标签的图像。分类数据集拥有数以百万计的图像，数十万甚至数十万个类别。  \n",
    "\n",
    "我们希望检测能够扩展到对象分类的级别。但是，对检测任务进行标注要比分类任务的标注简单的多。因此，我们不太可能在近期内看到与分类数据集具有相同尺度的检测数据集。  \n",
    "\n",
    "我们提出了一种新方法来利用我们已有的大量分类数据，并将其用于扩大当前检测系统的范围。我们的方法使用对象分类的分层视图，使我们可以将不同的数据集组合在一起。  \n",
    "\n",
    "我们还提出了一种联合训练算法，它允许我们在检测和分类数据上训练物体检测器。我们的方法利用标记检测图像来学习精确定位对象，同时使用分类图像来增加词汇量和健壮性。  \n",
    "\n",
    "使用这种方法，我们训练 YOLO9000，这是一种实时对象检测器，可以检测超过 9000 种不同的对象类别。首先，我们改进 YOLO 基础检测系统，生产出最先进的实时检测器 YOLOv2。然后，我们使用我们的数据集组合方法和联合训练算法来训练来自 ImageNet 的 9000 多个类以及 COCO 的检测数据的模型。  \n",
    "\n",
    "我们所有的代码和预先训练好的模型都可以在 http://pjreddie.com/yolo9000/ 在线获得。  \n",
    "\n",
    "## 2. Better  \n",
    "\n",
    "与最先进的检测系统相比，YOLO 存在各种各样的缺点。YOLO 与 Fast R-CNN 相比的误差分析表明，YOLO 产生了大量的定位错误。此外，与区域提议方法相比，YOLO 召回率相对较低。因此，我们主要关注改善召回和定位，同时保持分类准确性。  \n",
    "\n",
    "计算机视觉通常趋向于更大，更深的网络。更好的性能通常取决于训练更大的网络或将多个模型组合在一起。但是，对于 YOLOv2，我们需要一个更精确的检测器，它仍然很快。我们不是扩大我们的网络，而是简化网络，然后让表示更易于学习。我们将以往工作中的各种想法与我们自己的新颖概念汇集起来，以提高 YOLO 的表现。表2 列出了结果总结。  \n",
    "\n",
    "**Batch Normalization** 批量归一化可以显着改善收敛性，同时不需要其他形式的正则化[7]。通过在 YOLO 中的所有卷积图层上添加批量标准化，我们在 mAP 中获得了 2% 以上的改进。批量标准化也有助于规范模型。通过批量标准化，我们可以从模型中删除 dropout 而不过拟合。  \n",
    "\n",
    "**High Resolution Classifier** 所有最先进的检测方法都使用在 ImageNet 上预先训练好的分类器。从 AlexNet 开始，大多数分类器对小于256×256 的输入图像进行操作[8]。最初的 YOLO 以 224×224 训练分类器网络，并将分辨率提高到 448 以进行检测。这意味着网络必须同时切换到学习对象检测并调整到新的输入分辨率。  \n",
    "\n",
    "对于 YOLOv2，我们首先使用 448×448 全分辨率图像上对分类网络微调 10 个周期。这给网络调整其滤波器的时间以更好地处理更高分辨率的输入。然后，我们对检测网络进行微调。这个高分辨率的分类网络使我们的 mAP 增加了近 4%。  \n",
    "\n",
    "**Convolutional With Anchor Boxes** YOLO 直接使用卷积特征提取器顶部的全连接层来预测边界框的坐标。Faster R-CNN 使用手工选取的先验预测边界框[15]。Faster R-CNN 中的区域提议网络（RPN）仅使用卷积层来预测锚箱的偏移和置信度。由于预测层是卷积的，所以 RPN 在特征映射中的每个位置预测这些偏移。预测偏移而不是坐标可以简化问题并使网络更易于学习。  \n",
    "\n",
    "我们从 YOLO 中移除全连接层，并使用锚箱来预测边界框。首先我们消除一个池化层，以使网络卷积层的输出具有更高的分辨率。我们还缩小网络，使其在416 大小的输入图像上运行，而不是 448×448。我们这样做是因为我们需要在我们的特征映射中有奇数个位置，因此只有一个中心单元。对象，尤其是大对象，往往占据图像的中心，所以最好在中心拥有一个位置来预测这些对象，而不是四个位置都在附近。YOLO 的卷积层将图像下采样 32 倍，所以通过使用 416 的输入图像，我们得到 13×13 的输出特征图。  \n",
    "\n",
    "当我们移动到锚箱时，我们也将类别预测机制与空间位置分离开来，而是预测每个锚箱的类和对象。在 YOLO 之后，对象性预测仍然预测了 ground truth 和提议框的 IOU，并且类别预测预测了该类别的条件概率。  \n",
    "\n",
    "使用锚箱，我们会得到精确度的小幅下降。YOLO 每个图像仅预测 98 个盒子，但使用锚盒我们的模型预测超过一千个。如果没有锚箱，我们的中间模型将获得 69.5 mAP，召回率为 81%。使用锚箱，我们的模型获得了 69.2 mAP，召回率为 88%。 尽管 mAP 下降，但召回率的增加意味着我们的模型有更大的提升空间。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/5/2.jpg?raw=true)\n",
    "\n",
    "**Dimension Clusters** 与 YOLO 一起使用时，我们遇到了两个与锚盒有关的问题。首先是盒子尺寸是手工挑选的。网络可以学习适当调整方框，但是如果我们选择更好的网络先验，我们可以让网络更容易学习预测好的检测结果。  \n",
    "\n",
    "我们不用手工先验选择，而是在训练集边界框上运行 k 均值聚类，以自动找到良好的先验。如果我们使用具有欧几里得距离的标准 k-means，那么较大的盒子比较小的盒子产生更多的误差。然而，我们真正想要的是先进的，导致良好的 IOU 分数，这是独立于框的大小。因此对于我们的距离度量我们使用：  \n",
    "\n",
    "$d(box, centroid) = 1 − IOU(box, centroid)$\n",
    "\n",
    "我们对 k 的各种值运行 k均值，并绘制最接近质心的平均值 IOU，见图2。我们选择 k = 5 作为模型复杂度和高召回率之间的良好折衷。cluster 质心与手工选取的锚盒显着不同。有更少的短，宽盒子和更高，更薄的盒子。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/5/5.jpg?raw=true)\n",
    "\n",
    "我们将平均 IOU 与我们的聚类策略中的最接近的先验以及表1 中的手工选取的锚箱进行比较。仅有 5 个先验质心的性能类似于 9 个锚箱，平均 IOU 为61.0，而 9 个锚箱为 60.9。如果我们使用 9 个质心，我们会看到更高的平均 IOU。这表明使用 k-means 生成我们的边界框可以更好地表示模型并使其更容易学习。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/5/3.jpg?raw=true)\n",
    "\n",
    "**Direct location prediction** 当在 YOLO 中使用锚箱时，我们会遇到第二个问题：模型不稳定，尤其是在早期迭代过程中。大多数不稳定来自于预测盒子的（x，y）位置。在区域提议网络中，网络预测值 tx 和 ty 以及（x，y）中心坐标的计算公式如下：  \n",
    "\n",
    "$x = (t_x * w_a) − x_a, \\ y = (t_y * h_a) − y_a$\n",
    "\n",
    "例如，对 tx = 1 的预测会使该框向右移动锚盒的宽度，预测 tx = -1 会将其向左移动相同的量。  \n",
    "\n",
    "这个公式是不受限制的，所以任何锚箱都可以在图像中的任何一点结束，而不管箱子在什么位置预测。随机初始化模型需要很长时间才能稳定以预测合理的偏移。  \n",
    "\n",
    "我们不是预测偏移量，而是遵循 YOLO 的方法并预测相对于网格单元位置的位置坐标。这使得 ground truth 限定在0到1之间。我们使用 logistic 激活来限制网络的预测落在这个范围内。  \n",
    "\n",
    "网络预测输出特征映射中每个单元的 5 个边界框。网络为每个边界框预测 5 个坐标，tx，ty，tw，th 和 to。如果单元格从图像的左上角偏移了（cx，cy）并且之前的边界框具有宽度和高度 pw，ph，则预测对应于：  \n",
    "\n",
    "$b_x = \\sigma(t_x) + c_x \\\\ b_y = \\sigma(t_y) + c_y, \\\\ b_w = p_we^{t_w}, \\\\ b_h = p_h e^{t_h}$\n",
    "\n",
    "由于我们限制位置预测，因此参数更容易学习，从而使网络更加稳定。使用维度聚类并直接预测边界框中心位置，可以使 YOLO 比锚框的版本提高近 5%。  \n",
    "\n",
    "**Fine-Grained Features** 这个修改过的 YOLO 预测 13×13 特征图上的检测结果。虽然这对于大型物体是足够的，但它可以从用于定位较小物体的更细粒度特征中受益。Faster R-CNN 和 SSD 都在网络中的多尺度特征图上运行提议网络，以获得一系列的提议。我们采用不同的方法，只需添加一个直通层，从上一层中以 26×26 分辨率提取特征。  \n",
    "\n",
    "直通层通过将相邻特征堆叠到不同的通道而不是空间位置来连接较高分辨率的特征和低分辨率特征，类似于 ResNet 中的 identity mappings。这将26×26×512 特征图变为 13×13×2048 特征图，其可以与原始特征级联。我们的检测器运行在这个扩展特征图的顶部，以便它可以访问细粒度的特征。 这使性能提高了1%。  \n",
    "\n",
    "**Multi-Scale Training** 原来的 YOLO 使用 448×448 的输入分辨率。通过添加锚箱，我们将分辨率更改为 416×416。但是，由于我们的模型仅使用卷积层和汇聚层，因此可以实时调整大小。我们希望 YOLOv2 能够在不同尺寸的图像上运行，因此我们可以在模型中进行训练。  \n",
    "\n",
    "我们不改变输入图像大小，而是每隔几次迭代就改变一次网络。每 10 个批次我们的网络会随机选择一个新的图像尺寸大小。由于我们的模型缩减了 32 倍，所以我们从 32 的倍数中抽取：{320,352，...，608}。因此，最小的选项是 320×320，最大的是 608×608。我们调整网络的尺寸并继续训练。  \n",
    "\n",
    "这个策略迫使网络学习如何在各种输入维度上做好预测。这意味着相同的网络可以预测不同分辨率下的检测结果。网络在较小的尺寸下运行速度更快，因此YOLOv2 可以在速度和准确性之间轻松进行折衷。  \n",
    "\n",
    "在低分辨率下，YOLOv2 作为一种便宜，相当准确的检测器工作。在 288×288 大小下，它的运行速度超过 90 FPS，而 MAP 几乎与 Fast R-CNN 一样好。 这使其成为小型 GPU，高帧率视频或多视频流的理想选择。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/5/6.jpg?raw=true)\n",
    "\n",
    "在高分辨率下 YOLOv2 是一款先进的检测器，在 VOC 2007 上具有 78.6 mAP 的性能，同时仍以高于实时速度运行。请参阅表3，了解 YOLOv2 与 VOC 2007 上的其他框架的比较。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/5/4.jpg?raw=true)\n",
    "\n",
    "**Further Experiments** 我们对 YOLOv2 进行 VOC 2012 检测训练。表4 显示了 YOLOv2 与其他最先进的检测系统的比较性能。YOLOv运行速度远高于竞争方法，达到 73.4 mAP。我们还训练 COCO，并与表5 中的其他方法进行比较。在 VOC 度量（IOU = 0.5）上，YOLOv2 获得 44.0 mAP，与 SSD 和 Faster R-CNN 相当。 \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/5/7.jpg?raw=true)\n",
    "\n",
    "## 3. Faster  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/5/8.jpg?raw=true)\n",
    "\n",
    "我们希望检测结果准确，但我们也希望检测速度更快。大多数用于检测的应用程序（如机器人或自驾车）都依赖于低延迟预测。为了最大限度地提高性能，我们从头开始设计 YOLOv2。  \n",
    "\n",
    "大多数检测框架依赖于 VGG-16 作为基本特征提取器[17]。VGG-16 是一个功能强大，准确的分类网络，但它存在不必要的复杂度。以 224×224 分辨率在单个图像上进行单次传递，VGG-16 的卷积层需要306.90 亿次浮点运算。  \n",
    "\n",
    "YOLO 框架使用基于 Googlenet 架构的自定义网络[19]。这个网络比 VGG-16 更快，一次前向只有 85.2 亿次运行。然而，它的准确性略低于 VGG-16。对于 224×224 的 single-crop，YOLO 定制模型的ImageNet的 top-5 准确率为 88.0%，而 VGG-16 则为 90.0%。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/5/9.jpg?raw=true)\n",
    "\n",
    "**Darknet-19** 我们提出了一个新的分类模型作为 YOLOv2 的基础。我们的模型建立先前在网络设计的工作以及该领域的常识上。与 VGG 模型类似，我们大多使用 3×3 滤波器，并在每个池化步骤之后使通道数量加倍[17]。继 Network in Network（NIN）的工作之后，我们使用全局平均汇集做预测以及1×1 滤波器来压缩 3×3 卷积之间的特征表示[9]。我们使用批量归一化来稳定训练，加速收敛，并规范模型[7]。  \n",
    "\n",
    "我们的最终模型叫做 Darknet-19，它有 19 个卷积层和 5 个 Maxpool 层。Darknet-19 只需要 55.8 亿次操作来处理图像，但在 ImageNet 上实现了72.9%的 top-1 精度和 91.2% 的 top-5 精度。  \n",
    "\n",
    "**Training for classification** 我们使用 DarkNet 神经网络框架，使用随机梯度下降，初始学习率为 0.1，多项式速率衰减为 4，权重衰减为 0.0005，动量为 0.9，在标准 ImageNet 1000 类别分类数据集上对网络进行 160 个 epoch 的训练[13]。在训练过程中，我们使用标准数据增强技巧，包括随机 crop，旋转和色相，饱和度和曝光转移。  \n",
    "\n",
    "如上所述，在我们对 224×224 图像进行初始训练之后，我们对网络进行了细调（448）。为了进行这种微调，我们使用上述参数进行训练，但仅用 10 个时期，并且学习率为 10-3。在这个更高的分辨率下，我们的网络实现了 76.5% 的 top-1 精度和 93.3% 的 top-5 精度。  \n",
    "\n",
    "**Training for detection** 我们通过去除最后一个卷积层来修改这个网络，并在三个 1024 通道的 3×3 卷积层上附加需要检测输出个数的 1×1 卷积层。对于 VOC，我们预测 5 个盒子，每盒 5 个坐标，每盒 20 个类别，所以有 125 个过滤器 我们还添加了从最后的 3×3×512 层到倒数第二层卷积层的直通层，以便我们的模型可以使用细粒度特征。  \n",
    "\n",
    "我们训练网络 160 个 epoch，初始学习率为 10-3，在 60 和 90 epoch 除以 10。我们使用 0.0005 的权重衰减和 0.9 的动量。使用类似 YOLO 和 SSD 的数据增强策略。我们对 COCO 和 VOC 使用相同的训练策略。  \n",
    "\n",
    "## 4. Stronger  \n",
    "\n",
    "我们提出了一个联合训练分类和检测数据的机制。我们的方法使用用来检测的标注图像来学习边界框坐标预测和对象以及如何对常见对象进行分类等特定于检测的信息。它使用仅具有类标签的图像来扩展其可检测类别的数量。  \n",
    "\n",
    "在训练期间，我们混合来自检测和分类数据集的图像。当我们的网络看到标记为检测的图像时，我们可以根据完整的 YOLOv2 损失函数进行反向传播。当它看到分类图像时，我们只会反向传播类别特定部分中的损失。  \n",
    "\n",
    "这种方法带来了一些挑战。检测数据集只有通用对象和通用标签，如“狗”或“船”。分类数据集具有更广泛和更深的标签范围。ImageNet 拥有一百多种狗，包括“诺福克梗”，“约克夏梗”和“贝灵顿梗”。如果我们想在两个数据集上进行训练，则需要采用一致的方式来合并这些标签。  \n",
    "\n",
    "大多数分类方法使用跨所有可能类别的 softmax 层来计算最终的概率分布。使用 softmax 假定类是相互排斥的。这给数据集合并带来了问题，例如，你不希望将 ImageNet 和 COCO 结合使用，因为类“诺福克梗”和“狗”不是相互排斥的。  \n",
    "\n",
    "相反，我们可以使用多标签模型来组合不会互相排斥的数据集。这种方法忽略了我们所知道的关于数据的所有结构，例如所有的 COCO 类都是互斥的。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/5/10.jpg?raw=true)\n",
    "\n",
    "**Hierarchical classification** ImageNet 标签是从 WordNet 中提取的，这是一个结构概念及其相关性的语言数据库[12]。在 WordNet 中，“诺福克梗”和“约克夏梗”都是“猎犬”的下位词，它是一种“猎狗”，它是一种“狗”，它是一种“犬”，等等。分类为标签假设一个扁平结构，但是对于组合数据集，结构正是我们所需要的。  \n",
    "\n",
    "WordNet 的结构是有向图，而不是树，因为语言很复杂。例如，“狗”既是一种“犬”，也是一种“家养动物”，它们都是 WordNet 中的同义词。我们没有使用完整的图结构，而是通过从 ImageNet 中的概念构建分层树来简化问题。  \n",
    "\n",
    "为了构建这棵树，我们检查 ImageNet 中的视觉名词，并查看它们通过 WordNet 图到根节点的路径，在这种情况下是“物理对象”。许多 synsets 只有一条路径通过图形，所以首先我们将所有这些路径添加到我们的树中。然后，我们反复检查我们留下的概念，并尽可能少地添加生成树的路径。所以如果一个概念有两条通向根的路径，一条路径会为我们的树增加三条边，另一条路只增加一条边，我们选择较短的路径。  \n",
    "\n",
    "最终的结果是 WordTree，一个视觉概念的分层模型。为了使用 WordTree 进行分类，我们预测每个节点的条件概率，以得到 synset 的每个同义词的下位概率。例如，在“梗”节点我们预测：  \n",
    "\n",
    "$P_r(Norfolk terrier|terrier) \\\\ P_r(Yorkshire terrier|terrier) \\\\ P_r(Bedlington terrier|terrier) \\\\ ...$\n",
    "\n",
    "如果我们想要计算特定节点的绝对概率，我们只需沿着通过树到达根节点的路径并乘以条件概率即可。所以如果我们想知道一幅图片是否属于诺福克梗，我们计算：  \n",
    "\n",
    "$P_r(Norfolk terrier) = P_r(Norfolk terrier|terrier) * P_r(terrier|hunting dog) * ... * P_r(animal|physical object)$\n",
    "\n",
    "出于分类目的，我们假设图像包含一个对象：$P_r(physical object) = 1$。  \n",
    "\n",
    "为了验证这种方法，我们在使用 1000 类 ImageNet 构建的 WordTree 上训练 Darknet-19 模型。为了构建 WordTree1k，我们在所有中间节点中添加了将标签空间从 1000 扩展到 1369 的中间节点。在训练过程中，我们在树上传播 ground truth 标签，这样如果图像被标记为“诺福克梗”，它也会被标记为 “狗”和“哺乳动物”等。为了计算条件概率，我们的模型预测了 1369 个值的向量，并且我们计算了相同概念的下标的所有 sysnsets 上的 softmax，参见图5。  \n",
    "\n",
    "使用与之前相同的训练参数，我们的分层 Darknet-19 达到 71.9% 的 top-1 精度和 90.4% 的 top-5 精度。尽管增加了 369 个附加概念，并且我们的网络预测了一个树状结构，但我们的精度仅略有下降。以这种方式进行分类也有一些好处。性能会在新的或未知的对象类别上降低。例如，如果网络看到一只狗的照片，但不确定它是什么类型的狗，它仍然会高度自信地预测“狗”，但在下位数中散布的可信度较低。  \n",
    "\n",
    "该方式也适用于检测。现在，我们不是假设每个图像都有一个对象，而是使用 YOLOv2 的对象预测器给我们 $P_r(physical object)$ 的值。检测器预测边界框和概率树。我们遍历树，在每次分割中采用最高的置信度路径，直到达到某个阈值，然后我们预测该对象类。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/5/11.jpg?raw=true)\n",
    "\n",
    "**Dataset combination with WordTree** 我们可以使用 WordTree 以合理的方式将多个数据集组合在一起。我们只需将数据集中的类别映射到树中的synsets 即可。图6 显示了一个使用 WordTree 组合来自 ImageNet 和 COCO 的标签的示例。WordNet 非常多样化，所以我们可以将这种技术用于大多数数据集。  \n",
    "\n",
    "**Joint classification and detection** 现在我们可以使用 WordTree 组合数据集，我们可以在分类和检测上训练联合模型。我们想要训练一个非常大型的检测器，所以我们使用 COCO 检测数据集和完整的 ImageNet 版本中的前 9000 个类创建我们的组合数据集。我们还需要评估我们的方法，以便添加 ImageNet 检测挑战中尚未包含的任何类。该数据集的相应 WordTree 具有 9418个类。ImageNet 是一个更大的数据集，所以我们通过对 COCO 进行过采样来平衡数据集，使得 ImageNet 仅比检测数据集大 4 倍。  \n",
    "\n",
    "使用这个数据集我们训练 YOLO9000。我们使用基本的 YOLOv2 架构，但只有 3 个先验而不是 5 个来限制输出大小。当我们的网络看到一个检测图像时，我们会像平常一样反向传播损失。对于分类损失，我们只会在标签的相应级别或更高的级别上反向传播损失。例如，如果标签是“狗”，我们确实将任何错误分配给树中进一步的预测，“德国牧羊犬”与“黄金猎犬”，因为我们没有这些信息。  \n",
    "\n",
    "当它看到分类图像时，我们只会反向传播分类损失。要做到这一点，我们只需找到预测该类最高概率的边界框，然后计算其预测树上的损失。我们还假设预测框与至少 0.3 IOU 的 ground truth 标签重叠，并且基于此假设我们反向传播对象损失。  \n",
    "\n",
    "使用这种联合训练，YOLO9000 学习使用 COCO 中的检测数据在图像中查找对象，并学习使用来自 ImageNet 的数据对各种这些对象进行分类。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/5/12.jpg?raw=true)\n",
    "\n",
    "我们在 ImageNet 检测任务上评估 YOLO9000。使用 COCO，ImageNet 共享 44 个对象类别的检测任务，这意味着 YOLO9000 只能看到大多数测试图像的分类数据，而不是检测数据。YOLO9000 在不相交的 156 个对象类中总共获得 19.7 mAP。这个 mAP 高于 DPM 的结果，但 YOLO9000 仅在部分监督下接受不同数据集的训练[4]。它也同时实时检测 9000 个其他对象类别。  \n",
    "\n",
    "当我们分析 YOLO9000 在 ImageNet 上的表现时，我们发现它可以很好地了解动物的新物种，但是对服装和设备等类别上效果不好。新动物更容易学习，因为 COCO 中的动物可以很好地概括对象预测。相反，COCO 没有针对任何类型的服装的包围盒标签，只针对人员，因此 YOLO9000 正在努力模仿“太阳镜”或“泳裤”等类别。  \n",
    "\n",
    "## 5. Conclusion  \n",
    "\n",
    "我们介绍 YOLOv2 和 YOLO9000，即时检测系统。YOLOv2 在各种检测数据集中都是最先进的，并且比其他检测系统更快。此外，它可以在各种图像尺寸下运行，以提供速度和准确性之间的平滑折衷。  \n",
    "\n",
    "我们的许多技术都是在对象检测之外推广的。ImageNet 的 WordTree 表示为图像分类提供了更丰富，更详细的输出空间。使用分层分类的数据集组合在分类和分割领域将会很有用。像多尺度训练这样的训练技术可以为各种视觉任务提供益处。  \n",
    "\n",
    "对于未来的工作，我们希望使用类似的技术进行弱监督图像分割。我们还计划使用更强大的匹配策略来改善我们的检测结果，以便在训练期间将弱标签分配给分类数据。计算机视觉从到大量标记数据中得到发展。我们将继续寻找方法，将不同的数据来源和数据结构结合起来，形成更强大的视觉世界模型。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv3: An Incremental Improvement  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "我们向 YOLO 提供一些更新！我们做了一些小的设计更改以使其更好。我们还训练了这个非常好的新网络。它比上次稍大一些，但更准确。尽管如此，不用担心，它仍然很快，在 320×320 YOLOv3，运行时间为 22 ms，22.2 mAP，与 SSD 一样准确，但速度提高了三倍。当我们看看旧的 0.5 IOU mAP 检测指标YOLOv3 是相当不错的。在 Titan X 上，它在 51 ms 内达到 57.9 AP，而 RetinaNet 在 198 ms 内达到 57.5 AP，性能相似但速度快 3.8 倍。与往常一样，所有代码都在 https://pjreddie.com/yolo/ 。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "今年我没有做很多研究，在 Twitter 上花了很多时间。与 GAN 玩了一下。去年我留下了一点点的动力；我设法对 YOLO 进行了一些改进。但是，说实话，没有太有趣的，只是一小堆变化使它变得更好，也帮助了其他人的研究。  \n",
    " \n",
    "关于技术报告的好处是他们不需要介绍，你们都知道我们为什么来到这里。因此，这篇介绍的结尾将为本文的其余部分提供路标。首先我们会告诉你 YOLOv3 的处理方式。然后我们会告诉你我们是怎么做的。我们还会告诉你我们尝试过的一些事情没有奏效。最后，我们将考虑这一切意味着什么。  \n",
    "\n",
    "## 2. The Deal  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/6/1.jpg?raw=true)\n",
    "\n",
    "所以这里是与 YOLOv3 的交易：我们主要从其他人那里获得好点子。我们还训练了一个比其他人更好的新分类器网络。我们将从头开始介绍整个系统，以便您能够理解这一切。  \n",
    "\n",
    "### 2.1. Bounding Box Prediction  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/6/2.jpg?raw=true)\n",
    "\n",
    "在 YOLO9000 之后，我们的系统使用 dimension clusters 作为锚箱来预测边界框[15]。网络为每个边界框预测 4 个坐标，tx，ty，tw，th。如果单元格从图像的左上角偏移了（cx，cy）并且之前的边界框具有宽度和高度 pw，ph，则预测对应于：  \n",
    "\n",
    "$b_x = \\sigma(t_x) + c_x \\\\ b_y = \\sigma(t_y) + c_y, \\\\ b_w = p_we^{t_w}, \\\\ b_h = p_h e^{t_h}$\n",
    "\n",
    "在训练期间，我们使用平方误差损失的总和。如果一些坐标预测的 ground truth 是 $\\hat t_*$，我们的梯度就是ground truth value 减去我们的预测：$\\hat t_* - t_*$。通过反转上面的方程可以很容易地计算出这个 ground truth 值。  \n",
    "\n",
    "YOLOv3 使用逻辑回归预测每个边界框的对象分数。如果先前的边界框比之前的任何其他边界框与真实边界框重叠的更过，则该值应该为1。如果以前的边界框不是最好的，但是确实与真实边界框重叠了一定的阈值以上，我们忽略了这个预测[17]。我们使用.5的阈值。与[17]不同，我们的系统仅为每个真实对象分配一个边界框。如果先前的边界框未分配给真实对象，则不会对坐标或类别预测造成损失，只会对是否有对象造成损失。  \n",
    "\n",
    "### 2.2. Class Prediction  \n",
    "\n",
    "每个框使用多标签分类预测边界框可能包含的类。我们不使用 softmax，因为我们发现它对于高性能没有必要，相反我们只是使用独立的逻辑分类器。在训练过程中，我们使用二元交叉熵损失来进行类别预测。  \n",
    "\n",
    "当我们转向更复杂的领域时，这个公式可以有所帮助。在这个数据集中有许多重叠的标签（即女性和人物）。使用 softmax 强加了一个假设，即每个盒子只有一个类别，通常情况并非如此。多标签方法更好地模拟数据。  \n",
    "\n",
    "### 2.3. Predictions Across Scales  \n",
    "\n",
    "YOLOv3 预测 3 种不同尺度的盒子。我们的系统使用与特征金字塔网络相似的概念来提取这些尺度的特征。从我们的基本特征提取器中，我们添加了几个卷积层。其中最后一个预测了三维张量编码边界框，对象和类别预测。在我们用 COCO 进行的实验中，我们预测每个尺度的 3 个盒子，因此对于 4 个边界盒子偏移量，1 个目标预测和 80 个类别预测，张量为 $N×N×[3 *（4 + 1 + 80）]$。  \n",
    "\n",
    "接下来，我们从之前的 2 个图层中取得特征图，并将其上采样 2×。我们还从网络中的较早层获取特征图，并使用将其与我们的上采样特征进行合并。这种方法使我们能够从早期特征映射中的上采样特征和更细粒度的信息中获得更有意义的语义信息。然后，我们再添加几个卷积图层来处理这个组合的特征图，并最终预测出一个相似的张量，虽然现在是两倍的大小。  \n",
    "\n",
    "我们再次执行相同的设计来预测最终瓷都的方框。因此，我们对三个尺度的预测将从所有之前的计算中获益，并从早期的网络中获得精细的特征。  \n",
    "\n",
    "我们仍然使用 k-means 聚类来确定我们的先验边界框。我们只是选择了 9 个 clusters 和 3 个尺度，然后在整个尺度上均匀分割 clusters。在 COCO 数据集上，9 个 clusters 分别为（10×13）,（16×30）,（33×23）,（30×61）,（62×45）,（59×119）,（116×90）,（156×198）,（373×326）。  \n",
    "\n",
    "### 2.4. Feature Extractor  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/6/3.jpg?raw=true)\n",
    "\n",
    "我们使用新的网络来执行特征提取。我们的新网络是用于 YOLOv2，Darknet-19 的网络和那些新颖的残差网络的混合方法。我们的网络使用连续的 3×3 和1×1 卷积层，但现在也有一些 shortcut 连接。它有 53 个卷积层，所以我们称之为....Darknet-53！  \n",
    "\n",
    "这个新网络比 Darknet-19 功能强大得多，但仍比 ResNet-101 或 ResNet-152 更有效。以下是一些 ImageNet 结果：  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/6/4.jpg?raw=true)\n",
    "\n",
    "每个网络都使用相同的设置进行训练，并以 256×256 的 single crop 进行测试。运行时间是在 Titan X 上以 256×256 进行测量的。因此，Darknet-53 可与最先进的分类器相媲美，但浮点运算更少，速度更快。Darknet-53 比 ResNet-101 好，速度快 1.5 倍。Darknet-53 与 ResNet-152 具有相似的性能，速度提高 2 倍。 \n",
    "\n",
    "Darknet-53 也可以实现每秒最高的测量浮点运算。这意味着网络结构可以更好地利用 GPU，使其评估更有效率，从而更快。这主要是因为 ResNets 的层数太多，效率不高。  \n",
    "\n",
    "### 2.5. Training  \n",
    "\n",
    "我们仍然训练完整的图像，没有 hard negative mining 或任何东西。我们使用多尺度训练，大量的数据增强，批量规范化，所有标准的东西。我们使用Darknet 神经网络框架进行训练和测试。  \n",
    "\n",
    "## 3. How We Do  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/6/5.jpg?raw=true)\n",
    "\n",
    "YOLOv3 非常好！请参见表3。就 COCO 奇怪的 average mean AP 指标而言，它与 SSD 变体相当，但速度提高了 3 倍。尽管如此，它仍然比像 RetinaNet 这样的其他模型落后很多。  \n",
    "\n",
    "然而，当我们在 IOU = 0.5（或图表中的AP50）看到 mAP 的“旧”检测度量时，YOLOv3 非常强大。它几乎与 RetinaNet 相当，并且远高于 SSD 变体。这表明 YOLOv3 是一款非常强大的探测器，擅长为对象制作像样的盒子。然而，随着 IOU 阈值增加，性能下降显示 YOLOv3 正在努力使盒子与物体完美对齐。  \n",
    "\n",
    "在过去，YOLO 在小物体上表现不好。但是，现在我们看到了这种趋势的逆转。随着新的多尺度预测，我们看到 YOLOv3 具有相对较高的 $AP_S$ 性能。但是，它在中等和更大尺寸的物体上的表现相对较差。需要更多的调查来了解这一点。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/6/6.jpg?raw=true)\n",
    "\n",
    "当我们绘制 $AP_{50}$ 指标的精确度和速度时（见图5），我们看到 YOLOv3 与其他检测系统相比具有显着的优势。也就是说，速度越来越快。  \n",
    "\n",
    "## 4. Things We Tried That Didn’t Work  \n",
    "\n",
    "我们在研究 YOLOv3 时尝试了很多东西。很多都不起作用。这是我们可以记住的东西。  \n",
    "\n",
    "**Anchor box x, y offset predictions** 我们尝试使用常规锚点框预测机制，您可以使用线性激活将 x，y 偏移预测为框宽度或高度的倍数。我们发现这种方法降低了模型的稳定性，并且效果不佳。  \n",
    "\n",
    "**Linear x, y predictions instead of logistic** 我们尝试使用线性激活来直接预测 x，y 偏移而不是逻辑激活。这导致了 MAP 的几点下降。  \n",
    "\n",
    "**Focal loss** 我们尝试使用 Focal loss。它降低了我们的平均分 2 点。YOLOv3 对 Focal loss 试图解决的问题可能已经很强大，因为它具有单独的对象预测和条件类别预测。因此，对于大多数例子来说，类别预测没有损失？ 或者其他的东西？ 我们并不完全确定。  \n",
    "\n",
    "**Dual IOU thresholds and truth assignment** Faster RCNN 在训练期间使用两个 IOU 阈值。如果一个预测与 ground truth 重叠超过 .7，name这个预测就是正样本，重叠 在[.3 - .7]的预测被忽略掉，重叠小于 .3 的预测是负样本。我们尝试了类似的策略，但无法取得好成绩。  \n",
    "\n",
    "我们非常喜欢我们目前的表述，似乎至少在本地最佳状态。有些技术可能最终会产生好的结果，也许他们只是需要一些调整来稳定训练。  \n",
    "\n",
    "## 5. What This All Means  \n",
    "\n",
    "YOLOv3 是一个很好的检测器。速度很快，很准确。COCO 平均 AP 介于 .5 和 .95 IOU 之间的指标情况并不如此。但是，对于旧的检测度量 .5 IOU 来说非常好。  \n",
    "\n",
    "为什么我们要改变指标？最初的 COCO 论文只是有这样一句含糊不清的句子：“一旦评估服务器完成，就会添加对评估指标的全面讨论”。Russakovsky 等人报告说，人类很难区分 .3 与 .5 的 IOU 区别。如果人类很难区分这种差异，那么它有多重要？  \n",
    "\n",
    "但是也许更好的问题是：“现在我们有了这些检测器，我们要做什么？”很多做这项研究的人都在 Google 和 Facebook。我想至少我们知道这项技术掌握得很好，绝对不会用于收集您的个人信息并将其出售给....等等，您是说这正是它的用途？ 哦。  \n",
    "\n",
    "那么其他大量资助视觉研究的人都是军人，他们从来没有做过任何可怕的事情，例如用新技术杀死很多人哦等等.....  \n",
    "\n",
    "我有很多希望，大多数使用计算机视觉的人都只是在做快乐，好的东西，比如计算国家公园里斑马的数量[13]，或者追踪他们的猫在它们周围徘徊[19]。 但计算机视觉已经成为可疑的用途，作为研究人员，我们有责任至少考虑我们的工作可能造成的损害，并考虑如何减轻这种损害。我们非常珍惜这个世界。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD: Single Shot MultiBox Detector  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "我们提出了一种使用单个深度神经网络检测图像中物体的方法。我们的方法，名为SSD，将边界框的输出空间离散化为一组根据不同长宽比和每个特征图位置缩放的默认框。在预测时，网络会在每个默认框中为每个目标类别的存在生成分数，并对框进行调整以更好地匹配目标形状。另外，该网络将来自多个不同分辨率的特征图的预测结合起来，以自然处理各种尺寸的目标。相对于需要目标提议的方法而言，SSD 非常简单，因为它完全消除了提议生成和随后的像素或特征重新采样阶段，并将所有计算封装在单个网络中。这使得 SSD 易于训练和直接集成到需要检测组件的系统中。PASCAL VOC，COCO 和 ILSVRC 数据集上的实验结果证实，SSD 与利用额外目标提议步骤的方法具有可比准确性，并且速度更快，同时为训练和推理提供了统一的框架。对于 300×300 的输入，SSD 在 VOC2007 测试中以 59 FPS 的速度在 Nvidia Titan X 上达到 74.3% 的 mAP，并且对于 512×512 输入，SSD 达到 76.9% 的 mAP，优于同类最先进的 Faster R-CNN 模型。与其他单级方法相比，即使输入图像尺寸较小，SSD 也具有更高的精度。代码位于：https：//github.com/weiliu89/caffe/tree/ssd 。  \n",
    "\n",
    "## 1 Introduction  \n",
    "\n",
    "当前最先进的目标检测系统是以下方法的变体：假定边界框，每个框的像素或特征重采样，并应用高质量分类器。自从选择性搜索[1]通过基于 Faster R-CNN [2] 在 PASCAL VOC，COCO 和 ILSVRC 检测获得当前的领先结果（尽管具有更深的特征如[3]）以来，该流程在检测基准上盛行。虽然准确，但这些方法对嵌入式系统而言计算量过大，即使使用高端硬件，对于实时应用而言速度太慢。通常这些方法的检测速度是以每秒帧数（SPF）来衡量的，甚至是最快的高精度检测器，Faster R-CNN，仅以每秒 7 帧（FPS）的速度运行。已经有很多尝试通过改善检测流程的每个阶段来构建更快的检测器（参见第 4 节中的相关工作），但是到目前为止，显著提高的速度都以检测准确率显著降低为代价。  \n",
    "\n",
    "本文介绍了第一种基于深度网络的物体检测器，它不对边界框假设的像素或特征进行重采样，并且与所采用的方法一样准确。这使高准确率检测的速度显著提高（VOC 测试中 59 FPS 74.7%，RAPN 7 FPS 73.2%，YOLO 45 FPS 63.4%）。速度的根本改进来自消除边界框提议和随后的像素或特征重采样阶段。我们并不是第一个这样做的人（cf[4,5]），但通过增加一系列改进，我们设法比以前的尝试显著提高了准确率。我们的改进包括使用小型卷积滤波器来预测边界框位置中的目标类别和偏移量，对不同宽高比检测使用单独的预测器（滤波器），并将这些滤波器应用于网络后期阶段的多个特征映射中，以便执行多个尺度的检测。通过这些修改 - 特别是使用多层进行不同尺度的预测 - 我们可以使用较低分辨率的输入实现高精度，进一步提高检测速度。虽然这些贡献可能看起来很小，但我们注意到由此产生的系统提高了 PASCAL VOC 的实时检测精度，从 YOLO 的63.4% mAP 到我们 SSD 的 74.3% mAP。与最近在残差网络上的工作相比，这是相对于检测精度的相对提高[3]。此外，显著提高高质量检测的速度可以拓宽计算机视觉有用设置的范围。  \n",
    "\n",
    "我们总结我们的贡献如下：  \n",
    "\n",
    "- 我们引入了SSD，这是一种针对多种类别的 single-shot 检测器，比先前用于 single-shot 检测器（YOLO）的先进技术更快，并且准确率更高，实际上与显式的执行区域提议和 pooling 的较慢的技术一样准确（包括Faster R-CNN）。  \n",
    "\n",
    "- SSD 的核心是使用应用于特征图的小型卷积滤波器来预测固定的一组默认边界框的类别分数和框偏移量。  \n",
    "\n",
    "- 为了实现高检测精度，我们根据不同尺度的特征图生成不同尺度的预测，并通过纵横比明确区分预测。  \n",
    "\n",
    "- 即使在低分辨率输入图像上，这些设计特征也可实现简单的端到端训练和高精度，从而进一步提高速度与精度之间的平衡。  \n",
    "\n",
    "- 实验包括在 PASCAL VOC，COCO 和 ILSVRC 对不同输入尺寸模型的时间和精度分析，并与最新的一系列最新技术方法进行比较。  \n",
    "\n",
    "## 2 The Single Shot Detector (SSD)  \n",
    "\n",
    "本节介绍我们提出的 SSD 检测框架（第2.1节）和相关的训练方法（第2.2节）。之后， 第3节介绍了数据集特定的模型细节和实验结果。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/7/1.jpg?raw=true)\n",
    "\n",
    "### 2.1 Model  \n",
    "\n",
    "SSD 方法基于前馈卷积网络，该网络可生成固定大小的边界框集合并对这些框的物体进行评分，然后执行非极大值抑制步骤以生成最终检测结果。早期的网络层基于用于高质量图像分类的标准体系结构（在任何分类层之前截断），我们将其称为基础网络。然后，我们在网络中添加辅助结构来生成具有以下主要特征的检测结果：  \n",
    "\n",
    "**Multi-scale feature maps for detection** 我们将卷积特征层添加到截断的基础网络的末尾。这些图层逐渐减小，并允许预测多个比例的检测结果。用于预测检测结果的卷积模型在每个特征图是不一样的（参见Overfeat[4]和 YOLO[5]是在单一尺度的特征图上操作的）。  \n",
    "\n",
    "**Convolutional predictors for detection** 每个添加的特征层（或来自基础网络的现有特征层）可以使用一组卷积滤波器产生一组固定的检测预测。这些都在图2 中的 SSD 网络架构之上指出。对于具有 p 个通道的大小为 m×n 的特征层，用于预测潜在检测的参数的基本元素是 3×3×p 的小内核，其产生类别的分数或相对于默认框坐标的形状偏移。在应用内核的每个 m×n 位置，它会产生一个输出值。边界框偏移输出值是相对于每个特征映射位置的默认框位置而测量的（参见YOLO [5]的架构，该架构使用中间全连接层而不是卷积滤波器）。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/7/2.jpg?raw=true)\n",
    "\n",
    "**Default boxes and aspect ratios** 在网络顶部的多个特征图，我们将一组默认边界框与每个功能地图单元关联起来。默认框以卷积方式平铺特征映射，以便每个框相对于其对应单元的位置是固定的。在每个特征图单元格中，我们预测单元格中相对于默认方框形状的偏移量，以及指示每个框中存在类实例的类别分数。具体而言，对于给定位置的 k 个框中的每个框，我们计算 c 类分数和相对于原始默认框形状的 4 个偏移量。这导致在特征映射的每个位置周围应用总共 (c + 4)k 个滤波器，为 m×n 特征映射产生 (c + 4)kmn 输出。有关默认方框的说明，请参见图1。我们的默认方框与 Faster R-CNN [2]中使用的方框类似，但我们将它们应用于不同分辨率的多个特征图。在几个特征图中允许不同的默认框形状，让我们有效地离散化可能的输出框形状。  \n",
    "\n",
    "### 2.2 Training  \n",
    "\n",
    "训练 SSD 和训练使用区域提议的典型检测器之间的关键区别在于，需要将 ground truth 信息分配给检测器的特定输出。在 YOLO[5]，Faster R-CNN[2]和MultiBox [7]的区域提议阶段，还需要这些版本的训练。一旦确定了这种分配，端到端应用损失函数和反向传播。训练还涉及选择一套默认框和尺度以进行检测以及 hard negative mining 和数据增强策略。  \n",
    "\n",
    "**Matching strategy** 在训练期间，我们需要确定哪些默认框对应于 ground truth 检测并相应地训练网络。对于每个 ground truth 框，我们都从默认框中进行选择，这些框根据位置，纵横比和比例而变化。我们首先将每个 ground truth 框与最佳 jaccard 重叠的默认框匹配（如MultiBox [7]）。与MultiBox 不同，我们然后将默认框与 jaccard 重叠高于阈值（0.5）的任何 ground truth 相匹配。这简化了学习问题，允许网络预测多个重叠默认框的高分数，而不是要求它只挑选具有最大重叠的分数。  \n",
    "\n",
    "**Training objective** SSD 训练目标来源于 MultiBox 目标[7,8]，但扩展到处理多个目标类别。设 $x_{i,j}^p = \\{1,0\\}$ 为将第 $i$ 个默认方框与类别 $p$ 的第 $j$ 个 ground truth 方框相匹配的指标。在上面的匹配策略中，我们可以使 $\\sum_i x_{i,j}^p \\geq 1$。整体目标损失函数是定位损失（loc）和置信度损失（conf）的加权和：  \n",
    "\n",
    "$L(x,c,l,g) = \\frac {1}{N} (L_{conf}(x,c) + \\alpha L_{loc}(x,l,g)) \\tag{1}$\n",
    "\n",
    "其中 N 是匹配的默认框的数量。如果 N = 0，则将损失设置为 0。定位损失是预测框（l）和 ground truth 盒（g）参数之间的平滑 L1 损失[6]。类似于Faster R-CNN [2]，我们回归到默认边界框（d）的中心（cx，cy）和其宽度（w）和高度（h）的偏移。  \n",
    "\n",
    "$L_{loc}(x,l,g) = \\sum_{i \\in Pos}^{N} \\sum_{m \\in \\{cx,cy,w,h\\}} x_{i,j}^k \\ {smooth}_{L1}(l_i^m - \\hat g_j^m) \\\\\n",
    "\\hat g_j^{cx} = (g_j^{cx} - d_i^{cx}) / d_i^w \\ \\hat g_j^{cy} = (g_j^{cy} - d_i^{cy}) / d_i^h \\\\\n",
    "\\hat g_j^w = \\log (\\frac {g_j^w}{d_i^w}) \\ \\hat g_j^h = \\log (\\frac {g_j^h}{d_i^h}) \\tag{2}$\n",
    "\n",
    "置信度损失是多个类别置信度（c）下的 softmax 损失。  \n",
    "\n",
    "$L_{conf}(x,c) = - \\sum_{i \\in Pos}^{N} x_{i,j}^p \\log(\\hat c_i^p) - \\sum_{i \\in Neg}^{N} \\log(\\hat c_i^0) \\ where \\ \\hat c_i^p = \\frac{\\exp (c_i^p)}{\\sum_{p}\\exp (c_i^p)} \\tag{3}$\n",
    "\n",
    "并通过交叉验证将权重项 $\\alpha$ 设置为 1。  \n",
    "\n",
    "**Choosing scales and aspect ratios for default boxes** 为了处理不同的目标尺寸，一些方法[4,9]建议处理不同尺寸的图像，然后合并结果。但是，通过利用单个网络中几个不同层的特征映射进行预测，我们可以模拟相同的效果，同时还可以跨所有目标尺度共享参数。以前的作品[10,11]表明，使用来自较低层的特征图可以提高语义分割质量，因为较低层捕捉输入目标的更多细节。同样，[12]表明，从特征映射汇集的全局上下文可以帮助平滑分割结果。受这些方法的启发，我们使用较低和较高的特征图进行检测。图1 显示了框架中使用的两个示例性特征映射（8×8 和 4×4）。实际上，我们可以使用更多的特征图并保证小的计算开销。  \n",
    "\n",
    "已知网络中不同层次的特征图具有不同的（经验的）接受区域大小[13]。幸运的是，在 SSD 框架内，默认框不需要与每个层的实际接受域相对应。我们设计默认盒子的平铺，以便特定的特征图学习对物体的特定比例作出响应。假设我们想要使用 m 个特征图进行预测。每个特征映射的默认框的比例计算如下：  \n",
    "\n",
    "$s_k = s_{min} + \\frac {s_{max} - s_{min}}{m-1}(k-1), \\ k \\in [1,m] \\tag{4}$\n",
    "\n",
    "其中 $s_{min}$ 为 0.2，$s_{max}$ 为0.9，意味着最低层的尺度为 0.2，最高层的尺度为 0.9，其间的所有层均匀间隔。我们为默认框添加不同的长宽比，并将它们表示为 $a_r \\in \\{1,2,3,1/2,1/3\\}$。我们可以计算每个默认框的宽度($w_k^a = s_k \\sqrt a_r$)和高度($h_k^a = s_k / \\sqrt a_r$)。对于纵横比 1，我们还添加一个默认盒子，其比例为 $s'_k = \\sqrt {s_k s_{k+1}}$，每个特征映射位置产生 6 个默认盒子。我们将每个默认框的中心设置为（$\\frac {i+0.5}{|f_k|},\\frac {j+0.5}{|f_k|}$），其中 |fk| 是第 k 个 square 特征映射的大小，$i,j \\in [0,|f_k|]$。在实践中，人们还可以设计默认框的分布以在特定数据集上表现最好。如何设计最佳平铺也是一个悬而未决的问题。  \n",
    "\n",
    "通过对来自许多特征图所有位置的不同比例和纵横比的所有默认方框进行预测，我们获得了多种预测，涵盖了各种输入目标的大小和形状。例如，在图1 中，狗匹配 4×4 特征映射中的默认框，但不匹配 8×8 特征映射中的任何默认框。这是因为那些箱子有不同的比例，并且与狗 box 不匹配，因此在训练期间被认为是否定的。  \n",
    "\n",
    "**Hard negative mining** 在匹配步骤之后，大多数默认框为负数，特别是当可能的默认框数量很大时。这在正面和负面的训练例子之间引入显著的不平衡。我们不是使用所有负面的例子，而是使用每个默认框的最高置信度来排序它们，并选择最高的置信度，以便负数和正数之间的比率至多为 3：1。我们发现这导致更快的优化和更稳定的训练。  \n",
    "\n",
    "**Data augmentation** 为了使模型对各种输入目标大小和形状更加健壮，每个训练图像都是通过以下选项之一随机采样的：  \n",
    "\n",
    "- 使用整个原始输入图像。\n",
    "- 对 patch 进行采样，以使与目标重叠的最小 jaccard 为 0.1,0.3,0.5,0.7 或 0.9。\n",
    "- 随机抽样一个 patch。  \n",
    "\n",
    "每个采样 patch 的大小为原始图像大小的 [0.1,1]，纵横比在 1/2 和 2 之间。如果 ground truth box 的中心位于采样 patch 中，我们保留 ground truth box 的重叠部分。在上述采样步骤之后，除了应用类似于[14]中描述的一些光学量度失真之外，每个采样的 patch 被调整大小为固定尺寸并且以 0.5的概率水平翻转。  \n",
    "\n",
    "## 3 Experimental Results  \n",
    "\n",
    "**Base network** 我们的实验全部基于 VGG16 [15]，它在 ILSVRC CLS-LOC 数据集上预先训练[16]。类似于 DeepLab-LargeFOV [17]，我们将 fc6 和 fc7 转换为卷积层，fc6 和 fc7 的子采样参数，将 pool5 从 2×2 - s2 转换为 3×3 - s1，并使用 trous 算法[18] 填补“漏洞”。我们删除所有的 dropout 层和 fc8 图层。我们使用初始学习率为 10-3，0.9 动量，0.0005 权重衰减和批量为 32 的 SGD 进行微调。每个数据集的学习速率衰减策略稍有不同，稍后我们将对其进行详细描述。完整的训练和测试代码建立在 Caffe[19]上，并且是开源的：https://github.com/weiliu89/caffe/tree/ssd 。  \n",
    "\n",
    "### 3.1 PASCAL VOC2007  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/7/3.jpg?raw=true)\n",
    "\n",
    "在这个数据集上，我们在 VOC2007 测试集上（4952图像）与 Fast R-CNN [6]和 Faster R-CNN [2]进行了比较。所有方法都在相同的预先训练好的 VGG16 网络上进行微调。  \n",
    "\n",
    "图2 显示了 SSD300 模型的架构细节。我们使用 conv4_3，conv7（fc7），conv8_2，conv9_2，conv10_2 和 conv11_2 来预测位置和置信度。我们在 $conv4\\_3^3$ 上设置了比例为 0.1 的默认框。我们使用“xavier”方法初始化所有新添加的卷积图层的参数[20]。对于 conv4_3，conv10_2 和 conv11_2，我们只在每个特征地图位置关联 4 个默认框 - 忽略纵横比 1/3 和 3。对于所有其他图层，我们放置 6 个默认框，如 2.2 节中所述。正如[12]所指出的，由于 conv4_3 与其他层具有不同的特征尺度，我们使用[12]中介绍的 L2 归一化技术将特征映射中每个位置的特征标准缩放到 20，并通过反向传播过程学习尺度。我们使用 10-3 学习率进行 40k 次迭代，然后继续以 10-4 和 10-5 进行 10k 迭代训练。在 VOC2007 训练时，表1 显示我们的低分辨率 SSD300 模型比 Fast R-CNN 更精确。当我们在更大的 512×512 输入图像上训练 SSD 时，它更加准确，超过 Faster R-CNN 的 1.7%。如果我们更多（即07 + 12）的数据训练 SSD，我们看到 SSD300 已经比 Faster R-CNN 好 1.1%，而且 SSD512 的性能提高了 3.6%。如果我们采用在 3.4 节中描述的在 COCO trainval35k 上训练的模型，并在 07 + 12 数据集对 SSD512 进行微调，我们获得了最好的结果：81.6% 的 mAP。  \n",
    "\n",
    "为了更详细地理解我们两个 SSD 模型的性能，我们使用了[21]中的检测分析工具。图3 显示 SSD 可以检测到高质量的各种目标类别（大白色区域）。大部分自信的检测结果都是正确的。召回率约为 85-90%，而“弱”（0.1 jaccard 叠）标准的召回率更高。与 R-CNN [22]相比，SSD 具有较少的定位误差，表明 SSD 可以更好地定位目标，因为它直接学习回归目标形状并对目标类别进行分类，而不是使用两个分离步骤。然而，SSD 对类似的目标类别（特别是对于动物）有更多的混淆，部分原因是我们共享多个类别的位置。图4 显示 SSD 对边界框大小非常敏感。换句话说，小型物体的性能比大型物体差得多。这并不奇怪，因为这些小目标甚至可能在顶层没有任何信息。增加输入尺寸（例如从 300×300 到 512×512）可以帮助改进检测小物体，但仍然有很大的改进空间。积极的一面，我们可以清楚地看到 SSD 在大型物体上的表现非常好。而且它对不同的目标宽高比非常稳健，因为我们使用每个特征图位置的各种宽高比的默认框。  \n",
    "\n",
    "### 3.2 Model analysis  \n",
    "\n",
    "为了更好地理解 SSD，我们进行了受控实验，以检查每个组件如何影响性能。对于所有实验，除了对设置或组件进行指定更改之外，我们使用相同的设置和输入大小（300×300）。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/7/4.jpg?raw=true)\n",
    "\n",
    "**Data augmentation is crucial** Fast 和 Faster R-CNN 使用原始图像和水平翻转来训练。我们使用更广泛的抽样策略，类似于 YOLO[5]。表2 显示，采用这种抽样策略，我们可以提高 8.8% 的MAP。我们不知道我们的抽样策略会对 Faster R-CNN 带来多少好处，但它们很可能从中受益较少，因为它们在分类过程中使用了特征 pooling 步骤，这对于设计目标 translation 来说相对稳健。  \n",
    "\n",
    "**More default box shapes is better** 如 2.2 节所述， 默认情况下，我们使用每个位置 6 个默认框。如果我们移除具有 1/3 和 3 纵横比的盒子，则性能下降 0.6%。通过进一步去除具有 1/2 和 2 纵横比的盒子，性能再下降 2.1%。使用各种默认的盒子形状似乎使得预测盒子的任务更容易。  \n",
    "\n",
    "**Atrous is faster** 如第 3 节所述，我们使用 DeepLab-LargeFOV [17]中的二次采样 VGG16 的 atrous 版本。如果我们使用完整的 VGG16，保持pool5 为 2×2 - s2，而不是 fc6 和 fc7 的子采样参数，并添加 conv5_3 进行预测，结果大致相同，而速度慢了大约 20%。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/7/5.jpg?raw=true)\n",
    "\n",
    "**Multiple output layers at different resolutions is better** SSD 的主要贡献是在不同的输出层上使用不同尺度的默认盒子。为了衡量获得的优势，我们逐步删除图层并比较结果。为了公平比较，每次我们删除一个图层时，我们调整默认的方块平铺，以保持与原始方块类似的方块总数（8732）。这是通过在剩余图层上堆叠更多比例的盒子并根据需要调整盒子的比例来完成的。我们没有详尽地优化每个设置的平铺。表3 显示精度降低，随着层数减少，从74.3 单调递减至 62.4。当我们在一个图层上堆叠多个比例的盒子时，许多图像边界上都需要小心处理。我们尝试了 Faster R-CNN [2]中使用的策略，忽略了边界上的框。我们观察一些有趣的趋势。例如，如果我们使用非常粗糙的特征映射（例如conv11_2（1×1）或 conv10_2（3×3）），会大大伤害性能。原因可能是修剪后我们没有足够的大框来覆盖大型目标。当我们主要使用更精细的分辨率图时，即使在修剪足够数量的大型盒子之后,性能开始再次增加。如果我们仅使用 conv7 进行预测，那么性能是最糟糕的，这就强化了这样的信息，即在不同层次上传播不同比例的盒子至关重要。此外，由于我们的预测不像[6]中那样依赖于 ROI pooling，所以我们在低分辨率特征图中没有 collapsing bins 问题[23]。SSD 架构结合了各种分辨率特征图的预测，以达到与 R-CNN 相当的精确度，同时使用较低分辨率的输入图像。  \n",
    "\n",
    "### 3.3 PASCAL VOC2012  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/7/6.jpg?raw=true)\n",
    "\n",
    "除了我们使用 VOC2012 trainval 和 VOC2007 trainval 和test（21503图像）进行训练，我们使用与上述 VOC2007 实验基本相同的设置，并对 VOC2012 test (10991 images)进行测试。我们用 10-3 的学习速率训练模型进行 60k 的迭代，然后 10-4 进行 20k 的迭代。表4 显示了我们的 SSD300 和 SSD512模型的结果。我们看到了与我们在 VOC2007 测试中观察到的相同的性能趋势。我们的 SSD300 比 Fast/Faster RCNN 具有更高的准确率。通过将训练和测试图像大小增加到 512×512，我们比Faster R-CNN 的准确率高 4.5%。与 YOLO 相比，SSD 更精确，可能是由于使用了多个特征映射的卷积默认盒子以及我们在训练期间的匹配策略。当在 COCO 训练的模型上进行微调时，我们的 SSD512 达到 80.0% mAP，比 Faster R-CNN 高 4.1%。  \n",
    "\n",
    "### 3.4 COCO  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/7/7.jpg?raw=true)\n",
    "\n",
    "为了进一步验证 SSD 框架，我们在 COCO 数据集上训练了我们的 SSD300 和 SSD512 体系结构。由于 COCO 中的物体倾向于比 PASCAL VOC 物体更小，因此我们对所有层使用较小的默认框。我们遵循第 2.2 节中提到的策略。但现在我们最小的默认框的尺度为 0.15 而不是 0.2，conv4_3 的默认框的尺度为0.07（例如 300×300 图像中国 21 个像素）。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/7/8.jpg?raw=true)\n",
    "\n",
    "我们使用 trainval35k [24]进行训练。我们首先用 10-3 的学习率训练模型进行 160k 次迭代，然后继使用 10-4 和 10-5 分别迭代 40k 次。表5 显示了test-dev2015 的结果。类似于我们在 PASCAL VOC 数据集中观察到的结果，SSD300 在 mAP@0.5 和 mAP @ [0.5：0.95] 中都优于 Fast R-CNN。SSD300 与ION [24]和 Faster R-CNN [25]具有相似的 mAP@0.75，但 mAP@0.5 更差。通过将图像大小增加到 512×512，我们的 SSD512 在两个标准中均优于Faster R-CNN [25]。有趣的是，我们发现 SSD512 在 mAP@0.75 中提高了 5.3%，但在 mAP@0.5 中仅提高了 1.2%。我们还观察到大型物体的 AP（4.8%）和AR（4.6%）好得多，但小型物体的 AP（1.3%）和 AR（2.0%）的改善相对较少。与 ION 相比，大型和小型物体的 AR 改善更为相似（5.4% vs. 3.9%）。我们推测 Faster R-CNN 在较小物体上比 SSD 更具竞争力，因为它在 RPN 部分和 Fast R-CNN 部分都执行了两个盒子细化步骤。在图5 中，我们用 SSD512 模型展示了 COCO test-dev 上的一些检测示例。  \n",
    "\n",
    "### 3.5 Preliminary ILSVRC results  \n",
    "\n",
    "我们将与 COCO 相同的网络架构应用于 ILSVRC DET 数据集[16]。我们使用[22]中使用的 ILSVRC2014 DET train 和 val1 训练 SSD300 模型。我们首先以 10-3 的学习速率对模型进行训练，进行 320k 次迭代，然后继续以 10-4 和 10-5 的学习率分别迭代 40k 和 80k 次。我们可以在 val2 集上实现 43.4 mAP[22]。同样，它验证了 SSD 是用于高质量实时检测的通用框架。  \n",
    "\n",
    "### 3.6 Data Augmentation for Small Object Accuracy  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/7/9.jpg?raw=true)\n",
    "\n",
    "没有与 Faster R-CNN 后续一样的特征重采样步骤，小型目标的分类任务对于 SSD 来说相对困难，正如我们的分析（参见图4）所示。在 2.2 节的数据增强策略有助于显著提高性能，尤其是对于 PASCAL VOC 等小数据集。策略产生的随机 crop 可以被认为是“放大”操作，并且可以生成许多更大的训练示例。为了实现创建更多小型训练示例的“缩小”操作，我们首先在我们进行任何随机裁剪操作之前，将图像随机放置在填充了平均值的原始图像大小 16 倍的画布上。因为通过引入这种新的“扩展”数据增加技巧，我们有更多的训练图像，所以我们必须加倍训练迭代次数。我们已经看到，跨多个数据集的 mAP 持续增长 2%-3%，如表6 所示。具体来说，图6 显示新增强技巧显著提高了小物体的性能。这个结果强调了数据增强策略对最终模型精度的重要性。  \n",
    "\n",
    "改进 SSD 的另一种方法是设计更好的默认盒子的 tiling，以使其位置和比例更好地与特征图上每个位置的接受域对齐。我们将此留作未来工作。  \n",
    "\n",
    "### 3.7 Inference time  \n",
    "\n",
    "考虑到我们方法产生的大量盒子，在推理期间有效地执行非极大值抑制（nms）是必要的。通过使用 0.01 的可信度阈值，我们可以过滤掉大部分框。然后，我们将 nms 与每个类的 0.45 的 jaccard 重叠应用并保持每个图像的前 200 个检测。对于 SSD300 和 20 VOC 类，此步骤每个图像的成本大约为 1.7毫秒，接近所有新添加层花费的总时间（2.4毫秒）。我们使用 Titan X 和具有 Intel Xeon E5-2667v3@3.20GHz 的 cuDNN v4 测量批量大小为 8 的速度。  \n",
    "\n",
    "表7 显示了 SSD，Faster R-CNN [2]和 YOLO [5]之间的比较。我们的 SSD300 和 SSD512 方法在速度和准确率上均优于 Faster R-CNN。虽然 Fast YOLO [5]可以以 155 FPS 运行，但它的精度降低了近 22% 的 mAP。据我们所知，SSD300 是第一个实现 70% 以上 mAP 的实时方法。请注意，大约 80% 的前向时间花费在基础网络上（本例中为VGG16）。因此，使用更快的基础网络可能会进一步提高速度，这也可能使 SSD512 模型实时。  \n",
    "\n",
    "## 4 Related Work  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/7/10.jpg?raw=true)\n",
    "\n",
    "图像中的目标检测有两种建立的方法，一种基于滑动窗口，另一种基于区域建议分类。在卷积神经网络出现之前，这两种方法的技术水平 - 可变形零件模型（DPM）[26]和选择性搜索[1] - 具有可比较的性能。然而，R-CNN [22]在将选择性搜索区域提议和基于卷积网络的后分类相结合后引起的巨大改进之后，区域提议目标检测方法变得普遍。  \n",
    "\n",
    "最初的 R-CNN 方法已经以各种方式得到改进。第一套方法提高了后分类的质量和速度，因为它需要对成千上万的图像 crop 进行分类，这是昂贵且耗时的。SPPnet [9]显著加快了原有的 R-CNN 方法。它引入了一个空间金字塔池化层，该层对区域大小和比例更加健壮，并允许分类层重用通过几种图像分辨率生成的特征图上计算的特征。Fast R-CNN[6]扩展了 SPPnet，以便通过最大限度地减少置信度和边界框回归的损失，最终在 MultiBox [7]中引入用于学习目标的方法来端到端的微调所有图层。  \n",
    "\n",
    "第二套方法提高了使用深度神经网络生成提议的质量。在最近的作品中，如 MultiBox [7,8]，基于低级图像特征的选择性搜索区域提议被直接从单独的深度神经网络生成的提议所取代。这进一步提高了检测精度，但导致了一些复杂的设置，需要对它们之间的依赖关系进行两个神经网络的训练。Faster R-CNN [2]将选择性搜索提议替换为区域提议网络（RPN）提议，并且通过在这两个网络的交替微调共享卷积层和预测层的参数。通过这种方式，区域提议可用于汇集中级特征，最终的分类步骤更便宜。我们的 SSD 与 Faster R-CNN 中区域提议网络（RPN）非常相似，因为我们也使用一组固定的（默认）框进行预测，类似于 RPN 中的锚点框。但不是使用这些来 pool 特征并评估另一个分类器，而是为每个目标类别在每个盒子中同时生成一个分数。因此，我们的方法避免了将 RPN 与 Fast R-CNN 合并的复杂性，并且更容易训练，更快速且直接地集成到其他任务中。  \n",
    "\n",
    "与我们的方法直接相关的另一组方法，完全跳过提议步骤，直接预测多个类别的边界框和置信度。OverFeat[4]是一种深度版滑动窗口方法，在知道底层目标类别的可信度之后，直接从最顶端的特征图的每个位置预测边界框。YOLO [5]使用整个最重要的特征图来预测多个类别和边界框的可信度。我们的 SSD 方法属于此类别，因为我们没有提议步骤，但使用默认方框。但是，我们的方法比现有方法更灵活，因为我们可以在不同比例的多个特征图的每个特征位置上使用不同长宽比的默认方框。如果我们仅从最顶端的特征图的每个位置使用一个默认框，我们的 SSD 将具有与 OverFeat [4]相似的架构；如果我们使用整个最顶层的特征映射并添加一个全连接的预测层而不是我们的卷积预测器，并且没有明确考虑多个纵横比，我们可以近似地再现 YOLO[5]。  \n",
    "\n",
    "## 5 Conclusions  \n",
    "\n",
    "本文介绍了 SSD，这是一种适用于多种类别的快速 single-shot 物体检测器。我们模型的一个关键特性是使用连接到网络顶部多个特征映射的多尺度卷积包围盒输出。这种表示使我们能够有效地模拟可能的盒子形状的空间。我们通过实验验证，在给定适当的训练策略后，大量仔细选择的默认边界框会提高性能。我们构建的 SSD 模型比现有的方法至少要多一个数量级的盒子预测采样位置，比例尺和纵横比[5,7]。我们证明，鉴于相同的 VGG-16 基础架构，SSD 在准确性和速度方面与其最先进的物体检测器相比毫不逊色。我们的 SSD512 模型在 PASCAL VOC 和 COCO 的精度明显优于 Faster R-CNN，速度提高了 3 倍。我们的实时 SSD300 模型运行速度为 59 FPS，比当前的实时 YOLO [5]更快，同时产生明显优越的检测精度。  \n",
    "\n",
    "除了独立的实用程序外，我们相信我们的单片和相对简单的 SSD 模型为采用目标检测组件的大型系统提供了有用的构建模块。一个充满希望的未来方向是探索其作为使用循环神经网络来同时检测和跟踪视频中的物体的系统的一部分的用途。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSSD : Deconvolutional Single Shot Detector  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "本文的主要贡献是将附加上下文引入到最先进的通用对象检测中。为了实现这个目标，我们首先将最先进的分类器（Residual-101 [14]）与快速检测框架（SSD [18]）相结合。然后我们用解卷积层附加到 SSD + Residual-101 模型中以在物体检测中引入额外的大规模上下文，并提高精度，特别是对于小物体，称为我们的结果系统为DSSD，用于去卷积 single shot detector。虽然这两个贡献很容易在高层次上描述，但 naive 的实现并不成功。相反，我们表明仔细添加额外的学习转换阶段，特别是反卷积中的前馈连接模块和新的输出模块，使这种新方法成为进一步检测研究的潜在方法。结果显示在 PASCAL VOC 和 COCO 检测上。我们的 DSSD 采用 513×513 输入，在 VOC2007 测试中达到 81.5% 的 mAP，在 VOC2012 测试中达到 80.0%mAP，在 COCO 上达到33.2% mAP，在每个数据集上表现都优于最先进的 R-FCN 方法[3]。  \n",
    "\n",
    "## 1. Introduction  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/8/1.jpg?raw=true)\n",
    "\n",
    "在寻找进一步提高检测准确性的方法时，除了提高边界框预测过程的空间分辨率之外，明显的目标是更好的特征网络并增加更多上下文，特别是对于小目标。以前的 SSD 版本基于 VGG [26]网络，但许多研究人员使用 Residual-101 获得了更好的任务精度[14]。在检测之外寻求并行研究，已经有工作使用所谓的“encoderdecoder”网络来集成上下文信息，其中网络中间的瓶颈层用于对输入图像的信息进行编码，然后逐渐增大的层将其解码为整个图像上。由此产生的 wide, narrow, wide 的网络结构通常被称为沙漏。这些方法在最近关于语义分割[21]和人体姿态估计[20]的研究中特别有用。  \n",
    "\n",
    "不幸的是，使用更深的 Residual-101，或者在 SSD 特征层的末端增加解卷积层的修改，都是“out of the box”的。相反，需要认真构建用于集成反卷积的组合模块，以及用于在训练期间隔离 Residual-101 层的输出模块，并允许有效的学习。  \n",
    "\n",
    "## 3. Deconvolutional Single Shot Detection (DSSD) model  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/8/2.jpg?raw=true)\n",
    "\n",
    "我们首先回顾了 SSD 的结构，然后描述了使用 Residual-101 作为 SSD 基础网络时显著提高训练效率的新预测模块。接下来我们将讨论如何添加反卷积层来创建沙漏网络，以及如何整合新的反卷积模块以传递最终 DSSD 模型的语义上下文信息。  \n",
    "\n",
    "### SSD  \n",
    "\n",
    "Single Shot MultiBox 检测器（SSD [18]）建立在“基础”网络的顶部，该网络以一些卷积层结束（或截断到结束）。SSD 增加了一系列逐渐缩小的卷积层，如图1 顶部蓝色所示（基本网络以白色显示）。每个添加的图层以及一些较早的基础网络图层用于预测某些预定义默认边界框的分数和偏移量。这些预测由 3x3x#channels dimensional filters 执行，每个类别分数有一个过滤器，并且回归的边界框的每个维度都有一个过滤器。它使用非极大值抑制（NMS）来后处理预测以获得最终检测结果。更多细节可以在[18]中找到，其中检测器使用 VGG [26]作为基础网络。  \n",
    "\n",
    "### 3.1. Using Residual-101 in place of VGG  \n",
    "\n",
    "我们的第一个修改是使用 Residual-101 代替原始 SSD 模型中使用的 VGG，特别是我们使用[14]中的 Residual-101 网络。目标是提高准确率。图1 顶部显示了以 Residual-101 为基础网络的 SSD。在这里，我们在 conv5_x 块之后添加图层，并从 conv3_x，conv5_x 和其他图层预测分数和框偏移量。这本身并不能改善结果。考虑到表4 中的消融研究结果，最上面一行显示了 PASCAL VOC 2007 测试中，在 321×321 输入上的使用 Residual-101 的 SSD MAP 为 76.4。这比在 300×300 输入上的使用 VGG 的 SSD 的 77.5低（参见表3）。然而，增加一个额外的预测模块，如下所述，可显著提高性能。  \n",
    "\n",
    "### Prediction module  \n",
    "\n",
    "在原始 SSD [18]中，目标函数直接应用于选定的特征映射，并且由于梯度太大，在 conv4_3 层使用了 L2 归一化层。MS-CNN [2]指出，改进每个任务的子网络可以提高准确性。遵循这个原则，我们为每个预测层添加一个残差块，如图 2 变体（c）所示。 我们还尝试了原始的 SSD 方法（a）和带有跳跃连接的残差块（b）以及两个序列残差块（d）。表4 显示了不同预测模块的消融研究，并在第4 节中进行了讨论。我们注意到，对高分辨率输入图像，Residual-101 和预测模块的效果明显好于没有预测模块的 VGG。  \n",
    "\n",
    "### Deconvolutional SSD  \n",
    "\n",
    "为了在检测中包含更多的高级上下文，我们将预测转移到原始 SSD 设置之后的一系列解卷积层，从而有效地制作了不对称的沙漏网络结构，如图1 底部所示。我们实验中的 DSSD 模型建立在具有 Residual-101 的 SSD 上。增加额外的解卷积层以连续增加特征图层的分辨率。为了增强特征，我们采用了Hourglass 模型[20]中的“skip connection”思路。尽管沙漏模型在编码器和解码器阶段都包含对称图层，但由于两个原因，我们使解码器阶段非常浅。首先，检测是视觉中的一项基本任务，可能需要为下游任务提供信息。因此，速度是一个重要因素。构建对称网络意味着推理时间将翻番。这不是我们想要的快速检测框架。其次，没有预训练模型，其中包括在 ILSVRC CLS-LOC 数据集[25]的分类任务上训练的解码器阶段，因为分类给出了单个完整图像标签而不是检测中的局部标签。最先进的检测器依靠迁移学习的力量。与随机初始化模型相比，在 ILSVRC CLS-LOC 数据集[25]的分类任务上预先训练的模型使我们的检测器的准确率更高，收敛更快。由于我们的解码器没有预先训练好的模型，所以我们不能利用迁移学习来解码层，必须从随机初始化开始训练解码层。去卷积层的一个重要方面是计算成本，尤其是在去卷积过程之外还添加来自先前层的信息时。  \n",
    "\n",
    "### Deconvolution Module  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/8/3.jpg?raw=true)\n",
    "\n",
    "为了帮助整合来自早期特征映射和去卷积层的信息，我们引入了如图3 所示的去卷积模块。该模块适合整个 DSSD 结构，如图1 底部的实心圆所示。去卷积模块受到 Pinheiro 等人的启发，提出精化网络的反卷积模块的一个因式分解模型具有与更复杂网络相同的精确度，并且网络效率更高。我们做了以下修改并在图3 中显示它们。首先，在每个卷积层之后添加批量标准化层。其次，我们使用学习的解卷积层而不是双线性上采样。最后，我们测试不同的组合方法：element-wise sum 和 element-wise product。实验结果表明，element-wise product 提供了最好的精度（见表4底部）。  \n",
    "\n",
    "## 4. Experiments  \n",
    "\n",
    "### Base network  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/8/5.jpg?raw=true)\n",
    "\n",
    "我们的实验都基于 Residual-101[14]，它是在 ILSVRC CLS-LOC 数据集上预先训练的[25]。遵循 R-FCN [3]，我们将 conv5 阶段的有效步幅从 32 个像素改变为 16 个像素，以增加特征图分辨率。在 conv5 阶段中第一个带有步幅 2 的卷积层被修改为 1。然后遵循 trous 算法[15]，对于 kernel 大小大于 1 的 conv5 阶段中的所有卷积层，我们将它们的膨胀从 1 增加到 2 来修复减少步幅造成的“洞”。继 SSD 之后，我们使用 Residual 块来添加一些额外的图层，并减少特征图大小。  \n",
    "\n",
    "表2 显示了原始 VGG 体系结构和 Residual-101 中的选定特征层。depth 是网络中所选图层的位置。只考虑卷积和 pooling 层。注意这两个网络中第一个预测层的深度很重要。虽然 Residual-101 包含 101 层，但我们需要使用密集特征图层来预测较小的对象，因此我们别无选择，只能选择 conv3 x 块中的最后一个特征图层。如果我们只考虑其 kernel 大小大于 1 的层，这个数将下降到9。这意味着该层中的神经元的感受野可能小于 VGG 中的 conv4_3 中的神经元。与 Residual-101 中的其他图层相比，由于特征强度较弱，此图层会导致较差的预测性能。  \n",
    "\n",
    "### PASCAL VOC 2007  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/8/6.jpg?raw=true)\n",
    "\n",
    "我们在 2007 trainval 和 2012 年 trainval 中联合训练了我们的模型。  \n",
    "\n",
    "表3 显示了我们关于 PASCAL VOC2007 test 检测的结果。SSD300\\* 和 SSD512\\* 是使用最新的扩展数据增强技术的 SSD 的结果，它已经比许多其他最先进的检测器更好。通过将 VGGNet 替换为 Residual-101，如果输入图像很小，则性能相似。例如，SSD321-Residual-101 类似于 SSD300\\*-VGGNet，虽然Residual-101 似乎更快地收敛（例如，我们只使用 VGGNet 的一半迭代来训练我们的 SSD 版本）。有趣的是，当我们增加输入图像尺寸时，Residual-101 比 VGGNet 好大约 1%。我们推测，Residual-101 具有较大的输入图像尺寸是非常重要的，因为它比 VGGNet 更深，因此对象在某些非常深的层（例如conv5_x）中仍然具有强大的空间信息。更重要的是，我们看到通过添加解卷积层和跳过连接，我们的 DSSD321 和 DSSD513 比没有这些额外层的那些要好约 1-1.5%。这证明了我们提出的方法的有效性。值得注意的是，尽管 DSSD 不需要任何手工制作的上下文区域信息，但 DSSD513 比尝试包含 MRCNN [10]和 ION [1]等上下文信息的其他方法好得多。此外，我们的单模型精度比当前最先进的检测器 R-FCN [3]提高1%。  \n",
    "\n",
    "总之，DSSD 在两个测试任务中都显示出对具有特定背景和小对象的类的巨大改进。例如，具有特殊背景的飞机，船，牛和羊等。天空中的飞机，草地上的牛等等，瓶子通常很小。这表明 SSD 中的小物体检测的弱点通过所提出的 DSSD 模型得到了解决，并且具有独特上下文的类别可以获得更好的性能。  \n",
    "\n",
    "### Ablation Study on VOC2007  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/8/7.jpg?raw=true)\n",
    "\n",
    "为了了解我们添加到 SSD 模块的有效性，我们在 VOC2007 上运行具有不同设置的模型，并在表4 中记录它们的评估。PM = Prediction 模块，DM = Deconvolution 模块。大小为 321 的输入上使用 Residual-101 的纯 SSD 的 MAP 是 76.4%。这个数字实际上比 VGG 模型差。通过添加预测模块，我们可以看到结果正在改进，最好的情况是在预测之前使用一个残差块作为中间层。这个想法是为了避免允许目标函数的梯度直接流入残差网络的主干。如果在预测之前叠加两个 PM，我们看不到太大的差异。  \n",
    "\n",
    "在添加解卷积模块（DM）时，Elementwise-product 显示所有方法中最好（78.6%）。文献[8]的结果类似于评估结合视觉和文本特征的不同方法。我们也尝试使用近似双线性池方法[9]，Lin 等人提出的原始方法的低维近似，但是训练速度正在减慢，并且训练误差也非常缓慢地下降。因此，我们没有在这里使用或评估它。更好的特征组合可以被认为是提高 DSSD 模型的准确性的未来的工作。  \n",
    "\n",
    "在添加和微调 DM 组件后，我们也试图微调整个网络，但是我们没有看到任何改进，反而降低了性能。  \n",
    "\n",
    "### PASCAL VOC 2012  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/8/8.jpg?raw=true)\n",
    "\n",
    "对于 DSSD，我们使用训练好的 SSD 作为预先训练的模型。根据表4 消融研究，我们只需要训练阶段1 模型。冻结原始 SSD 模型的所有权重，我们在前 30k 迭代中以 10-3 的训练速率训练解卷积层，然后在接下来的 20k 迭代中训练速率为 10-4。表5 中显示的结果再一次验证了 DSSD 优于其他所有的结果。应该注意的是，我们的模型是唯一一个在不使用额外训练数据（即COCO），多次裁剪或 ensemble 测试方法的情况下达到 80.0% mAP 的模型。  \n",
    "\n",
    "### COCO  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/8/9.jpg?raw=true)\n",
    "\n",
    "由于 Residule-101 使用批量标准化，为了获得更稳定的结果，我们将批量大小设置为 48（这是我们可用于在具有 4 个 P40 GPU 的机器上进行训练的最大批量）用于训练 SSD321 和 20 以用于训练 SSD513 COCO。  \n",
    "\n",
    "从表6可以看出，即使输入图像尺寸非常小（300×300），SSD300\\* 也比 Faster R-CNN [24]和 ION [1]更好。通过用 Residual-101 代替 VGGNet，我们看到了具有类似输入图像尺寸（321对比300）的重大改进（28.0%对25.1%）。有趣的是，在更高的 Jaccard 重叠阈值（0.75）下，SSD321-Residual-101 的效率提高了约 3.5%（29.3％比25.8％），而在 0.5 阈值时，SSD321-Residual-101 只提高了 2.3%。我们还观察到，对于大型物体，它的好处是 7.9%，对小物体没有改善。我们认为这表明 Residual-101 比 VGGNet 具有更好的特征，这有助于大型物体的巨大改进。通过在 SSD321-Residual-101 顶部添加解卷积层，我们可以看到它在小物体上表现更好（7.4% vs 6.2%），不幸的是我们没有看到大物体的改进。  \n",
    "\n",
    "对于更大的模型，SSD513-Residual-101 的性能已经比现有技术的 R-FCN 好了 1.3%（31.2%比29.9%）[3]。切换到 Residual-101 主要在大型和中型物体中进行提升。DSSD513-Residual-101 在所有尺寸的物体上都有所改进，达到 33.2% 的 mAP，比 R-FCN 好 3.3%。根据这一观察，我们推测 DSSD 在增加输入图像尺寸时将受益更多，尽管训练和推理时间更长。  \n",
    "\n",
    "### Inference Time  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/8/10.jpg?raw=true)\n",
    "\n",
    "为了加快推理时间，我们使用以下公式在测试时删除网络中的批量标准化层。在等式1，卷积层的输出将通过减去平均值，除以方差的平方根，然后通过训练期间学习的参数进行缩放和平移。为了在测试过程中简化和加速模型，我们可以重写卷积层的权重（方程2）和偏差（方程3），并删除批量规范化相关变量，如公式4 所示。我们发现这个技巧将速度提高了 1.2 倍--1.5 倍，并将内存占用减少了三倍。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/8/11.jpg?raw=true)\n",
    "\n",
    "出于多种原因，我们提出的模型并不像原始 SSD 那么快。首先，具有更多层的 Residual-101 网络比（缩小的）VGGNet 慢。其次，我们添加到模型中的额外层，特别是预测模块和去卷积模块，会带来额外的开销。加速  DSSD 的一种潜在方法是用简单的双线性上采样替代解卷积层。第三，我们使用更多的默认框。表7 显示，我们使用比以前版本的 SSD（43688 与 17080）多 2.6 倍的默认框。这些额外的默认方框不仅在预测中需要更多的时间，而且在随后的非最极大值抑制时也需要更多时间。  \n",
    "\n",
    "### Visualization  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/8/12.jpg?raw=true)\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/8/13.jpg?raw=true)\n",
    "\n",
    "在图4 中，我们用 SSD321 和 DSSD321 模型展示了 COCO test-dev 上的一些检测示例。与 SSD 相比，我们的 DSSD 模型在两种情况下有所改进。第一种情况是在包含小物体或密集物体的场景中，如图 4a 所示。由于输入尺寸较小，SSD 在小物体上效果不佳，但 DSSD 显示出明显的改善。第二种情况是针对某些具有不同上下文的类。在图4 中，我们可以看到具有特定关系的类的结果可以得到改善：领带和穿着西装的男士，棒球棒和棒球运动员，足球和足球运动员，网球拍和网球运动员以及滑板和跳跃者。  \n",
    "\n",
    "## 5. Conclusion  \n",
    "\n",
    "我们提出了一种将上下文添加到最先进的对象检测框架的方法，并展示了它在基准数据集上的有效性。虽然我们期望在寻找更高效，更有效的方法来结合编码器和解码器的特征方面做出许多改进，但我们的模型仍然可以在 PASCAL VOC 和 COCO 上实现最新的检测结果。我们的新 DSSD 模型能够超越以前的 SSD 框架，特别是在小物体或特定背景的物体上，同时仍保持与其他检测器相当的速度。虽然我们只将编码器 - 解码器沙漏模型应用于 SSD 框架，但这种方法也可以应用于其他检测方法，例如 R-CNN 系列方法[12,11,24]。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network for Real-time Embedded Object Detection  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "目标检测是计算机视觉领域的主要挑战，涉及场景中的目标分类和目标定位。尽管近年来深度神经网络已经显示出能够产生非常强大的技术来应对物体检测的挑战，但是使这种目标检测网络能够广泛部署在嵌入式设备上面临的最大挑战之一是高度计算和内存要求。最近，越来越多的人关注探索更适合嵌入式设备的小型深度神经网络体系结构，如 Tiny YOLO 和 SqueezeDet。受 SqueezeNet 中引入的 Fire 微体系结构的效率以及 SSD 中引入的 single shot 检测结构的性能的启发，本文引入了 Tiny SSD，一种用于嵌入实时目标检测的 single shot 检测深度卷积神经网络，其中包含高度优化的基于 SSD 的辅助卷积特征层，专门用于最小化模型大小，同时保持目标检测性能。由此产生的 Tiny SSD 拥有 2.3MB（比Tiny YOLO小约26倍）的模型尺寸，同时在 VOC 2007 上仍然达到 61.3% 的 mAP（比Tiny YOLO高约4.2%）。这些实验结果表明，非常小的深度神经网络架构可以设计用于实时目标检测，非常适合嵌入式场景。  \n",
    "\n",
    "## I. INTRODUCTION  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/9/1.jpg?raw=true)\n",
    "\n",
    "物体检测可以被认为是计算机视觉领域的主要挑战，因为它涉及场景中物体分类和物体定位的组合（见图1）。现代深度学习的发展[7]，[6]已经在物体检测方面取得了重大进展，大部分研究集中在设计越来越复杂的物体检测网络以提高精度，如SSD [9]，R- CNN [1]，Mask R-CNN [2]以及这些网络的其他扩展变体[4]，[8]，[15]。尽管事实上这样的目标检测网络已经显示出先进的目标检测精度超过了先前的先进方法所能达到的水平，但由于计算和内存限制，这样的网络通常难以用于嵌入式应用。事实上，这些网络中更快的变种如 Faster R-CNN [13]只能在高端图形处理单元（GPU）上实现一位数的帧速率。因此，考虑到这样的网络能够实现的大量操作场景，从智能手机到空中无人机，非常期望用于实时嵌入式目标检测的更高效的深度神经网络。  \n",
    "\n",
    "最近，人们越来越关注探索更适合于嵌入式设备的小型深度神经网络体系结构以进行目标检测。例如，Redmon 等人推出了 YOLO [11]和 YOLOv2 [12]，它们的设计考虑到速度，并能够在高端 Nvidia Titan X 台式机 GPU 上实现实时目标检测性能。但是，YOLO 和 YOLOv2 的模型尺寸仍然非常大（分别为 753 MB和 193 MB），对于大多数嵌入式设备而言，它们从内存角度来看过于庞大。此外，在嵌入式芯片上运行时，它们的目标检测速度大大降低[14]。为了解决这个问题，引入了 Tiny YOLO [10]，其网络架构大大降低，大大减少了模型大小（60.5 MB），并大大减少了达到所需检测精度所需的浮点操作数量（仅69.7亿次操作）（在二十类 VOC2017 测试组中为 57.1%）。同样，Wu 等人介绍了 SqueezeDet [16]，一个全卷积神经网络，利用 SqueezeNet [5]中介绍的端到端目标检测网络架构内的高效 Fire 微体系结构。鉴于 Fire 微体系结构的高效性，所产生的 SqueezeDet 专门为自动驾驶而缩小了模型尺寸。但是，SqueezeDet 仅在有限的目标类别（仅三个）下才能用于目标检测，因此它没有证明处理大量类别的能力。因此，高效深度神经网络架构的设计非常适合实时嵌入式目标检测，同时在多种目标类别上实现更高的目标检测精度仍然是值得探讨的挑战。  \n",
    "\n",
    "为了实现目标检测精度和实时嵌入式要求（即小型模型尺寸和实时嵌入式推理速度）之间的良好平衡，我们从 SqueezeNet [5]中介绍的 Fire 微体系结构的令人难以置信的效率和 SSD 中引入的 single-shot detection 架构证明了强大的目标检测性能[9]。本文所实现的网络体系结构是 Tiny SSD，一种专门为实时嵌入式目标检测而设计的单次检测深卷积神经网络。Tiny SSD 由非均匀高度优化的 Fire 子网络堆栈组成，该子网络堆叠馈送到高度优化的基于 SSD 的辅助卷积特征层的非统一子网络堆栈中，专门用于最小化模型大小，同时保持目标检测性能。  \n",
    "\n",
    "本文组织如下。第2 节介绍了在 Tiny SSD 网络架构中利用的高度优化的 Fire 子网络堆栈。第3 节描述了 Tiny SSD 网络架构中使用的基于 SSD 的卷积特征层的高度优化的子网络堆栈。第4 节介绍了评估 Tiny SSD 实时嵌入式物体检测功效的实验结果。最后，在第5 节中得出结论。  \n",
    "\n",
    "## II. OPTIMIZED FIRE SUB-NETWORK STACK  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/9/2.jpg?raw=true)\n",
    "\n",
    "用于实时嵌入式目标检测的 Tiny SSD 网络的整体网络体系结构由两个主要的子网络堆栈组成：i）非均匀的 Fire 子网络 stack,，以及ii）基于 SSD 辅助卷积特征层的的高度优化的非均匀子网络 stack,，其中第一子网络 stack 馈送到第二子网络 stack 中。在本节中，我们首先详细讨论 Tiny SSD 网络架构的第一个子网络 stack 背后的设计理念：优化的 Fire 子网络 stack。  \n",
    "\n",
    "为嵌入式推理设计更小的深度神经网络架构的一种强大方法是采取更有理论的方法，并利用架构设计策略来实现更高效的深度神经网络微架构。这种理论性方法的一个非常具有说明性的例子是 SqueezeNet[5]网络架构，其中采用了三个关键设计策略：  \n",
    "\n",
    "1）尽可能减少 3×3 滤波器的数量，  \n",
    "2）尽可能减少 3×3 滤波器的输入通道数，  \n",
    "3）在网络后期执行下采样。  \n",
    "\n",
    "这种原理设计的策略导致设计作者称之为 Fire 模块，它由一个 1x1 滤波器的压缩卷积层组成（实现了第二种设计策略，有效地将 3×3 滤波器输入通道的数量减少），进入由 1×1 滤波器和 3×3 滤波器组成的扩展卷积层（其实现了有效减少 3×3 滤波器数量的第一设计策略）。图2 显示了 Fire 微架构的一个例子。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/9/4.jpg?raw=true)\n",
    "\n",
    "受到 Fire 微架构设计的优雅和简单的启发，我们将 Tiny SSD 网络架构的第一个子网络堆栈设计为标准卷积层，然后是一组高度优化的 Fire 模块。设计该子网络堆栈的关键挑战之一是确定 Fire 模块的理想数量以及每个 Fire 模块的理想微体系结构，以实现目标检测性能和模型大小以及推理速度之间的良好平衡。首先，根据经验确定，优化的 Fire 子网络堆栈中的 10 个 Fire 模块提供了强大的物体检测性能。就理想的微体系结构而言，Fire 微体系结构的关键设计参数是形成这种微体系结构的每个尺寸（1×1或3×3）的滤波器数量。在首次引入 Fire 微架构的 SqueezeNet 网络架构[5]中，Fire 模块的微架构在很大程度上是统一的，许多模块共享相同的微架构配置。为了在模块化基础上实现更加优化的 Fire 微体系结构，每个 Fire 模块中每个尺寸的滤波器数量经过优化，尽可能少的参数，同时仍保持整体物体检测精度。因此，Tiny SSD 网络架构中优化的 Fire 子网络堆栈本质上是高度不均匀的，以便实现最佳的子网络架构配置。表I 显示了 Tiny SSD 中高度优化的 Fire 子网络堆栈的整体架构，以及子网络堆栈中每层的参数数量。  \n",
    "\n",
    "## III. OPTIMIZED SUB-NETWORK STACK OF SSD-BASED CONVOLUTIONAL FEATURE LAYERS  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/9/3.jpg?raw=true)\n",
    "\n",
    "在本节中，我们首先详细讨论 Tiny SSD 网络体系结构的第二个子网络背后的设计理念：基于 SSD 的辅助卷积特征层高度优化的子网络堆栈。  \n",
    "\n",
    "近年来最广泛使用和最有效的目标检测网络宏观架构之一是 single-shot multibox detection（SSD）宏观架构[9]。SSD 宏架构使用一组辅助卷积特征层和卷积预测器来增强基础特征提取网络架构。辅助卷积特征层被设计为使得它们以渐进方式减小尺寸，从而使得能够灵活地在不同尺度下检测场景内的目标。然后可以利用每个辅助卷积特征图层来获得：i）目标类别的置信度分数，或者ii）相对于默认边界框坐标的形状偏移量。可以以这种方式以强大的端到端 single-shot 方式为每个目标类别获得多个目标检测。  \n",
    "\n",
    "受 SSD 宏观架构的强大物体检测性能和多尺度灵活性的启发[9]，Tiny SSD 的第二个子网络堆栈由一组辅助卷积特征层和具有高度优化的微体系结构配置的卷积预测器组成（参见图3）。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/9/5.jpg?raw=true)\n",
    "\n",
    "与 Fire 微体系结构一样，设计该子网络堆栈的一个关键挑战是确定每个辅助卷积特征层和卷积预测器的理想微体系结构，以实现目标检测性能和模型大小以及推理速度之间的良好平衡。辅助卷积特征层微体系结构的关键设计参数是形成这种微体系结构的滤波器的数量。因此，类似于构建高度优化的 Fire 子网络堆栈所采取的策略，每个辅助卷积特征层中的滤波器的数量被优化以最小化参数的数量，同时保持完整 Tiny SSD 网络的整体目标检测精度。因此，Tiny SSD 网络架构中辅助卷积特征层的优化子网络堆栈本质上是非常不均匀的，以便实现最佳的子网络架构配置。表2 显示了 Tiny SSD 网络架构内辅助卷积特征层的优化子网络堆栈的整体架构，以及每层中的参数数目。  \n",
    "\n",
    "## IV. PARAMETER PRECISION OPTIMIZATION  \n",
    "\n",
    "在本节中，让我们讨论 Tiny SSD 的参数精度优化策略。对于计算要求和内存要求更严格的嵌入式场景，减少深度神经网络计算和内存占用的有效策略是降低深度神经网络中参数的数据精度。特别是，现代 GPU 已经转向加速数据精度操作，并且更好地处理降低的参数精度，因此利用这些因素的能力可以为嵌入式方案带来显著的改进。对于 Tiny SSD，参数用半精度浮点表示，从而导致深度神经网络模型尺寸的进一步减小，同时对物体检测精度的影响可以忽略不计。  \n",
    "\n",
    "## V. EXPERIMENTAL RESULTS AND DISCUSSION  \n",
    "\n",
    "为研究 Tiny SSD 在实时嵌入式目标检测中的实用性，我们检查了 VOC2007/2012 数据集的模型大小，目标检测精度和计算操作。出于评估的目的，Tiny YOLO 网络[10]被用作基准参考比较，因为它在嵌入式目标检测方面的受欢迎程度，并且还被证明在 VOC 2007/2012 数据集中拥有文献中用于目标检测的最小模型尺寸之一 （只有 60.5MB 大小，仅需要 69.7 亿次操作）。VOC2007/2012 数据集包含已用 20 种不同类型的目标标注的自然图像，图4 显示了示例。测试的深度神经网络使用 VOC2007/2012 训练数据集进行训练，平均精度（mAP ）在 VOC2007 测试数据集上计算，以评估深度神经网络的目标检测精度。  \n",
    "\n",
    "### A. Training Setup  \n",
    "\n",
    "建议的 Tiny SSD 网络在批量为 24 的 Caffe 框架中进行了 22 万次迭代训练。RMSProp 被用作基本策略，学习率设置为 0.00001 和 γ= 0.5。  \n",
    "\n",
    "### B. Discussion  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/9/6.jpg?raw=true)\n",
    "\n",
    "表III 显示了 Tiny SSD 网络在 VOC 2007 测试数据集上的模型大小和目标检测精度，以及 Tiny YOLO 的模型大小和目标检测精度。可以进行一些有趣的观察。首先，由此产生的 Tiny SSD 拥有 2.3MB 的模型尺寸，比 Tiny YOLO 小约 26 倍。与 Tiny YOLO 相比，Tiny SSD 的模型尺寸显著缩小，说明其大大降低了利用 Tiny SSD 进行实时嵌入式目标检测的内存要求。其次，可以观察到由此产生的 Tiny SSD 仍然能够在 VOC 2007 测试数据集上达到 61.3% 的 mAP，比使用 Tiny YOLO 获得的高出约 4.2%。图5 展示了与 Tiny YOLO 相比，所提出的 Tiny SSD 产生的几个目标检测结果。可以观察到，在某些情况下，Tiny SSD 具有与 Tiny YOLO 类似的目标检测结果，而在某些情况下，Tiny YOLO 将更精确的类别标签分配给检测到的目标，性能优于Tiny YOLO。例如，在第一个图像案例中，Tiny SSD 能够检测到场景中的椅子，而 Tiny YOLO 错过了椅子。在第三个图像案例中，Tiny SSD 能够识别场景中的狗，而Tiny YOLO 检测狗周围的两个边界框，其中一个边界框错误地将其标记为猫。与 Tiny YOLO 相比，这种目标检测精度的显著提高说明了 Tiny SSD 提供更可靠的嵌入式目标检测性能的功效。此外，如表四所示，Tiny SSD 需要 57109 万个 MAC 操作来执行推断，使其非常适合实时嵌入式目标检测。这些实验结果表明，非常小的深度神经网络架构可以设计用于实时目标检测，适用于嵌入式场景。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/9/7.jpg?raw=true)\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/9/8.jpg?raw=true)\n",
    "\n",
    "## VI. CONCLUSIONS  \n",
    "\n",
    "本文介绍了一种称为 Tiny SSD 的单次检测深卷积神经网络，用于实时嵌入式目标检测。由高度优化的非均匀 Fire 子网络堆栈和基于 SSD 的辅助卷积特征层高度优化的非均匀子网络堆栈，专门设计用于在保持物体检测性能的同时最小化模型大小，Tiny SSD 拥有的模型大小比 Tiny YOLO 小 26 倍，仅需57109 万个 MAC 操作，而在 VOC 2007 测试数据集上仍然比 Tiny YOLO 高出约 4.2%。这些结果展示了设计非常小的深度神经网络体系结构（如Tiny SSD）在嵌入式场景中进行实时目标检测的功效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R-FCN: Object Detection via Region-based Fully Convolutional Networks  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "我们提出了基于区域的全卷积网络，以实现准确和高效的目标检测。与先前的基于区域的检测器相比，如 Fast/Faster R-CNN [6,18]，其应用昂贵的区域子网络数百次，我们的基于区域的检测器是全卷积的，几乎所有的计算都在整个图像上共享。为了实现这一目标，我们提出位置敏感分数图来解决图像分类中的平移不变性与目标检测中的平移变异之间的两难境地。因此，我们的方法自然会采用全卷积图像分类器骨干，如最新的残差网络（ResNets）[9]，用于目标检测。我们使用 101 层 ResNet 在 PASCAL VOC 数据集上展示了有竞争的结果（例如，2007 set 83.6% mAP）。同时，我们的结果是在每张图像 170ms 的测试时间速度下实现的，比 Faster R-CNN 速度快 2.5-20 倍。代码公开发布在：https://github.com/daijifeng001/r-fcn 。  \n",
    "\n",
    "## 1 Introduction  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/9/1.jpg?raw=true)\n",
    "\n",
    "一个流行的用于目标检测的深度网络[8,6,18]可以由感兴趣区域（RoI）pooling 层[6]划分为两个子网络：（i）独立于 ROI 的共享“全卷积”子网络 （ii）不共享计算的 Roi-wise 子网络。这种分解[8]历来是由先进的分类体系结构产生的，例如 AlexNet [10]和 VGG Nets [23]，它们由两个子网络组成 - 一个以空间 pooling 层结尾的卷积子网络， 接着是全连接（fc）层。 因此，图像分类网络中的（最后一个）空间 pooling 层自然地变为目标检测网络中的 RoI pooling 层[8,6,18]。  \n",
    "\n",
    "但最近的最先进的图像分类网络，如残差网络（ResNets）[9]，GoogLeNets [24,26]在设计上是全卷积的。通过类推，使用所有卷积层在目标检测体系结构中构造共享的卷积子网络似乎是自然的，使得 RoI 方式的子网络没有隐藏层。然而，正如在这项工作中经验性地研究的那样，这种简单的解决方案的检测准确率相当差，不能与用于分类任务的准确率相比。为了解决这个问题，在 ResNet 论文[9]中，Faster R-CNN 检测器[18]的 RoI pooling 层非自然地插入在两组卷积层之间 - 这创建了一个更深的 RoI 子网络，由于每个 RoI 的计算没有共享参数，因此速度较低。  \n",
    "\n",
    "我们认为上面所述的原因是由于图像分类对图像转换不变性不敏感，而目标检测对转换差异比较敏感引起的。一方面，图像级别的分类任务依赖于平移不变性 - 图像内物体的移动应该是无差别的。因此，深度（完全）卷积体系结构尽可能转移不变，这一点可以从 ImageNet 分类的主要结果[9,24,26]中得到证实。另一方面，物体检测任务需要在一定程度上需要对平移的物体进行定位。例如，在候选框内转换目标应该产生有意义的响应来描述候选框与目标的重叠程度。我们假设图像分类网络中较深的卷积层对转换不太敏感。为了解决这个困境，ResNet 论文的检测流程[9]将 RoI pooling 层插入卷积中 - 这个区域特定的操作会降低平移不变性，并且当在不同区域进行评估时，RoI 后处理卷积层不再是平移不变的。然而，由于该设计引入了大量 region-wise 层（表1），因此这种设计牺牲了训练和测试效率。  \n",
    "\n",
    "在本文中，我们开发了一个称为基于区域的全卷积网络（R-FCN）的框架用于物体检测。我们的网络由共享的全卷积体系结构组成，就像 FCN [15]一样。为了将转换方差并入 FCN，我们使用一组专门的卷积层作为 FCN 输出来构建一组位置敏感分数图。这些分数图中的每一个都相对于相对空间位置（例如，“在目标的左边”）编码位置信息。在这个 FCN 之上，我们添加了一个位置敏感的 RoI pooling 层，它从这些评分图中捕获信息，而后面没有权重（卷积/ fc）层。整个架构是端到端学习的。所有可学习的层都是卷积的并且在整个图像上共享，除了编码目标检测所需的空间信息。图1 说明了关键思想，表1 比较了基于区域的检测器的方法。  \n",
    "\n",
    "使用 101 层残差网络（ResNet-101）[9]作为主干网，我们的 R-FCN 在 2007 PASCAL VOC 中获得 83.6% mAP 的结果，2012 达到 82.0%。同时，使用ResNet-101，我们的结果是在每个图像 170ms 的测试时间速度下实现的，比[9]中 Faster R-CNN + ResNet-101 速度快了 2.5 倍到 20 倍。这些实验表明，我们的方法设法解决了转换不变性/方差之间的困境，而全卷积图像级分类器（如ResNet）可以有效地转换为全卷积目标检测器。代码公开发布在：https://github.com/daijifeng001/r-fcn 。  \n",
    "\n",
    "## 2 Our approach  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/10/2.jpg?raw=true)\n",
    "\n",
    "**Overview** 继 R-CNN[7]之后，我们采用了流行的两阶段目标检测策略，它包括：（i）区域提议和（ii）区域分类。尽管不依赖区域提议的方法确实存在（例如[17,14]），但基于区域的框架在几个基准测试中仍然具有领先的准确率[5,13,20]。我们通过区域提议网络（RPN）[18]提取候选区域，它本身就是全卷积体系结构。在[18]之后，我们共享了 RPN 和 R-FCN 之间的特征。图2 显示了该系统的概述。  \n",
    "\n",
    "给出提议区域（RoIs）之后，R-FCN 架构旨在将 RoI 归类为目标类别和背景。在 R-FCN 中，所有可学习的权重层都是卷积的，并在整个图像上计算。最后一个卷积层为每个类别产生一组 $k^2$ 个位置敏感分数图，因此具有带有 C 个目标类别（背景为+1）的 $k^2(C+1)$ - 信道输出层。$k^2$ 分数图对应于描述相对位置的 k×k 空间网格。例如，在 k×k = 3×3 的情况下，9 个分数图编码目标类别的{左上，中上，右上，...，右下}的情况。  \n",
    "\n",
    "R-FCN 以一个位置敏感的 RoI pooling 层结束。该层聚合最后一个卷积层的输出并为每个 RoI 生成分数。通过端到端的训练，这个 RoI 层为最后一个卷积层提供了专门的位置敏感分数图。图1 说明了这个想法。图3 和图4 显示了一个例子。详细介绍如下。  \n",
    "\n",
    "**Backbone architecture** 本文中 R-FCN 的化身基于 ResNet-101[9]，但其他网络[10,23]也适用。ResNet-101 有 100个 卷积层，其次是全局平均池化和 1000 类的 fc 层。我们删除平均池化层和 fc 层，并仅使用卷积层来计算特征映射。我们使用由[9]的作者发布的 ResNet-101，在 ImageNet 上预先训练。ResNet-101 中的最后一个卷积块是 2048-d，我们附加了一个随机初始化的 1024-d 1×1 卷积层以减小尺寸（准确地说，这增加了表1 中的深度1）。然后我们应用 $k^2(C+1)$ - 信道卷积层来生成评分图，如下所述。  \n",
    "\n",
    "**Position-sensitive score maps & Position-sensitive RoI pooling** 为了将位置信息明确地编码到每个 RoI 中，我们用规则网格将每个RoI 矩形分成 k×k 个 bins。对于尺寸为 w×h 的 RoI 矩形，bin 的大小为 $\\approx w/k \\times h/k$ 。在我们的方法中，最后的卷积层被构造为为每个类别产生 $k^2$ 分数图。在第(i,j)个 bin 内部，我们定义了一个位置敏感的 RoI 池化操作  \n",
    "\n",
    "$r_x(i,j | \\Theta) = \\sum_{(x,y) \\in bin(i,j)} z_{i,j,c}(x + x_0, y + y_0 | \\Theta) /n \\tag{1}$\n",
    "\n",
    "位置敏感分数图的概念部分受到[3]的启发，该工作将 FCN 扩展为实例级语义分割。我们进一步介绍位置敏感的 RoI pooling 层，它可以学习目标检测的分数映射。RoI 层之后没有可学习的层，可以实现近乎无成本的区域计算并加速训练和推理。  \n",
    "\n",
    "**Training** 通过预先计算的区域提议，很容易实现 R-FCN 架构的端到端训练。根据[6]，我们在每个 RoI 上定义的损失函数是交叉熵损失和方框回归损失的总和。  \n",
    "\n",
    "我们的方法很容易在训练期间采用 online hard example mining（OHEM）。我们可忽略的 per-RoI 计算使得几乎无成本的示例挖掘成为可能。假设每个图像有 N 个提议，在正向传递中，我们评估所有 N 个提议的损失。然后，我们按损失对所有 RoIs（正面和负面）进行排序，并选择损失最高的 B 个 RoIs。反向传播[11]是基于所选示例执行的。由于我们的 per-RoI 计算可以忽略不计，所以前向时间几乎不受 N 的影响，与[22]中的 OHEM Fast R-CNN 相比，它可能使训练时间加倍。我们在下一节的表3 中提供全面的时间统计。  \n",
    "\n",
    "**Inference** 如图2 所示，计算了 RPN 和 R-FCN 之间共享的特征图（在单一比例为 600 的图像上）。然后，RPN 部分提出 RoI，R-FCN 部分在其上评估分类分数并回归边界框。在推断过程中，我们评估了 300 个 RoIs。作为标准实践，使用阈值 0.3 IOU [7]对结果进行非极大值抑制（NMS）后处理。  \n",
    "\n",
    "**À trous and stride** 我们的全卷积体系结构享有 FCN 广泛用于语义分割的网络修改的好处[15,2]。特别是，我们将 ResNet-101 的有效步幅从 32 像素降低到到 16 像素，从而提高了分数图分辨率。conv4 阶段之前和之后的所有图层[9] (stride= 16）不变；第一个 conv5 模块中的 stride = 2 操作被修改为 stride = 1，并且 conv5 阶段的所有卷积滤波器都被“hole algorithm”修改，以弥补减少的步幅。为了进行公平的比较，RPN 在 conv4 阶段上（与 R-FCN 共享）计算，如[9]中 Faster R-CNN 所示，所以 RPN 不受这种方法的影响。下表显示了 R-FCN 的消融结果（k × k = 7 × 7, no hard example mining）。这个诀窍可以提高 2.6 点的 MAP。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/10/3.jpg?raw=true)\n",
    "\n",
    "**Visualization** 在图3 和图4 中，当 k×k = 3×3 时，我们可视化 R-FCN 学习的位置敏感得分图。这些图预计会在目标的特定相对位置强烈激活。 例如，“top-center-sensitive”分数图大致在物体的顶部中心位置附近呈现高分数。如果一个候选方块与一个真实物体精确重叠（图3），则 RoI 中的大多数 $k^2$ 区域被强烈激活。相反，如果一个候选框与一个真实目标没有正确重叠（图4），则 RoI 中的某些 $k^2$ 区域未被激活，并且投票得分较低。  \n",
    "\n",
    "## 3 Related Work  \n",
    "\n",
    "R-CNN [7]已经证明了使用深度网络的区域提议[27,28]的有效性。R-CNN 评估裁剪区域和变形区域的卷积网络，计算不在区域间共享（表1）。SPPnet[8]，Fast R-CNN[6]和 Faster R-CNN [18]是“semi-convolutional”，其中卷积子网络对整个图像执行共享计算，另一个子网络评估各个区域。  \n",
    "\n",
    "已经有可以被认为是“全卷积”模型的物体检测器。OverFeat[21]通过在共享卷积特征映射上滑动多尺度窗口来检测目标；同样，在 Fast R-CNN[6]和[12]中，研究了替代区域建议的滑动窗口。在这些情况下，可以将单一尺度的滑动窗口重新设计为单个卷积层。Faster R-CNN[18]中的 RPN 组件是一种全卷积检测器，可以相对于多种尺寸的参考盒（锚）预测边界盒。最初的 RPN 在[18]中是类别不可知的，但是它的类特定的对应物是适用的（参见[14]），我们将在下面进行评估。  \n",
    "\n",
    "另一个物体检测器家族采用全连接（fc）层来在整个图像上生成整体物体检测结果，如[25,4,17]。  \n",
    "\n",
    "## 4 Experiments  \n",
    "\n",
    "### 4.1 Experiments on PASCAL VOC  \n",
    "\n",
    "我们在 包含 20 个类的 PASCAL VOC [5]上进行实验。我们根据[6]对 VOC 2007 trainval 和 VOC 2012 trainval 的联合数据集上进行训练（“07 + 12”）模型，并对 VOC 2007 测试集上进行评估。目标检测精度通过 mean Average Precision（mAP）来测量。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/10/4.jpg?raw=true)\n",
    "\n",
    "#### Comparisons with Other Fully Convolutional Strategies  \n",
    "\n",
    "虽然全卷积检测器是可用的，但实验表明，它们达到良好的准确性是不平凡的。我们使用 ResNet-101 研究以下全卷积策略（或“几乎”全卷积策略，每个RoI 只有一个分类器 fc 层）：  \n",
    "\n",
    "**Naïve Faster R-CNN** 如介绍中所讨论的，可以使用 ResNet-101 中的所有卷积层来计算共享特征映射，并且在最后一个卷积层（conv5之后）之后采用 RoI 池化。使用简单的 21 类 fc 层评估每个 ROI。  \n",
    "\n",
    "**Class-specific RPN** 这个 RPN 按照[18]中描述的进行训练，除了 2 类（目标或非目标）卷积分类器层被 21 类卷积分类器层代替。为了公平比较，对于这个特定类别的 RPN，我们使用 ResNet-101 的 conv5 图层来处理这个问题。  \n",
    "\n",
    "**R-FCN without position-sensitivity** 通过设置 k = 1，我们移除了 R-FCN 的位置灵敏度。这相当于每个 RoI 内的 global pooling。  \n",
    "\n",
    "分析。表2 显示了结果。我们注意到 ResNet 论文[9]中使用 ResNet-101 的标准（非 naïve)）Faster R-CNN（参见表3）达到了 76.4% 的 mAP，它在 conv4 和 conv5 之间插入了 RoI 池化层[9]。相比之下，naïve 的 Faster R-CNN（在 conv5 之后应用 RoI 池化）具有 68.9% 的极低的 mAP（表2）。这种比较经验证明了通过在 Faster R-CNN 系统的层之间插入 RoI 池化来尊重空间信息的重要性。  \n",
    "\n",
    "类别特定 RPN 的 mAP 为 67.6%（表2），比标准 Faster CNN  76.4% 低约 9%。这种比较符合[6,12]中的观察结果 - 事实上，类别特定 RPN 类似于使用密集滑动窗口作为提议的特殊形式的 Faster R-CNN [6]。  \n",
    "\n",
    "另一方面，我们的 R-FCN 系统具有更高的准确率（表2）。其 mAP（76.6%）与标准 Faster CNN 的 76.4% 相当（表3）。这些结果表明，我们的位置敏感策略设法对用于定位目标的有用空间信息进行编码，而在 RoI 池化之后不使用任何可学习的层。  \n",
    "\n",
    "位置灵敏度的重要性通过设置 k = 1 来进一步证明，其中 R-FCN 不能收敛。在这种退化情况下，在 RoI 内不能明确捕获空间信息。此外，我们报告说，如果它的 ROI 池化输出分辨率是1×1，但 naïve Faster R-CNN 能够收敛，但是 mAP 进一步下降到 61.7%（表2）。  \n",
    "\n",
    "#### Comparisons with Faster R-CNN Using ResNet-101  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/10/5.jpg?raw=true)\n",
    "\n",
    "表4 显示了更多的比较。与[8]中的多尺度训练策略一样，我们在每次训练迭代中调整图像的大小，以便从{400,500,600,700,800}像素中随机抽样。我们仍然测试 600 像素的单一比例，因此不添加测试时间成本。该 mAP 是 80.5%。另外，我们在 MS COCO 训练集上训练我们的模型，然后在 PASCAL VOC 训练集上对其进行微调。R-FCN 实现了 83.6% 的 mAP（表4），接近[9]中使用 ResNet-101 的“Faster R-CNN +++”系统。我们注意到，我们的竞争结果是在每张图像 0.17 秒的测试速度下获得的，比 Faster R-CNN +++ 快 20 倍，因为它进一步包含了迭代框回归，上下文和多尺度测试[9]。这些比较也 PASCAL VOC 2012 测试集上观察到（表5）。  \n",
    "\n",
    "##### On the Impact of Depth  \n",
    "\n",
    "下表显示了使用不同深度的 ResNets 的 R-FCN 结果[9]。当深度从 50 增加到 101 时，我们的检测精度增加，但深度达到 152 时，检测精度下降。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/10/6.jpg?raw=true)\n",
    "\n",
    "##### On the Impact of Region Proposals  \n",
    "\n",
    "R-FCN 可以很容易地应用于其他区域建议方法，如选择性搜索（SS）[27]和边缘框（EB）[28]。下表显示了使用不同提议的结果（使用ResNet-101）。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/10/7.jpg?raw=true)\n",
    "\n",
    "### 4.2 Experiments on MS COCO  \n",
    "\n",
    "接下来我们对 MS COCO 数据集[13]进行评估，该数据集有 80 个目标类别。我们的实验涉及 80k 训练数据，40k val 数据和 20k 测试开发数据。我们将交替训练[18]从 4 步扩展到 5 步，当共享特征时略微提高了该数据集的准确性；我们还报告说，两步训练足以达到相当好的准确性，但特征不共享。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/10/8.jpg?raw=true)\n",
    "\n",
    "结果如表6 所示。我们 single-scale 训练的 R-FCN 基线的结果为 48.9%/27.6%。这与 Faster R-CNN 基线（48.4%/27.2%）相当，但我们的测试速度提高了2.5倍。值得注意的是，我们的方法在小尺寸的物体上表现更好（由[13]定义）。我们的多尺度训练（单尺度测试）的 R-FCN 在测试数据上的结果为49.1%/27.8%，51.5%/29.2%。考虑到 COCO 广泛的目标尺度，我们进一步评估[9]中的多尺度测试变体，并使用{200,400,600,800,1000}的测试尺度。mAP 为 53.2%/31.5%。在 MS COCO 2015 比赛中，这个结果接近第一名的成绩。尽管如此，我们的方法更简单，并且不增加诸如[9]使用的上下文或迭代框回归等，并且训练和测试都更快。  \n",
    "\n",
    "## 5 Conclusion and Future Work  \n",
    "\n",
    "我们提出了基于区域的全卷积网络，这是一种简单但准确且高效的物体检测框架。我们的系统自然采用了最先进的图像分类骨干网，如 ResNet，它们都是全卷积设计的。我们的方法实现了与 Faster R-CNN 可比的结果，但在训练和推断期间速度更快。  \n",
    "\n",
    "我们故意将本文提出的 R-FCN 系统保持简单。已经有一系列针对语义分割（例如参见[2]）开发的 FCN 的正交扩展，以及用于目标检测的基于区域的方法的扩展（例如参见[9,1,22]）。我们希望我们的系统能够轻松享受该领域的进展带来的好处。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/detection/10/9.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考  \n",
    "\n",
    "- 1 [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497)  \n",
    "- 2 [Light-Head R-CNN: In Defense of Two-Stage Object Detector](https://arxiv.org/abs/1711.07264)  \n",
    "- 3 [Cascade R-CNN: Delving into High Quality Object Detection](https://arxiv.org/abs/1712.00726)  \n",
    "- 4 [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640)  \n",
    "- 5 [YOLO9000: Better, Faster, Stronger](https://arxiv.org/abs/1612.08242)  \n",
    "- 6 [YOLOv3: An Incremental Improvement](https://arxiv.org/abs/1804.02767)  \n",
    "- 7 [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325)  \n",
    "- 8 [DSSD : Deconvolutional Single Shot Detector](https://arxiv.org/abs/1701.06659)  \n",
    "- 9 [Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network for Real-time Embedded Object Detection](https://arxiv.org/abs/1802.06488)  \n",
    "- 10 [R-FCN: Object Detection via Region-based Fully Convolutional Networks](https://arxiv.org/abs/1605.06409) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
