{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract  \n",
    "\n",
    "卷积网络是强大的视觉模型，可以产生不同层次的特征。我们表明，通过端到端的训练卷积网络本身，在语义分割任务上可以超过目前最先进的方法。关键洞察是建立“全卷积”网络，以任意大小的输入和有效的推理产生相应大小的输出。我们定义并详细说明全卷积网络，解释它们在空间密集预测任务中的应用，并且绘制与先前模型的连接。我们将目前存在的分类网络（AlexNet，VGG 和 GoogLeNet）应用于全卷积网络，并通过微调将它们的学习表示转移到分割任务。然后，我们设计一种新颖的架构，将来自深层粗略层次的语义信息与浅层精细层次的外观信息相结合，以产生精确细致的分割。我们的全卷积网络实现了PASCAL VOC（相对于2012年的平均IU提高20％至平均IU 62.2％），NYUDv2 和 SIFT Flow 的最先进的分割，而对于典型的图像，推断仅需不到五分之一秒。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction  \n",
    "\n",
    "Convnets 不仅改进了整体图像分类的效果，而且在结构化输出的定位任务上也取得了进展。这些包括边界框对象检测，关键点预测和定位方面的进展。  \n",
    "\n",
    "从粗到细推理的下一步是对每个像素进行预测。先前已经有文章将卷积网络用于语义分割，他们用据像素最近目标的label 作为像素的label输出，但是这种方法具有一定的缺陷。  \n",
    "\n",
    "我们展示了一个全卷积网络（FCN），在语义分割上进行端到端的像素到像素的训练，超越了现有技术水平。据我们所知，这是第一个针对像素点预测端到端训练训练 FCN的工作 （1）像素点预测（2）有监督预训练。全卷积在现有的网络基础上从任意尺寸的输入预测密集输出。学习和推理能在全图通过密集的前馈计算和反向传播一次执行。网内上采样层能在像素级别预测和通过下采样池化学习。  \n",
    "\n",
    "这种方法既快速又有效，并且不需要太多的复杂性。分批训练是常见的方法，但缺乏全卷积训练的有效性。我们的方法不利用复杂的预处理或后处理，包括超像素，区域提议或随机细化或局部分类。我们的模型通过将分类网络重新调整为全卷积网络并且从这些网络已经学习到的参数中微调全卷积网络，从而将分类认为转化为密集预测任务。相比之下，以前的方法用了没有经过有监督预训练的小网络。  \n",
    "\n",
    "语义分割面临着语义和位置之间固有问题：全局信息解决了是什么的问题，局部信息解决了在哪里的问题。深层特征通过非线性的局部到全局金字塔编码了位置和语义信息。我们定义了一个新颖的“skip”体系结构，在4.2节（见图3）中结合了深、粗层次的语义信息和浅、精细的表征信息。  \n",
    "\n",
    "在下一节中，我们将回顾有关深度分类网络，FCNs 和近期使用小网格进行语义分割的方法。以下部分解释了 FCN 设计和密集预测de 折衷方案，将我们的架构与网内上采样和多层组合相结合，并描述了我们的实验框架。最后，展示我们在 PASCAL VOC 2011-2，NYUDv2 和 SIFT Flow 的最新成果  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Related work  \n",
    "\n",
    "我们的方法利用了深度网络在图像分类和迁移学习方面取得的最新成果。迁移先在各种视觉识别任务上得到应用，然后在检测、以及用混合区域提议分类器模型实现的实例和语义分割中。我们现在重新设计和微调分类网络来指导语义分割的密集预测任务。我们绘制了 FCN 的空间，并在这个框架中放置了历史和近期的先前模型。  \n",
    "\n",
    "#### Fully convolutional networks  \n",
    "\n",
    "就我们所知，第一次将卷积网扩展到任意尺寸的输入的是Matan等人，它扩展了经典的 LeNet 来识别数字字符串。Wolf 和 Platt 将卷积网输出扩展为二维图，用来定位邮政地址块的四个角。Ning等人将全卷积网络用于秀丽线虫组织的粗糙的、多分类分割任务。  \n",
    "\n",
    "全卷积计算也被用在现在的一些多层次的网络结构中。 Sermanet 等人的滑动窗口检测，Pinheiro 和 Collobert 的语义分割，以及 Eigen 等人的图像恢复，都是基于全卷积网络的。全卷积训练很少，但是被 Tompson 等人用来学习一种非常有效的端到端的局部检测和姿态估计的空间模型，尽管他们没有解释或者分析这种方法。  \n",
    "\n",
    "另外，He 等人放弃分类网络的非卷积部分来做特征提取。他们结合 proposals 和空间金字塔 pooling 来产生一个局部的、固定长度的特征用于分类。尽管快速且有效，但是这种混合模型不能进行端到端的学习。  \n",
    "\n",
    "#### Dense prediction with convnets  \n",
    "\n",
    "近期的一些工作已经将卷积网应用于密集预测任务，包括 Ning 等人的语义分割 和 Ciresan 等人对电子显微镜的边界预测。以及Ganin 和 Lempitsky 通过混合卷积网和最邻近模型处理自然场景图像;还有 Eigen 等人的图像修复和深度估计。这些方法的相同点包括如下：\n",
    "* 限制容量和接收域的小模型\n",
    "* patchwise 训练\n",
    "* 超像素投影的预处理，随机场正则化、滤波或局部分类\n",
    "* 像 OverFeat 介绍的输入移位和输出交错的密集输出\n",
    "* 多尺度金字塔预处理\n",
    "* 饱和的 tanh 激活函数\n",
    "* 集成\n",
    "\n",
    "我们的方法没有这些机制。但我们研究了 patchwise 训练和从 FCNs 的角度出发的“shift-and-stitch”密集输出。也讨论了网内采样。  \n",
    "\n",
    "与现有方法不同，我们采用图像分类作为有监督的预训练来适应和扩展深度分类体系结构，并且全卷积地进行微调以从整个图像输入和整个图像 ground thruths 简单而有效地学习。  \n",
    "\n",
    "Hariharan 等人和 Gupta 等同样用深层分类网络进行语义分割，但在基于混合 proposal-classifier 模型。这些方法通过对边界框和区域提议进行抽样来来微调 R-CNN 系统，从而进行检测，语义分割和实例分割等任务。这两种方法都不是端到端学习的。  \n",
    "\n",
    "他们分别在 PASCAL VOC 分割和 NYUDv2 分割方面取得了最新的成果，所以我们直接比较了我们的独立端到端 FCN 和他们的语义分割结果。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fully convolutional networks  \n",
    "\n",
    "convnet 中的每一层数据都是大小为 h×w×d 的三维数组，其中 h 和 w 是空间维度，d 是特征或通道维度。第一层是图像，像素大小为 h×w，d 个颜色通道。更高层的位置对应于它们所路径连接的图像中的位置，这些位置被称为接受域。  \n",
    "\n",
    "Convnets 是以平移不变形作为基础的。它们的基本组件（convolution, pooling, and activation functions）在局部输入区域上运行，并且仅依赖于相对空间坐标。在特定层的位置 $(i,j)$ 的数据向量为 $X_{ij}$x，下一层中为 $Y_{ij}$。    \n",
    "\n",
    "一个由 FCN 组成的实值损失函数定义一个任务。如果损失函数是最终层的空间维数的总和 $l(x,\\theta) = $，则其梯度将是每个空间分量的梯度之和。因此，在整个图像上计算的随机梯度下降将与在0上的随机梯度下降相同，将所有最终图层接受域视为小图。  \n",
    "\n",
    "当这些感受域显着重叠时，当在整个图像上逐层计算时，前馈计算和反向传播都更为有效，而不是独立地逐个修补。  \n",
    "\n",
    "接下来，我们将解释如何将分类网转换为生成粗输出图的完全卷积网。 对于像素级预测，我们需要将这些粗略的输出连接回像素。 第3.2节描述了OverFeat [29]为此目的而引入的一个技巧。 我们通过将其重新解释为等同的网络修改来深入了解这一技巧。 作为一种有效的替代方法，我们在3.3节介绍了去卷积的去卷积层。 在3.4节中，我们考虑采用拼凑抽样的方法进行训练，并在4.3节给出证据，证明我们的整个图像训练速度更快，效果也更好。  \n",
    "\n",
    "### 3.1. Adapting classifiers for dense prediction  \n",
    "\n",
    "典型的识别网络，包括LeNet [21]，AlexNet [19]及其更深的继承者[31,32]，表面上采用固定大小的输入并产生非空间输出。 这些网完全连接的层具有固定的尺寸并丢弃空间坐标。 然而，这些完全连接的层也可以被视为与覆盖整个输入区域的内核的卷积。 这样做把它们投入到完全卷积网络中，输入任意大小和输出分类图。 这种变化如图2所示。（相反，非卷积网络，如Le et al。[20]所提出的那样，缺乏这种能力。）  \n",
    "\n",
    "此外，尽管得到的地图等同于对特定输入小片的原始网络的评估，但是在这些小片的重叠区域上的计算是高度分摊的。 例如，虽然AlexNet需要1.2 ms（在典型的GPU上）产生227×227图像的分类分数，但完全卷积版本需要22 ms来从500×500图像产生10×10个输出的网格， 比原来的方法快5倍以上1  \n",
    "\n",
    "这些卷积模型的空间输出图使得它们成为诸如语义分割之类的密集问题的自然选择。 在每个输出单元都具有地面真实性的情况下，前向和后向过程都是直接的，都利用了卷积的固有计算效率（和积极的优化）。  \n",
    "\n",
    "AlexNet示例的相应后向时间是单个图像为2.4 ms，完全卷积10 x 10输出映射为37 ms，导致类似于正向传递的加速。 这种密集的反向传播在图1中示出。  \n",
    "\n",
    "尽管我们将分类网络重新解释为完全卷积产生任意大小的输入的输出映射，但输出维度通常通过子采样来减少。 分类网络子采样保持过滤器小，计算要求合理。 这使得这些网络的完全卷积形式的输出变粗，从输入的大小减少了与输出单元的接受域的像素跨度相等的因子。  \n",
    "\n",
    "### 3.2. Shift-and-stitch is filter rarefaction  \n",
    "\n",
    "输入移位和输出交错是一个技巧，可以从没有插值的粗输出中产生密集的预测，由OverFeat引入[29]。 如果输出采样的因子为f，则输入向左（右侧和顶部填充）移动x个像素，向下移动y个像素，每个（x，y）∈{0，。。。 ，f - 1}×{0，。。。 ，f-1}。 这些f 2输入每个都通过convnet运行，并且输出是交错的，以便预测对应于其接受场的中心的像素。  \n",
    "\n",
    "只改变一个回旋点的过滤器和层跨度可以产生与这种移位和针迹技巧相同的输出。 考虑一个具有输入步长的图层（卷积或合并），以及一个滤波器权重为fij（忽略特征尺寸，这里与此无关）的后续卷积图层。 将下层的输入步幅设置为1，将其输出上采样为s，就像移位和针迹一样。 但是，使用上采样输出卷积原始滤波器并不会产生与诀窍相同的结果，因为原始滤波器只能看到其（现在是上采样）输入的缩减部分。 为了重现这个窍门，通过放大过滤器来稀释过滤器  \n",
    "公式  \n",
    "\n",
    "（以i和j为零）。 再现该技巧的全部净输出包括重复该逐层滤波器放大直到所有子采样被去除。  \n",
    "\n",
    "简单地减少网内的子采样是一个折衷：滤波器看到更好的信息，但有更小的接受域，需要更长的时间来计算。 我们已经看到，移位和拼接技巧是另一种折衷：在不减小滤波器的接收场大小的情况下，输出变得更密集，但禁止滤波器以比原始设计更精细的尺度访问信息。  \n",
    "\n",
    "虽然我们已经做了移位和缝合的初步实验，但是我们并没有在模型中使用它。 如下一节所述，我们发现通过上采样进行学习是更加有效和高效的，特别是当与后面描述的跳层融合相结合时。  \n",
    "\n",
    "### 3.3. Upsampling is backwards strided convolution  \n",
    "\n",
    "将粗输出连接到密集像素的另一种方法是插值。 例如，简单的双线性插值通过线性映射来计算来自最近四个输入的每个输出yij，线性映射仅依赖于输入和输出单元的相对位置。  \n",
    "\n",
    "从某种意义上讲，f因子的上采样是1 / f分数输入步长的卷积。 只要f是整数，上采样的一个自然的方法就是向后卷积（有时称为解卷积），并且输出跨步。 这样的操作实现起来是微不足道的，因为它简单地颠倒了卷积的前进和后退过程。 因此，上采样是在网络中通过从像素方向丢失的反向传播来进行端到端的学习。  \n",
    "\n",
    "注意，这样的层中的去卷积滤波器不需要是固定的（例如，对于双线性上采样），但是可以被学习。 一堆去卷积层和激活函数甚至可以学习非线性上采样。  \n",
    "\n",
    "在我们的实验中，我们发现网络上采样对于学习密集预测是快速有效的。 我们最好的分割体系结构使用这些图层来学习在4.2节中进行精细预测的上采样。  \n",
    "\n",
    "### 3.4. Patchwise training is loss sampling  \n",
    "\n",
    "在随机优化中，梯度计算是由训练分布驱动的。 尽管它们的相对计算效率取决于重叠和小批量大小，但是可以进行拼接训练和全卷积训练来产生任何分布。 整个图像完全卷积训练与每个批次由图像（或图像集合）损失之下的单位的所有接受域组成的拼接训练相同。 虽然这比补丁的统一采样更有效，但它减少了可能的批次数量。 然而，可以简单地恢复图像内的补丁的随机选择。 将损失限制为随机抽样的空间项子集（或等效地在输出与损失之间应用DropConnect掩码[36]）将梯度排除在梯度计算之外。  \n",
    "\n",
    "如果保存的补丁仍然有明显的重叠，完全卷积计算仍然会加快训练速度。 如果渐变是通过多次向后传递累积的，则批次可以包含来自多个图像的补丁  \n",
    "\n",
    "抽样训练中的采样可以纠正类别失衡[27,8,2]，并减轻密集斑块的空间相关性[28,16]。 在完全卷积训练中，类别平衡也可以通过权重损失来实现，损失采样可以用来解决空间相关性。  \n",
    "\n",
    "我们在4.3节中通过抽样来探索训练，并且没有发现它对密集预测产生更快或更好的收敛性。 整体形象培训是有效和高效的。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Segmentation Architecture  \n",
    "\n",
    "我们将ILSVRC分类器投射到FCN中，并将它们用于网络上采样和像素损失的密集预测。 我们通过微调来训练分割。 接下来，我们构建了一个新颖的跳过体系结构，将粗糙的，语义的和局部的，外观信息结合起来进行预测。  \n",
    "\n",
    "对于这项调查，我们在PASCAL VOC 2011分割挑战[7]上进行培训和验证。 我们用每像素多项式逻辑损失进行训练，并使用平均像素相交的标准度量来验证，包括背景在内的所有类别均值。 训练忽略在地面实况中被掩盖掉（像模棱两可或困难）的像素。  \n",
    "\n",
    "### 4.1. From classifier to dense FCN  \n",
    "\n",
    "我们从卷积化成熟的分类体系结构开始，如第3节所述。我们考虑赢得ILSVRC12的AlexNet3体系结构[19]，以及在ILSVRC14中做得非常好的VGG网络[31]和GoogLeNet4 [32]。我们选择了VGG的16层net5，我们发现它相当于这个任务的19层网络。对于GoogLeNet，我们只使用最后的丢失层，并通过丢弃最终的平均池层来提高性能。我们通过丢弃最终的分类器层来斩断每个网络，并将所有完全连接的层转换为卷积。我们在信道维数21附加1×1卷积来预测每个粗略输出位置的每个PASCAL类别（包括背景）的分数，然后是一个去卷积层，将粗略输出双线性上采样到如所述的像素密集输出在3.3节。表1比较了初步验证结果和每个网络的基本特征。我们汇报了固定学习率（至少175个时代）收敛后取得的最好结果。  \n",
    "\n",
    "从分类到细分的微调给每个网络提供了合理的预测。 即使是最糟糕的模式也达到了75％的最先进表现。 分割设备的VGG网络（FCN-VGG16）在val上的平均IU值为56.0，而在测试中为52.6 [16]。 额外的数据训练提高了val7的一个子集上的59.4的平均IU性能。 培训细节在4.3节中给出。  \n",
    "\n",
    "尽管类似的分类准确性，我们的GoogLeNet的实现不符合这种分割结果。  \n",
    "\n",
    "### 4.2. Combining what and where  \n",
    "\n",
    "我们定义了一个新的完全卷积网络（FCN）进行分割，结合了特征层次结构的层次并提高了输出的空间精度。 见图3  \n",
    "\n",
    "虽然完全卷积分类器可以如4.1所示进行细化分割，甚至在标准度量上得分也很高，但是它们的输出却不尽人意（见图4）。 最终预测层的32像素跨度限制了上采样输出的细节范围。  \n",
    "\n",
    "我们通过添加将最终预测层与较低层进行更好的链接的链接来解决这个问题。 这表1我们适应和扩展三个分类约束分割。 我们通过PASCAL VOC 2011验证集上的均值交叉点平均交叉比和推理时间（对NVIDIA Tesla K40c上的500×500输入进行20次试验的平均值）来比较性能。 在密集预测方面，我们详细介绍了适应网络的结构：参数层的数量，输出单元的感受野大小，网络中最粗糙的步幅。 （这些数字能够以固定的学习速率获得最佳的性能，而不是最佳的性能。）  \n",
    "\n",
    "将线拓扑转换为DAG，其边缘从较低层跳到较高层（图3）。 由于他们看到更少的像素，更精细的尺度预测应该需要更少的层次，所以从更浅的净输出中做出更合理。 将精细层和粗糙层结合起来，可以使模型进行局部预测，从而尊重全局结构。 通过与Florack等人的多尺度局部喷气机类比。 [10]，我们称我们的非线性局部特征层次为深层喷气  \n",
    "\n",
    "我们首先通过从16个像素跨度层预测来将输出跨度减半。 我们在pool4的顶部添加一个1×1的卷积层来产生额外的类别预测。 我们将这个输出与在步骤32的conv7顶部（卷积化的fc7）上计算的预测相加，通过添加2×上采样层并对两个预测求和。 （见图3）。 我们将2倍上采样初始化为双线性插值，但允许按3.3节所述学习参数。 最后，将步幅16的预测上采样回图像。 我们称之为净FCN-16s。 FCN-16是端到端的学习，用最后一个更粗的网络（我们现在称为FCN-32）的参数进行初始化。 作用于pool4的新参数是零初始化的，所以网络以未修改的预测开始。 学习率下降了100倍。  \n",
    "\n",
    "学习这个跳网将验证集上的性能提高了3.0，即平均IU为62.4。 图4显示了输出精细结构的改进。 我们将这种融合与仅从pool4层学习（导致性能较差）相比较，并且在不增加额外链接的情况下简单地降低学习速率（这导致性能改善不显着，而不改善输出质量）。  \n",
    "\n",
    "我们继续以这种方式融合来自pool3的预测和从pool4和conv7融合的2×预测的上采样，构建净FCN-8。 我们获得了表2.在PASCAL VOC2011验证子集上跳过FCN的比较7。 学习是端到端的，除了FCN-32s-fixed，其中只有最后一层被微调。 请注意，FCN-32s是FCN-VGG16，重命名为突出步幅。  \n",
    "\n",
    "对于62.7平均IU略有进一步的改进，并且在我们的输出的平滑性和细节方面略微改进。 在这一点上，我们的融合改进已经达到了收益递减，无论是强调大规模正确性的IU度量，还是在可见性方面，例如， 在图4中，所以我们不再继续融合更低层。  \n",
    "\n",
    "#### Refinement by other means  \n",
    "\n",
    "减少汇聚层的步伐是获得更好预测的最直接的方法。 但是，对于基于VGG16的网络来说，这样做是有问题的。 将pool5图层设置为stride 1要求我们的卷积化fc6具有14×14的内核大小，以保持其接受的字段大小。 除了计算成本之外，我们很难学习如此大的过滤器。 我们试图用较小的滤波器重新构建pool5以上的层，但是在实现可比较的性能方面没有成功; 一个可能的解释是从上层的ImageNet训练权重的初始化是重要的。  \n",
    "\n",
    "获得更好预测的另一种方法是使用3.2节中描述的shiftand-stitch技巧。 在有限的实验中，我们发现这种方法的改进成本比层融合要差。  \n",
    "\n",
    "### 4.3. Experimental framework  \n",
    "\n",
    "#### Optimization  \n",
    "\n",
    "我们以新加坡的动力训练。 对于FCN-AlexNet，FCN-VGG16和FCN-GoogLeNet，分别使用20个图像的minibatch大小和10-3,10-4和5-5的固定学习速率，通过行搜索选择。 尽管我们发现训练对这些参数不敏感（但对学习速率敏感），我们使用动量0.9，体重衰减为5 -4或2 -4，并将偏差的学习速率加倍。 我们对类评分卷积层进行零初始化，发现随机初始化以产生更好的性能和更快的收敛性。 包括在原分类网中使用的辍学率。  \n",
    "\n",
    "#### Fine-tuning  \n",
    "\n",
    "我们通过全网反向传播来微调所有层。 单独对输出分类器进行微调只会产生完全微调性能的70％，如表2所示。考虑到学习基本分类网络所需的时间，从零开始进行培训是不可行的。 （请注意，VGG网络是分阶段训练的，而我们是从完整的16层版本初始化的。）对于粗FCN-32版本，在单个GPU上进行微调需要三天，每个升级到大约一天 FCN-16和FCN-8s版本。  \n",
    "\n",
    "#### Patch Sampling  \n",
    "\n",
    "如第3.4节所述，我们的全图像训练有效地将每个图像分成一个规则的大网格，重叠的补丁。相比之下，先前的工作在整个数据集上随机采样补丁[27,2,8,28,11]，可能导致更高的方差批次，可能会加速收敛[22]。我们通过以前面描述的方式对损失进行空间抽样来研究这种折衷，做出一个独立的选择，以一定的概率1-p忽略每个最后的层单元。为了避免改变有效的批量，我们同时将每批次的图像数量增加1 / p。请注意，由于卷积的效率，这种拒绝采样的形式仍然比p值足够大的分段训练快（例如根据3.1节中的数字，至少p> 0.2）。图5显示了这种采样形式对收敛的影响。我们发现，与整幅图像训练相比，采​​样对收敛速度没有显着影响，但由于每批需要考虑的图像数量较多，所以需要更多的时间。因此，我们在其他实验中选择非抽样的整体图像训练。  \n",
    "\n",
    "#### Class Balancing  \n",
    "\n",
    "完全卷积训练可以通过对损失进行加权或抽样来平衡课程。 虽然我们的标签是轻微不平衡的（大约3/4是背景），但我们发现课堂平衡是不必要的。  \n",
    "\n",
    "#### Dense Prediction  \n",
    "\n",
    "通过网络内的解卷积层将分数上采样到输入维度。 最终层去卷积滤波器被固定为双线性插值，而中间上采样层被初始化为双线性上采样，然后被学习。 不使用Shift-and-stitch（3.2节）或者过滤器稀疏等价物  \n",
    "\n",
    "#### Augmentation  \n",
    "\n",
    "我们试图通过在每个方向上将图像翻译成32个像素（最粗糙的预测尺度）来随机镜像和“抖动”图像来增强训练数据。 这没有显着的改善。  \n",
    "\n",
    "#### More Training Data  \n",
    "\n",
    "我们用于表1的PASCAL VOC 2011分割挑战训练集标注了1112幅图像。 Hariharan等人 [15]已经收集了更大的一套8498 PASCAL训练图像的标签，用于训练先前的最先进的系统SDS [16]。 该培训数据将FCNVGG16验证得分7提高了3.4分，达到59.4均值IU.  \n",
    "\n",
    "#### Implementation  \n",
    "\n",
    "所有型号都经过Caffe [18]培训和测试，使用单个NVIDIA Tesla K40c。 模型和代码将在发布时公开发布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results  \n",
    "\n",
    "我们测试我们的FCN语义分割和场景分析，探索PASCAL VOC，NYUDv2和SIFT流程。 尽管这些任务在历史上区分了对象和区域，但是我们把这两个任务统一看作像素预测。 我们在每个数据集上评估我们的FCN跳过架构8，然后将其扩展到NYUDv2的多模式输入以及SIFT流的语义和几何标签的多任务预测。  \n",
    "\n",
    "#### Metrics  \n",
    "\n",
    "我们从常见语义分割和场景分析评估中报告四个度量，这些评估是像素精度和区域交集（IU）上的变化。 令nij为第i类预测属于第j类的像素的数目，其中有ncl个不同的类别，令ti = P j nij为第i类像素的总数。 我们计算：  \n",
    "\n",
    "公式  \n",
    "\n",
    "#### PASCAL VOC  \n",
    "\n",
    "表3给出了我们的FCN-8在PASCAL VOC 2011和2012测试集上的性能，并将其与以前最先进的SDS [16]和着名的R-CNN [12]。 我们在平均IU9上取得了最好的结果，相对保证率为20％。 推断时间减少了114倍（仅限于细分，忽略建议和细化）或者286倍（总体）。  \n",
    "\n",
    "#### NYUDv2  \n",
    "\n",
    "NYUDv2是使用Microsoft Kinect收集的RGB-D数据集。它具有1449个RGB-D图像，其中像素标签已经被合并成Gupta等人的40级语义分割任务。 [13]。我们报告了795个训练图像和654个测试图像的标准分割结果。 （注意：所有型号的选择都是在PASCAL 2011 val上进行的。）表4给出了我们的模型在几个变型中的性能。首先，我们在RGB图像上训练未经修改的粗糙模型（FCN-32）。为了增加深度信息，我们训练升级为四通道RGB-D输入（早期融合）的模型。这提供了很少的好处，可能是由于难以在整个模型中传播有意义的渐变。继Gupta等人的成功之后， [14]，我们尝试深度的三维HHA编码，就这个信息训练网络，以及RGB和HHA的“后期融合”，其中来自两个网络的预测在最后一层被求和，双流网是端到端的学习。最后我们把这个晚期的融合网络升级到16步的版本。  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion  \n",
    "\n",
    "完全卷积网络是一个丰富的模型类别，其中现代分类约束是一个特例。 认识到这一点，将这些分类网络扩展到分割，并通过多分辨率层组合改进体系结构，大大提高了现有技术水平，同时简化和加快了学习和推理。  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
