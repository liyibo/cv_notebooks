{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing and Understanding Neural Models in NLP  \n",
    "\n",
    "## Abstract  \n",
    "\n",
    "虽然神经网络已经成功应用于许多 NLP 任务，但由此产生的基于向量的模型很难解释。例如，他们如何实现组合性并不清楚，从单词和短语的含义来构建句子意义。在本文中，我们描述了用于 NLP 的神经模型中的组合性可视化的策略，其灵感来源于类似的计算机视觉工作。我们首先绘制 unit values ，用以表示negation,，强化和 concessive clauses 的组合性，从而使我们能够在 negation, 中看到众所周知的标记不对称。然后，我们介绍一些单元显著性的可视化方法，即从一阶导数对最终构成含义的贡献量。我们的通用方法在理解深度网络的组合性和其他语义属性方面可能有广泛的应用。  \n",
    "\n",
    "## 1 Introduction  \n",
    "\n",
    "神经模型在各种 NLP 任务中匹配或超越其他最先进系统的性能。然而，与传统的基于特征的分类器不同，这些分类器对各种可解释特征（词类，命名实体，单词形状，句法分析特征等）进行赋值和优化，深度学习模型的行为不太容易解释。深度学习模型主要通过多层神经结构对词嵌入（低维，连续，实值向量）进行操作，其每一层都被表征为一组隐藏的神经元单元。目前还不清楚深度学习模型是如何处理 composition,，实现 negation or intensification 等功能，或者结合句子不同部分的意义，建立句子意义。  \n",
    "\n",
    "在本文中，我们探索了多种策略来解释神经模型中的意义构成。我们采用传统方法如表示性绘图，并引入简单的策略来测量神经单元对意义构成的贡献程度，它的“显著性”或使用一阶导数的重要性。  \n",
    "\n",
    "本工作中呈现的可视化技术/模型揭示了神经模型的工作原理：例如，我们说明 LSTM 的成功是因为它能够比其他模型更加关注重要的关键词；多个composition 中的构成具有竞争性，并且这些模型能够捕捉负面的不对称，这是在自然语言理解中构成语义的重要属性；存在尖锐的空间局部性，某些维度以令人惊讶的定位方式标记否定和量化。尽管我们的尝试只涉及神经模型中的 superficial points 问题，并且每种方法都有其优缺点，但它们可以提供一些洞察神经模型在基于语言的任务中的行为，标志着在自然语言处理中理解它们如何实现有意义的 composition。  \n",
    "\n",
    "下一节将介绍一些视觉和自然语言处理中的可视化模型，这些启发了这项工作。我们在第 3 节描述数据集和采用的神经模型。不同的可视化策略和相应的分析结果分别在第 4,5,6 节中给出，接下来是一个简短的结论。  \n",
    "\n",
    "## 2 A Brief Review of Neural Visualization  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/2.jpg?raw=true)\n",
    "\n",
    "通常通过将嵌入空间投影到两个维度并观察到类似的词倾向于聚集在一起，相似性通常通过图形可视化。（Karpathy et al。，2015）试图从统计观点来解释 recurrent 神经模型。其他相关尝试包括（Fyshe 等，2015；Faruqui 等，2015）。  \n",
    "\n",
    "用于解释和可视化神经模型的方法在视觉中得到了更为重要的探索，尤其是对于卷积神经网络（CNN 或 ConvNets）（Krizhevsky等，2012），其中图像像素的原始矩阵被多层神经网络卷积并汇集到隐藏层。这些方法包括：  \n",
    "\n",
    "（1）反演：通过训练一个额外的模型来反演表征，从不同的神经层次反馈到初始输入图像。重建背后的直觉是，可以从当前表示重构的像素内容。反演算法允许当前表示与原始图像的相应部分对齐。  \n",
    "\n",
    "（2）反向传播和 Deconvolutional Networks：错误从输出层反向传播到每个中间层，最后传播到原始图像输入。反卷积网络以类似的方式工作，将输出逐层投影回初始输入，每一层与一个监督模型相关联，用于将较高的映射投影到较低的映射。这些策略使得有可能发现活跃区域或对最终贡献最大的区域。  \n",
    "\n",
    "（3）生成：这组作品根据已经训练过的神经模型指导的草图生成特定类别的图像。模型以随机初始化的图像开始。在图像构建的不同阶段激活的特定图层可以帮助解释。  \n",
    "\n",
    "尽管上述策略激发了我们在本文中提出的工作，但与 NLP 之间存在根本性差异。在 NLP 中词作为基本单位，因此（单词）向量而不是单个像素是基本单位。词的序列（例如，短语和句子）也以比像素的排列更结构的方式呈现。与我们的研究并行，Karpathy 等通过分析循环神经模型的预测和误差，从错误分析的角度探索了类似的方向。  \n",
    "\n",
    "## 3 Datasets and Neural Models  \n",
    "\n",
    "我们研究了两个神经模型训练的数据集，其中一个数据集规模相对较小，另一个规模较大。  \n",
    "\n",
    "### 3.1 Stanford Sentiment Treebank  \n",
    "\n",
    "斯坦福大学 Sentiment Treebank 是广泛用于神经模型评估的基准数据集。数据集包含每个分析树组件的 gold-standard  情绪标签，从句子到短语到单个词，用于 11855 个句子中的 215154 个短语。任务是在短语和句子两个层次上执行细粒度（非常积极，积极，中性，消极和非常消极）和粗粒度（积极与消极）分类。有关数据集的更多详细信息，请参阅 Socher et al。  \n",
    "\n",
    "虽然这个数据集的许多研究都使用递归分析树模型，但在这项工作中，我们只使用标准序列模型（RNN 和 LSTM），因为这些模型是当前最广泛使用的神经模型，并且顺序可视化更直接。因此，我们首先将每个分析树节点转换为一个 tokens 序列。该序列首先映射到短语/句子表示并馈入 softmax 分类器。短语/句子表示使用以下三种模式构建：具有 TANH 激活函数的标准 Recurrent 序列，LSTM 和双向 LSTM。 有关这三种模型的详细信息，请参阅附录。  \n",
    "\n",
    "### 3.2 Sequence-to-Sequence Models  \n",
    "\n",
    "seq2seq 是旨在生成给定输入的输出文本序列的神经模型。从理论上讲，seq2seq 模型可以适应 NLP 任务，这些任务可以为给定输入的预测输出，并且由于不同的输入和输出而用于不同的目的，例如机器翻译，其输入对应于源语句和输出到目标语句；如果输入对应于消息并且输出对应于响应，则可用于对话。seq2seq 需要接受大量数据的训练，以获得学习对之间的隐式语义和句法关系。  \n",
    "\n",
    "seq2seq 模型使用 LSTM 模型将输入序列映射到向量表示，然后基于预先获得的表示顺序预测 tokens。该模型定义了输出（Y）上的分布，并使用 softmax 函数依次预测给定输入（X）的 tokens。  \n",
    "\n",
    "## 4 Representation Plotting  \n",
    "\n",
    "我们从简单的 plots 开始，使用斯坦福情绪树库（Stanford Sentiment Treebank）揭示 local compositions。  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/NLP/1/1.jpg?raw=true)\n",
    "\n",
    "**Local Composition** 图1 显示了一个 60d 热图矢量，用于表示选定的单词/短语/句子，重点在于 extent 修改（状语和 adjectival）和 negation。短语或句子的嵌入是通过组合来自预训练模型的词表达来实现的。  \n",
    "\n",
    "我们在图2 中使用 tsne（Van der Maaten和Hinton，2008）对单词和短语进行可视化，故意添加一些随机单词以供比较。可以看出，神经模型很好地学习了 local compositionally 性质，将否定词+积极词（“不好”，“不好”）与负面词组合在一起。还要注意否定的不对称性：“not bad”更多的和负性词在聚在一起（如图1和图2所示）。这种不对称性在语言学中得到了广泛的讨论，例如由于显著性而引起的，因为'好'是该尺度的未标记方向。这表明，虽然模型在图1 中似乎集中于某些 negation，但神经模型不仅仅是学习将固定变换应用于“不”，而是能够捕捉不同单词构成中的细微差异。  \n",
    "\n",
    "**Concessive Sentences ** 在让步性句子中，两个从句具有相反的词性，通常与期望相反的含义相关。图3 绘制出两个让步从句随时间演变的表示图。这些图表明：  \n",
    "\n",
    "1.对于目标是预测特定语义维度（与语言模型词预测等一般任务相反）的情感分析等任务而言，过大的维度会导致许多维度无法运行（值接近0），从而导致两个维度 相反情绪的句子仅在几个维度上有所不同。 这可以解释为什么更多的维度不一定会导致这些任务更好的表现（例如，如（Socher等人，2013）中所报道的，当词维度设置在25和35之间时达到最佳性能）。  \n",
    "\n",
    "。。。  \n",
    "\n",
    "**Clause Composition** 在图4 中，我们更详细地探讨了此 clause composition。通过添加负面 clauses，比如“although it had bad acting”，或者““but it is too long”到简单的积极的“I like the movie”的结尾，这些表现力更接近负面情绪区域。相比之下，为负面 clause 增加一个concessive clause 并不会朝着积极的方向发展；“I hate X but ...”仍然是非常消极的，与“I hate X”没有什么不同。这种差异再次表明该模型能够捕捉负面不对称。  \n",
    "\n",
    "## 5 First-Derivative Saliency  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/3.jpg?raw=true)\n",
    "\n",
    "在本节中，我们描述了另一种策略，它受到视觉中的后向传播策略的启发。它衡量每个输入单位对最终决策的贡献量，可以用一阶导数来近似。  \n",
    "\n",
    "更正式地说，对于分类模型，输入 E 与标准类别标签 c 相关联。（根据NLP任务，输入可以是单词或单词序列的嵌入，而标签可以是 POS 标签，情感标签，预测的下一个单词索引等）给定输入词的嵌入 E 以及关联标签 c，训练后的模型将该对（E，c）与 $S-c(E)$ 分数联系起来。目标是决定 E 的哪些units 对 $S-c(E)$ 做出最重要的贡献，并因此决定是否选择类别标签 c。   \n",
    "\n",
    "### 5.1 Results on Stanford Sentiment Treebank  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/4.jpg?raw=true)\n",
    "\n",
    "我们首先在斯坦福树库上说明结果。我们在图5，图6 和图7 中绘制了三个句子的显著性分数，将训练的模型应用于每个句子。每行对应于表示每个维度的每个网格的对应词表示的显著性分数。这些例子是基于明确的情绪指标“hate”，这些指标给了他们所有的负面情绪。  \n",
    "\n",
    "**“I hate the movie”** 所有这三种模式都高度赞同“hate”，并削弱了其他 tokens 的影响力。LSTM 比标准的循环模型更加关注“hate”，但双向LSTM 显示最清晰的焦点，几乎不重视“hate”以外的单词。这大概是由于 LSTM 和 Bi-LSTM 中的门结构控制信息流，使得这些体系结构更好地滤除不太相关的信息。  \n",
    "\n",
    "**“I hate the movie that I saw last night”** 所有三种模式都指定了正确的观点。简单的循环模型在筛选出不相关的信息方面再次做得很差，对与情绪无关的单词分配过多的显著性。然而，尽管这个句子更长，但没有一个模型受到梯度消失问题的影响；“hate”的显著性在卷积操作之后仍然突出。  \n",
    "\n",
    "**““I hate the movie though the plot is interesting”** 简单的循环模型只强调第二个 clause “the plot is interesting”，并没有对第一个 clause “I hate the movie”给予评价。这看起来可能是由渐变消失引起的，但该模型正确地将该句子分类为非常消极，表明它成功合并来自第一个否定句子的信息。我们单独测试了单个 clause “though the plot is interesting”。标准循环模型自信地将其标记为正面。因此，尽管第一个句子中的单词显著性得分较低，但简单的循环系统设法依赖该句子，并且从后面的肯定句子中淡化信息 - 尽管后面的单词具有较高的显著性分数。这说明了显著性可视化的局限性。一阶导数不能捕获所有我们想要显示的信息，也许是因为它们只是粗略地接近 individual 贡献，并且可能不足以处理高度非线性的情况。相比之下，LSTM 强调第一个 clause，大大减弱了第二个 clause 的影响，而 Bi-LSTM 侧重于“hate the movie”和“plot is interesting”。  \n",
    "\n",
    "### 5.2 Results on Sequence-to-Sequence Autoencoder  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/6.jpg?raw=true)\n",
    "\n",
    "图9 表示在每个时间步骤预测对应 token 方面自动编码器的显著热图。随着解码的进行，我们通过反向传播计算每个前一个词的一阶导数。每个网格对应于每个 1000 维单词向量的平均显著性值的大小。热图给出了解码过程中神经模型行为的清晰概览。观察总结如下：  \n",
    "\n",
    "1.对于每个预测单词的时间步长，seq2seq 模型设法将单词预测返回到输入端的对应区域（自动学习对齐），例如，当需要预测的 token 为“hate”时，以 token “hate”为中心的输入区域会产生更大的影响。  \n",
    "\n",
    "2.神经解码将先前构建的表示与当前步骤中预测的单词相结合。随着解码的进行，初始输入对解码（即源句子中的tokens）的影响逐渐减小，因为更多的预先预测的单词被编码在矢量表示中。与此同时，语言模型的影响逐渐占据主导地位：当预测单词“boring”时，模型更重视早期预测 tokens \"plot\" 和 \"is\"，但对输入中对应的区域较少关注，eg. 输入中的单词“boring “。  \n",
    "\n",
    "## 6 Average and Variance  \n",
    "\n",
    "![Aaron Swartz](https://github.com/liyibo/cv_notebooks/blob/master/markdown_pics/OCR/1/5.jpg?raw=true)\n",
    "\n",
    "对于将单词嵌入视为参数以从头开始优化的设置（与使用预先训练的嵌入相反），我们提出了第二个令人惊讶的简单而直接的方法来可视化重要指标。我们首先计算句子中所有单词的单词嵌入的平均值。一个词的显著性或影响力的衡量标准是它偏离这个平均值。这个想法是，在训练过程中，模型会学习使 indicators 与 non-indicator 词不同，即使在经过多层计算之后，它们也能够脱颖而出。  \n",
    "\n",
    "图8 显示了方差图；每个网格对应于 $\\parallel e_{ij} - \\frac {1}{N_s} \\sum_{i' \\in N_S} e_{i'j} \\parallel^2$ 的值，其中 $e_{ij}$ 表示单词 i 的第 j 维的值，N 表示句子内 token 的数量。  \n",
    "\n",
    "如图所示，基于方差的显著性度量也很好地强调了相关的情感词。该模型确实有缺点：（1）它只能用于字嵌入是学习参数的场景（2）很明显，模型能够很好地显示局部组合性。  \n",
    "\n",
    "## 7 Conclusion  \n",
    "\n",
    "在本文中，我们提供了几种方法来帮助可视化和解释神经模型，了解神经模型如何构成 meanings，展示 negation 的不对称性并在某些方面解释 LSTM 在这些任务中的强大性能。  \n",
    "\n",
    "尽管我们的尝试只是触及神经模型中的表面观点，并且每种方法都有其优点和缺点，但它们可以一起提供有关神经模型在基于语言的任务中的行为的一些见解，标志着理解它们如何在自然语言处理获得意义组成的初始步骤。我们未来的工作包括使用可视化结果来执行错误分析，以及了解不同神经模型的强度限制。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考  \n",
    "\n",
    "- 1 [Visualizing and Understanding Neural Models in NLP](https://arxiv.org/abs/1506.01066)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
