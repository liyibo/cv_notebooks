{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图片风格转换就是将一幅图片的内容与另外一幅图片的风格融合在一起，形成一幅独特的照片。  \n",
    "\n",
    "图片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 摘要：  \n",
    "1 使用预训练好的VGG网络提取图片特征，低层特征用于重建图像风格，高层特征用于重建图片内容  \n",
    "2 创建一幅随机噪声组成的图片，通过构建的content loss 和 style loss 迭代优化这幅图片，最终作为输出结果  \n",
    "Note：content 和 style 图像的特征只计算一次，新建空的图片之后，迭代计算空图片在content 和 style 层的特征，计算当前的loss，反向更新空的图片像素值，最终该图片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优缺点  \n",
    "优点：可以端到端的分离和重组任意图像的内容和风格，用Gram矩阵来作为图像的风格表示，从而可以量化两幅图片的风格差异，具有创造力  \n",
    "缺点: 每转换一次，都需要重新训练整个模型，速度有待优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验：  \n",
    "### 1 定义用于内容和风格重建的特征层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONTENT_LAYERS = ('relu4_2', 'relu5_2')\n",
    "STYLE_LAYERS = ('relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 前向提取content feature  \n",
    "\n",
    "content： 输入的 content 图片\n",
    "vgg.preprocess content 图片预处理，减去imagenet里的像素均值  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute content features in feedforward mode\n",
    "g = tf.Graph()\n",
    "with g.as_default(), g.device('/cpu:0'), tf.Session() as sess:\n",
    "    image = tf.placeholder('float', shape=shape)\n",
    "    net = vgg.net_preloaded(vgg_weights, image, pooling)\n",
    "    content_pre = np.array([vgg.preprocess(content, vgg_mean_pixel)])\n",
    "    for layer in CONTENT_LAYERS:\n",
    "        content_features[layer] = net[layer].eval(feed_dict={image: content_pre})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 前向提取 style feature  \n",
    "\n",
    "用Gram矩阵来计算图像的风格，可以将Gram矩阵认为是同一层中不同的feature map之间的协方差矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute style features in feedforward mode\n",
    "for i in range(len(styles)):\n",
    "    g = tf.Graph()\n",
    "    with g.as_default(), g.device('/cpu:0'), tf.Session() as sess:\n",
    "        image = tf.placeholder('float', shape=style_shapes[i])\n",
    "        net = vgg.net_preloaded(vgg_weights, image, pooling)\n",
    "        style_pre = np.array([vgg.preprocess(styles[i], vgg_mean_pixel)])\n",
    "        for layer in STYLE_LAYERS:\n",
    "            features = net[layer].eval(feed_dict={image: style_pre})\n",
    "            features = np.reshape(features, (-1, features.shape[3]))\n",
    "            gram = np.matmul(features.T, features) / features.size\n",
    "            style_features[i][layer] = gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 新建空的图片，迭代优化  \n",
    "\n",
    "image ： 新建待优化的图片，也就是最终的融合后的图片  \n",
    "content_weight_blend ： 不同层的loss在总content loss 中的权重  \n",
    "tf.nn.l2_loss(net[content_layer] - content_features[content_layer]) ：content图片和待优化图片在相同feature map 上的平方误差损失  \n",
    "tf.nn.l2_loss(gram - style_gram) ： style图片和待优化图片在相同feature map 上的平方误差损失  \n",
    "total variation denoising ： 用于使输出的图片看起来更自然  \n",
    "loss ： 最终待优化的损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make stylized image using backpropogation\n",
    "with tf.Graph().as_default():    \n",
    "    noise = np.random.normal(size=shape, scale=np.std(content) * 0.1)\n",
    "    initial = tf.random_normal(shape) * 0.256\n",
    "    \n",
    "    image = tf.Variable(initial)\n",
    "    net = vgg.net_preloaded(vgg_weights, image, pooling)\n",
    "\n",
    "    # content loss\n",
    "    content_layers_weights = {}\n",
    "    content_layers_weights['relu4_2'] = content_weight_blend\n",
    "    content_layers_weights['relu5_2'] = 1.0 - content_weight_blend\n",
    "\n",
    "    content_loss = 0\n",
    "    content_losses = []\n",
    "    for content_layer in CONTENT_LAYERS:\n",
    "        content_losses.append(content_layers_weights[content_layer] * content_weight * (2 * tf.nn.l2_loss(\n",
    "                net[content_layer] - content_features[content_layer]) /\n",
    "                content_features[content_layer].size))\n",
    "    content_loss += reduce(tf.add, content_losses)\n",
    "\n",
    "    # style loss\n",
    "    style_loss = 0\n",
    "    for i in range(len(styles)):\n",
    "        style_losses = []\n",
    "        for style_layer in STYLE_LAYERS:\n",
    "            layer = net[style_layer]\n",
    "            _, height, width, number = map(lambda i: i.value, layer.get_shape())\n",
    "            size = height * width * number\n",
    "            feats = tf.reshape(layer, (-1, number))\n",
    "            gram = tf.matmul(tf.transpose(feats), feats) / size\n",
    "            style_gram = style_features[i][style_layer]\n",
    "            style_losses.append(style_layers_weights[style_layer] * 2 * tf.nn.l2_loss(gram - style_gram) / style_gram.size)\n",
    "        style_loss += style_weight * style_blend_weights[i] * reduce(tf.add, style_losses)\n",
    "\n",
    "    # total variation denoising\n",
    "    tv_y_size = _tensor_size(image[:,1:,:,:])\n",
    "    tv_x_size = _tensor_size(image[:,:,1:,:])\n",
    "    tv_loss = tv_weight * 2 * (\n",
    "            (tf.nn.l2_loss(image[:,1:,:,:] - image[:,:shape[1]-1,:,:]) /\n",
    "                tv_y_size) +\n",
    "            (tf.nn.l2_loss(image[:,:,1:,:] - image[:,:,:shape[2]-1,:]) /\n",
    "                tv_x_size))\n",
    "    # overall loss\n",
    "    loss = content_loss + style_loss + tv_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 优化输出结果  \n",
    "Luminosity transfer steps : 还不知道为什么用这个  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer setup\n",
    "train_step = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon).minimize(loss)\n",
    "# optimization\n",
    "best_loss = float('inf')\n",
    "best = None\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    stderr.write('Optimization started...\\n')\n",
    "    for i in range(iterations):\n",
    "        stderr.write('Iteration %4d/%4d\\n' % (i + 1, iterations))\n",
    "        train_step.run()\n",
    "        last_step = (i == iterations - 1)\n",
    "        if (checkpoint_iterations and i % checkpoint_iterations == 0) or last_step:\n",
    "            this_loss = loss.eval()\n",
    "            if this_loss < best_loss:\n",
    "                best_loss = this_loss\n",
    "                best = image.eval()\n",
    "            img_out = vgg.unprocess(best.reshape(shape[1:]), vgg_mean_pixel)\n",
    "            if preserve_colors and preserve_colors == True:\n",
    "                original_image = np.clip(content, 0, 255)\n",
    "                styled_image = np.clip(img_out, 0, 255)\n",
    "               \n",
    "                # Luminosity transfer steps:\n",
    "                # 1. Convert stylized RGB->grayscale accoriding to Rec.601 luma (0.299, 0.587, 0.114)\n",
    "                # 2. Convert stylized grayscale into YUV (YCbCr)\n",
    "                # 3. Convert original image into YUV (YCbCr)\n",
    "                # 4. Recombine (stylizedYUV.Y, originalYUV.U, originalYUV.V)\n",
    "                # 5. Convert recombined image from YUV back to RGB\n",
    "\n",
    "                # 1\n",
    "                styled_grayscale = rgb2gray(styled_image)\n",
    "                styled_grayscale_rgb = gray2rgb(styled_grayscale)\n",
    "\n",
    "                # 2\n",
    "                styled_grayscale_yuv = np.array(Image.fromarray(styled_grayscale_rgb.astype(np.uint8)).convert('YCbCr'))\n",
    "\n",
    "                # 3\n",
    "                original_yuv = np.array(Image.fromarray(original_image.astype(np.uint8)).convert('YCbCr'))\n",
    "\n",
    "                # 4\n",
    "                w, h, _ = original_image.shape\n",
    "                combined_yuv = np.empty((w, h, 3), dtype=np.uint8)\n",
    "                combined_yuv[..., 0] = styled_grayscale_yuv[..., 0]\n",
    "                combined_yuv[..., 1] = original_yuv[..., 1]\n",
    "                combined_yuv[..., 2] = original_yuv[..., 2]\n",
    "\n",
    "                # 5\n",
    "                img_out = np.array(Image.fromarray(combined_yuv, 'YCbCr').convert('RGB'))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
